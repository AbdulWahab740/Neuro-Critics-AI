{"pmid": "40691890", "pmcid": "12307457", "title": "Crossmodal semantic congruence and rarity improve episodic memory", "publication_year": "N/A", "abstract": "Semantic congruence across sensory modalities at encoding of information has been shown to improve memory performance over a short time span. However, the beneficial effect of crossmodal congruence is less well established when it comes to episodic memories over longer retention periods. This gap in knowledge is particularly wide for cross-modal semantic congruence under incidental encoding conditions, a process that is especially relevant in everyday life. Here, we present the results of a series of four experiments (total", "full_text": "Someone attending a symphony concert can recognize some of the musical instruments of the orchestra by sight, and in some cases, match their characteristic sounds by ear. This process requires access to crossmodal associations acquired during a lifetime regarding which sounds correspond to which visual objects. Generally speaking, it would seem rather advantageous for humans to harness on this kind of crossmodal relations from semantic memory, both to expedite the perception of events in the immediate sensory environment (as it has been shown in multiple studies on crossmodal perception and attention; see Beauchamp et al.,\nOne particularly relevant question is to understand if and how implicit encoding of semantically congruent multisensory events improves episodic memory—that is to say, memories about crossmodal events experienced without an intentional, explicit effort made to register and retain them. Although the effects of crossmodal semantic congruence after incidental encoding have been addressed profusely, studies have almost exclusively used short-term memory protocols, and evidence regarding episodic long-term memory is scarce. Particularly lacking is evidence regarding long retention intervals, in the order of hours and days. This is precisely the main question we address here. To approach this question, we have adopted the dual-process theory framework (e.g., Koen et al.,\nThe hypothesis that crossmodal semantic congruence at encoding will benefit later retrieval can be linked to improvements due to context congruence, extensively reported in memory literature. For example, memory for prose improves following the preactivation of related information (Bransford & Johnson,\nFor example, various crossmodal studies addressed the idea that when the different single-modality components of a crossmodal event are encoded as a unitary coherent representation, the whole multisensory representation can be later retrieved upon reencounter with just one of its original components. This hypothesis, based on Hamilton’s principle of\nEvidence showing memory improvements for semantically congruent audiovisual stimuli has been tested, using relatively short retention intervals, with the continuous recognition task (e.g., Lehmann & Murray,\nOther studies have addressed crossmodal semantic congruence effects in episodic memory, using relatively longer retention periods, larger memory loads, and sometimes intervening distractor tasks to prevent rehearsal. In a seminal study, Thompson and Paivio (\nIt is worth noting that Heikkilä et al. (\nIn all their experiments, Heikkilä et al. used only intentional, explicit study protocols. Studying incidental memory may be of particular interest, however, since it is more prevalent in everyday life routines than intentional memory, and it tends to decline more rapidly with age (Eysenck,\nMost of the studies discussed so far have used recognition (old/new) or, to a lesser extent, recall tasks focusing on the identity of the to-be-remembered object. Recent studies have reported positive effects of crossmodal congruence also for location memory (Marian et al.,\nAccording to the dual-process theory, performance on long-term memory recognition tests has been proposed to depend on two main types of retrieval processes, that rely on different networks of brain regions: recollection whereby participants retrieve qualitative information about an event, and familiarity whereby participants simply have a feeling that they have seen or heard something before not necessarily accompanied by recollection of more specific details of the context (Diana et al.,\nTherefore, one important question is whether memory benefits based on crossmodal semantic congruence could potentially affect recollection, familiarity, or both. Measuring these two different aspects of long-term memory separately is relevant on several counts. First, it can help disentangle a dual-coding account of crossmodal memory gains (Meyerhoff et al.,\nApart from the recent study by Duarte et al. (\nThe main goal this study was to address the impact of crossmodal congruence on episodic memory over long retention intervals, for images that are encoded under incidental situations. This basic question is addressed in four experiments. The main approach of Experiments\nTo address these questions, we designed an incidental encoding paradigm in which participants were asked to rate the congruence of the audio and visual components of a list of audiovisual stimuli, without knowing they would be given a memory test at a later moment. Pause et al.’s (\nThe methods, sample size estimation, hypothesis, analysis pipeline, and predictions of experiments were preregistered prior to data processing and analyses. A general description of the project and the pilot experiment can be found online (\nThis study was approved by the Institutional Committee for Ethical Review of Projects (CIREP) at the Universitat Pompeu Fabra, date 11.02.2021, ref number: 173.\nThe hypothesis addressed in Experiment\nBecause we were interested in at least a medium effect size on recollection (\nParticipants were recruited online through the Prolific platform (\nThree hundred seventy-two (372) image stimuli were selected from the THINGS database (Hebart et al.,\nDuring the encoding phase, 93 congruent and 93 incongruent audiovisual pairings were presented, in random order. Assignment of congruent and incongruent pairings of items was counterbalanced across participants. The onset of the images (4-s duration) and the corresponding sounds (2.5-s duration) were aligned. Participants were instructed to rate the congruence between image and sound on a scale from 1 to 4 (1 =\nIn the distraction phase, which was run immediately after the encoding phase, participants performed a mathematical task aimed at eliminating possible rehearsal or maintenance strategies that could affect the subsequent recognition memory test differently across participants. In this task, participants were shown pairs of numbers of up to seven digits, side by side. In each trial, they were instructed to respond with the left or right arrow keys as to which of the two numbers was larger. After each trial, feedback was presented indicating whether the participant was correct or not. There was a total of 100 trials, and this phase lasted approximately 2–3 min, including the time to read the instructions and the initial training trials. There were three previous training trials for this task with a reminder of the instruction for the task.\nIn the test phase, we presented the participants with a recognition memory task. This was done on the second day after the encoding phase, with a retention delay of approximately 48 h. All the 372 image stimuli were presented in a random order without the associated sound. This included all the images presented during the encoding phase, mixed randomly with an equal amount of new but equivalent images. Participants were instructed to respond on a confidence scale from 1 to 6, whether they thought the image had appeared during the initial encoding phase or not: 1 =\nAlthough we designed the experiment with a priori half congruent and half incongruent items as per the experimenter’s criteria, for the critical memory test comparing memory for congruent versus incongruent items, we considered only those items whose congruence had been judged accordingly by the participant’s individual ratings. Congruent items were items that were initially selected as congruent by design and that the participants rated as 3 or 4 on the 4-point scale, and incongruent items were items that were considered incongruent by design and rated 1 or 2. From here on, when we refer to congruent items, we mean items in the congruent stimulus category that had also been rated as congruent by subjects, and likewise for incongruent items. Please note that this renders a different number of trials per condition in each subject (please see Appendix 4 for descriptive statistics on trial numbers).\nData analyses followed the DPSD (Yonelinas,\nThe prediction for Experiment\nAcross all items, the average interparticipant congruence rating was 2.24 (\nOur main analysis in this experiment focused on recollection. Applying the DPSD model, the mean recollection (R parameter; see Fig.\nDistribution of individual estimates of the recollection parameter (\nLike the DPSD parameters, sensitivity to discern old items from new ones was significantly higher for events encoded as part of a congruent multisensory stimulus (Wilcoxon signed-rank test;\nThe results to emerge from Experiment\nThese results are in line with other studies that have reported gains related to crossmodal semantic congruence at encoding, compared with incongruence (Matusz et al.,\nAnother finding to emerge from this first experiment is that crossmodal congruence impacted the recollection process, hinting at another of the traditional hallmarks of episodic memory effects. One recent study has also addressed the effects of crossmodal semantic congruence adopting the DPSD model (Duarte et al.,\nIn Experiment\nWe hypothesized that in addition to any memory facilitation owing to the congruence effects of crossmodal inputs, subsequent memory may be overall decreased in the older population. The relevant question is, however, if the age-related memory decline would lead to an exacerbated crossmodal gain, given the increased potential for improvement. This pattern should be reflected in the following effects on memory performance (as measured with R, F, and\nAs for Experiment\nAcross all items, in Experiment\nBefore applying the DPSD model, we had to apply the same criteria as in Experiment\nAverage (± 95 CI) performance for (\nThe effect of congruence on\nExperiment\nUnfortunately, there is only one other study testing crossmodal semantic congruence effects in an older age group. Heikkilä et al. (\nIt is perhaps worth noting two further results emerging from Experiment\nIn the two initial experiments of this study, subjects were presented with as many crossmodal semantically congruent events as incongruent ones during the encoding session. An approximately equivalent proportion of congruent and incongruent crossmodal events is also typical of previous experiments using congruence manipulations (Craik & Tulving,\nOutside the domain of crossmodal studies, several memory phenomena attest to the importance of salience, surprisal, and higher memory for infrequent oddballs. Novelty has been noted as a likely candidate for an important determinant of later memory (Bunzeck & Düzel,\nExperiment\nFor both Experiment\nWe used the same recruitment protocol, hourly compensation, and performance criteria for inclusion of participants’ data as in the previous experiments, except for the requirement of a minimum amount of intermediate confidence responses, given the small number of trials in some conditions. All other details of the procedure are the same as in Experiment\nThe methods were the same as for Experiment\nWe conducted simulations using empirical data from the previous experiment to estimate the effect size when using a reduced set size of 22 items (incongruent condition of Experiment\nThe incongruent items in Experiment\nShapiro–Wilk tests for each combination of factor levels showed that all data conformed to normality (\nIf the effect of salience by rare/odd events is sufficiently large, then we would expect that rarer items will be better remembered than common ones. This means that incongruent items should be remembered relatively more in Experiment\nAcross all items, the average interparticipant congruence rating was 3.20 (\nAcross all items, the average interparticipant congruence rating was 1.45 (\nThe ANOVA on\nGiven the small number of trials in some of the conditions (specifically, incongruent trials in incongruent-rare Experiment\nThe ANOVA with the within-subject variable congruence and the between-subject variable distribution frequency, returned a main effect of congruence,\nThe ANOVA on F returned a main effect of congruence,\nThree conclusions emerge from the findings of the distribution manipulations across Experiments\nIn our everyday life we experience multitude of events, many of them providing complementary or coherent information through more than one sensory system. Here, we set out to understand the processes whereby this semantic coherence across the sensory modalities modulates memory for these events. Despite there being ample evidence that events whose semantic information is congruent across sensory modalities are better remembered (for reviews, see Matusz et al.,\nThe greater levels of recollection for images originally encoded with their semantically congruent sounds, compared with semantically incongruent ones, can be interpreted as a contextual effect. This hypothesis is grounded on previous evidence for a positive relationship between prior knowledge and the formation of new episodic memories, or the so-called congruence memory effect (Bein et al.,\nThe evidence that crossmodal memory gains bear the hallmarks of episodic memory effects (long retention interval, deep level of processing, incidental encoding) generally supports the multisensory memory reactivation account (see Matusz et al.,\nAlthough the main prediction of the DPSD regarding crossmodal congruence (and main focus in Experiments\nThis study also aimed to investigate possible interactions between healthy aging and memory, and the crossmodal congruence effect of congruence. We found the expected decline in overall memory performance in the older group, as reflected in\nThis study also set out to test whether increasing the salience of incongruent items by decreasing their relative proportion during the encoding phase to 10%, would cause an increase in memory performance in the incongruent condition. The issue of distribution frequency between congruent and incongruent items has been rarely addressed but it might be important on at least two counts. First, because according to some authors it could be at the base of some mixed outcomes, or even confounds, in past studies (Pecher & Zeelenberg,\nThe conclusions above are based on the analyses of\nGiven that the rather extreme distributional manipulation used here had only moderate effects on crossmodal congruence gains, one should perhaps be cautious when attributing large variations in the outcome of crossmodal effects to small changes in the distribution of the proportion of congruent and incongruent items (Thelen et al.,\nFinally, despite this study had clear outcomes and generally straightforward interpretation, it is fair to consider its limitations. First, there was a relatively high prevalence of congruent items rated as incongruent by participants (see Appendix 4). This might have been caused by the sounds being ambiguous with respect to the object category, given the large set of stimuli used. Although these stimuli were not included in the final analysis, this may have caused imbalances in the number of items for the congruent and incongruent conditions. Second, in Experiment\nIn conclusion, the present results show that images presented simultaneously with semantically congruent sounds in an incidental encoding task are recollected better and become more familiar than images presented with other sounds. The main finding of our study is that this crossmodal effect, often reported in the literature but over much shorter retention intervals of seconds or minutes, is extensive to episodic memories tested over a much longer intervals of 2 days and encoded under incidental circumstances. A second finding to emerge from this study is that rarity increases subsequent memory for infrequent stimuli, compared with more frequent stimuli. Despite this rarity effect modulates the size of the crossmodal congruence advantage, the underlying mechanisms may be different, given the different patterns regarding recollection and familiarity. Our results regarding potential differences in crossmodal congruence effects at older age are less conclusive, and did not reveal any sign of positive evidence.\nIn our view, this crossmodal memory advantage is consistent with a general framework describing a positive relationship between prior knowledge and the formation of new episodic memories, or so-called congruence memory effect (Bein et al.,\nTo increase the validity of this episodic memory test, the participants were not informed of a subsequent memory test (incidental memory task) and were only presented with each stimulus once. According to some authors (Pause et al.,\nThe successful storage and retrieval of information in long-term memory is one of the main ways we measure successful learning of academic and extra-academic materials. Especially given the relatively large effect size found in the present experiment, we propose that the understanding of how semantically congruent crossmodal stimuli can help people remember information, could be implemented in educational endeavours (see, e.g., Lidestam et al.,\nBelow is the link to the electronic supplementary material.\nSupplementary file1 (DOCX 1107 kb)", "content_for_embedding": "Someone attending a symphony concert can recognize some of the musical instruments of the orchestra by sight, and in some cases, match their characteristic sounds by ear. This process requires access to crossmodal associations acquired during a lifetime regarding which sounds correspond to which visual objects. Generally speaking, it would seem rather advantageous for humans to harness on this kind of crossmodal relations from semantic memory, both to expedite the perception of events in the immediate sensory environment (as it has been shown in multiple studies on crossmodal perception and attention; see Beauchamp et al.,\nOne particularly relevant question is to understand if and how implicit encoding of semantically congruent multisensory events improves episodic memory—that is to say, memories about crossmodal events experienced without an intentional, explicit effort made to register and retain them. Although the effects of crossmodal semantic congruence after incidental encoding have been addressed profusely, studies have almost exclusively used short-term memory protocols, and evidence regarding episodic long-term memory is scarce. Particularly lacking is evidence regarding long retention intervals, in the order of hours and days. This is precisely the main question we address here. To approach this question, we have adopted the dual-process theory framework (e.g., Koen et al.,\nThe hypothesis that crossmodal semantic congruence at encoding will benefit later retrieval can be linked to improvements due to context congruence, extensively reported in memory literature. For example, memory for prose improves following the preactivation of related information (Bransford & Johnson,\nFor example, various crossmodal studies addressed the idea that when the different single-modality components of a crossmodal event are encoded as a unitary coherent representation, the whole multisensory representation can be later retrieved upon reencounter with just one of its original components. This hypothesis, based on Hamilton’s principle of\nEvidence showing memory improvements for semantically congruent audiovisual stimuli has been tested, using relatively short retention intervals, with the continuous recognition task (e.g., Lehmann & Murray,\nOther studies have addressed crossmodal semantic congruence effects in episodic memory, using relatively longer retention periods, larger memory loads, and sometimes intervening distractor tasks to prevent rehearsal. In a seminal study, Thompson and Paivio (\nIt is worth noting that Heikkilä et al. (\nIn all their experiments, Heikkilä et al. used only intentional, explicit study protocols. Studying incidental memory may be of particular interest, however, since it is more prevalent in everyday life routines than intentional memory, and it tends to decline more rapidly with age (Eysenck,\nMost of the studies discussed so far have used recognition (old/new) or, to a lesser extent, recall tasks focusing on the identity of the to-be-remembered object. Recent studies have reported positive effects of crossmodal congruence also for location memory (Marian et al.,\nAccording to the dual-process theory, performance on long-term memory recognition tests has been proposed to depend on two main types of retrieval processes, that rely on different networks of brain regions: recollection whereby participants retrieve qualitative information about an event, and familiarity whereby participants simply have a feeling that they have seen or heard something before not necessarily accompanied by recollection of more specific details of the context (Diana et al.,\nTherefore, one important question is whether memory benefits based on crossmodal semantic congruence could potentially affect recollection, familiarity, or both. Measuring these two different aspects of long-term memory separately is relevant on several counts. First, it can help disentangle a dual-coding account of crossmodal memory gains (Meyerhoff et al.,\nApart from the recent study by Duarte et al. (\nThe main goal this study was to address the impact of crossmodal congruence on episodic memory over long retention intervals, for images that are encoded under incidental situations. This basic question is addressed in four experiments. The main approach of Experiments\nTo address these questions, we designed an incidental encoding paradigm in which participants were asked to rate the congruence of the audio and visual components of a list of audiovisual stimuli, without knowing they would be given a memory test at a later moment. Pause et al.’s (\nThe methods, sample size estimation, hypothesis, analysis pipeline, and predictions of experiments were preregistered prior to data processing and analyses. A general description of the project and the pilot experiment can be found online (\nThis study was approved by the Institutional Committee for Ethical Review of Projects (CIREP) at the Universitat Pompeu Fabra, date 11.02.2021, ref number: 173.\nThe hypothesis addressed in Experiment\nBecause we were interested in at least a medium effect size on recollection (\nParticipants were recruited online through the Prolific platform (\nThree hundred seventy-two (372) image stimuli were selected from the THINGS database (Hebart et al.,\nDuring the encoding phase, 93 congruent and 93 incongruent audiovisual pairings were presented, in random order. Assignment of congruent and incongruent pairings of items was counterbalanced across participants. The onset of the images (4-s duration) and the corresponding sounds (2.5-s duration) were aligned. Participants were instructed to rate the congruence between image and sound on a scale from 1 to 4 (1 =\nIn the distraction phase, which was run immediately after the encoding phase, participants performed a mathematical task aimed at eliminating possible rehearsal or maintenance strategies that could affect the subsequent recognition memory test differently across participants. In this task, participants were shown pairs of numbers of up to seven digits, side by side. In each trial, they were instructed to respond with the left or right arrow keys as to which of the two numbers was larger. After each trial, feedback was presented indicating whether the participant was correct or not. There was a total of 100 trials, and this phase lasted approximately 2–3 min, including the time to read the instructions and the initial training trials. There were three previous training trials for this task with a reminder of the instruction for the task.\nIn the test phase, we presented the participants with a recognition memory task. This was done on the second day after the encoding phase, with a retention delay of approximately 48 h. All the 372 image stimuli were presented in a random order without the associated sound. This included all the images presented during the encoding phase, mixed randomly with an equal amount of new but equivalent images. Participants were instructed to respond on a confidence scale from 1 to 6, whether they thought the image had appeared during the initial encoding phase or not: 1 =\nAlthough we designed the experiment with a priori half congruent and half incongruent items as per the experimenter’s criteria, for the critical memory test comparing memory for congruent versus incongruent items, we considered only those items whose congruence had been judged accordingly by the participant’s individual ratings. Congruent items were items that were initially selected as congruent by design and that the participants rated as 3 or 4 on the 4-point scale, and incongruent items were items that were considered incongruent by design and rated 1 or 2. From here on, when we refer to congruent items, we mean items in the congruent stimulus category that had also been rated as congruent by subjects, and likewise for incongruent items. Please note that this renders a different number of trials per condition in each subject (please see Appendix 4 for descriptive statistics on trial numbers).\nData analyses followed the DPSD (Yonelinas,\nThe prediction for Experiment\nAcross all items, the average interparticipant congruence rating was 2.24 (\nOur main analysis in this experiment focused on recollection. Applying the DPSD model, the mean recollection (R parameter; see Fig.\nDistribution of individual estimates of the recollection parameter (\nLike the DPSD parameters, sensitivity to discern old items from new ones was significantly higher for events encoded as part of a congruent multisensory stimulus (Wilcoxon signed-rank test;\nThe results to emerge from Experiment\nThese results are in line with other studies that have reported gains related to crossmodal semantic congruence at encoding, compared with incongruence (Matusz et al.,\nAnother finding to emerge from this first experiment is that crossmodal congruence impacted the recollection process, hinting at another of the traditional hallmarks of episodic memory effects. One recent study has also addressed the effects of crossmodal semantic congruence adopting the DPSD model (Duarte et al.,\nIn Experiment\nWe hypothesized that in addition to any memory facilitation owing to the congruence effects of crossmodal inputs, subsequent memory may be overall decreased in the older population. The relevant question is, however, if the age-related memory decline would lead to an exacerbated crossmodal gain, given the increased potential for improvement. This pattern should be reflected in the following effects on memory performance (as measured with R, F, and\nAs for Experiment\nAcross all items, in Experiment\nBefore applying the DPSD model, we had to apply the same criteria as in Experiment\nAverage (± 95 CI) performance for (\nThe effect of congruence on\nExperiment\nUnfortunately, there is only one other study testing crossmodal semantic congruence effects in an older age group. Heikkilä et al. (\nIt is perhaps worth noting two further results emerging from Experiment\nIn the two initial experiments of this study, subjects were presented with as many crossmodal semantically congruent events as incongruent ones during the encoding session. An approximately equivalent proportion of congruent and incongruent crossmodal events is also typical of previous experiments using congruence manipulations (Craik & Tulving,\nOutside the domain of crossmodal studies, several memory phenomena attest to the importance of salience, surprisal, and higher memory for infrequent oddballs. Novelty has been noted as a likely candidate for an important determinant of later memory (Bunzeck & Düzel,\nExperiment\nFor both Experiment\nWe used the same recruitment protocol, hourly compensation, and performance criteria for inclusion of participants’ data as in the previous experiments, except for the requirement of a minimum amount of intermediate confidence responses, given the small number of trials in some conditions. All other details of the procedure are the same as in Experiment\nThe methods were the same as for Experiment\nWe conducted simulations using empirical data from the previous experiment to estimate the effect size when using a reduced set size of 22 items (incongruent condition of Experiment\nThe incongruent items in Experiment\nShapiro–Wilk tests for each combination of factor levels showed that all data conformed to normality (\nIf the effect of salience by rare/odd events is sufficiently large, then we would expect that rarer items will be better remembered than common ones. This means that incongruent items should be remembered relatively more in Experiment\nAcross all items, the average interparticipant congruence rating was 3.20 (\nAcross all items, the average interparticipant congruence rating was 1.45 (\nThe ANOVA on\nGiven the small number of trials in some of the conditions (specifically, incongruent trials in incongruent-rare Experiment\nThe ANOVA with the within-subject variable congruence and the between-subject variable distribution frequency, returned a main effect of congruence,\nThe ANOVA on F returned a main effect of congruence,\nThree conclusions emerge from the findings of the distribution manipulations across Experiments\nIn our everyday life we experience multitude of events, many of them providing complementary or coherent information through more than one sensory system. Here, we set out to understand the processes whereby this semantic coherence across the sensory modalities modulates memory for these events. Despite there being ample evidence that events whose semantic information is congruent across sensory modalities are better remembered (for reviews, see Matusz et al.,\nThe greater levels of recollection for images originally encoded with their semantically congruent sounds, compared with semantically incongruent ones, can be interpreted as a contextual effect. This hypothesis is grounded on previous evidence for a positive relationship between prior knowledge and the formation of new episodic memories, or the so-called congruence memory effect (Bein et al.,\nThe evidence that crossmodal memory gains bear the hallmarks of episodic memory effects (long retention interval, deep level of processing, incidental encoding) generally supports the multisensory memory reactivation account (see Matusz et al.,\nAlthough the main prediction of the DPSD regarding crossmodal congruence (and main focus in Experiments\nThis study also aimed to investigate possible interactions between healthy aging and memory, and the crossmodal congruence effect of congruence. We found the expected decline in overall memory performance in the older group, as reflected in\nThis study also set out to test whether increasing the salience of incongruent items by decreasing their relative proportion during the encoding phase to 10%, would cause an increase in memory performance in the incongruent condition. The issue of distribution frequency between congruent and incongruent items has been rarely addressed but it might be important on at least two counts. First, because according to some authors it could be at the base of some mixed outcomes, or even confounds, in past studies (Pecher & Zeelenberg,\nThe conclusions above are based on the analyses of\nGiven that the rather extreme distributional manipulation used here had only moderate effects on crossmodal congruence gains, one should perhaps be cautious when attributing large variations in the outcome of crossmodal effects to small changes in the distribution of the proportion of congruent and incongruent items (Thelen et al.,\nFinally, despite this study had clear outcomes and generally straightforward interpretation, it is fair to consider its limitations. First, there was a relatively high prevalence of congruent items rated as incongruent by participants (see Appendix 4). This might have been caused by the sounds being ambiguous with respect to the object category, given the large set of stimuli used. Although these stimuli were not included in the final analysis, this may have caused imbalances in the number of items for the congruent and incongruent conditions. Second, in Experiment\nIn conclusion, the present results show that images presented simultaneously with semantically congruent sounds in an incidental encoding task are recollected better and become more familiar than images presented with other sounds. The main finding of our study is that this crossmodal effect, often reported in the literature but over much shorter retention intervals of seconds or minutes, is extensive to episodic memories tested over a much longer intervals of 2 days and encoded under incidental circumstances. A second finding to emerge from this study is that rarity increases subsequent memory for infrequent stimuli, compared with more frequent stimuli. Despite this rarity effect modulates the size of the crossmodal congruence advantage, the underlying mechanisms may be different, given the different patterns regarding recollection and familiarity. Our results regarding potential differences in crossmodal congruence effects at older age are less conclusive, and did not reveal any sign of positive evidence.\nIn our view, this crossmodal memory advantage is consistent with a general framework describing a positive relationship between prior knowledge and the formation of new episodic memories, or so-called congruence memory effect (Bein et al.,\nTo increase the validity of this episodic memory test, the participants were not informed of a subsequent memory test (incidental memory task) and were only presented with each stimulus once. According to some authors (Pause et al.,\nThe successful storage and retrieval of information in long-term memory is one of the main ways we measure successful learning of academic and extra-academic materials. Especially given the relatively large effect size found in the present experiment, we propose that the understanding of how semantically congruent crossmodal stimuli can help people remember information, could be implemented in educational endeavours (see, e.g., Lidestam et al.,\nBelow is the link to the electronic supplementary material.\nSupplementary file1 (DOCX 1107 kb)", "topic": "Brain"}
{"pmid": "40631142", "pmcid": "12303962", "title": "Altered functional network topology and connectivity in female nurses with shift work sleep disorder", "publication_year": "N/A", "abstract": "", "full_text": "Shift work, prevalent among healthcare workers like nurses, significantly disrupts the endogenous circadian rhythm and acts as a major risk factor for sleep disorders (\nWith rapid advancements in neuroimaging technology, recent studies have indicated that SWSD in nurses is closely associated with alterations in brain function, such as abnormal activity in the default mode network (DMN) and attention-related circuits (\nTo address this knowledge gap, we employed graph theory analysis of resting-state fMRI data to conduct a systematic investigation into the brain functional network topology of female nurses with SWSD. Our study was guided by three primary hypotheses: (1) that female nurses with SWSD would exhibit disrupted brain network organization, manifesting as reduced global and local efficiency compared to HCs; (2) that SWSD would be associated with altered functional connectivity, especially weakened connections within and between key networks like the DMN, visual network (VN), and limbic network (LN); and (3) that these neuroimaging-derived network metrics would correlate with clinical measures of sleep disturbance. By elucidating the neurobiological underpinnings of SWSD, this study aimed to advance our understanding of its pathophysiology and inform the development of novel strategies for prevention and intervention. A flowchart detailing the research process is presented in\nThis study recruited female nurses from the Yancheng School of Clinical Medicine, Nanjing Medical University from May to July 2024. To minimize the acute effects of recent shift work and to capture the chronic neurobiological alterations associated with SWSD, all participants were scanned on a scheduled day off between 6:00 PM and 9:00 PM. Prior to the scan, participants were instructed to lie still in a supine position with their eyes closed, remain awake, and avoid systematic thinking, allowing their minds to wander freely. Foam padding was used to minimize head motion. The resting-state scan was part of a broader imaging protocol, and the scanning conditions were kept consistent for all participants. Inclusion criteria for the SWSD group (\nThe required sample size was determined by an\nPrior to MRI scanning, general demographic information and clinical data were collected, including age, years of education, Beck Anxiety Inventory (BAI) scores, Beck Depression Inventory-II (BDI-II) scores, PSQI scores, and Insomnia Severity Index (ISI) scores.\nRs-fMRI and structural 3D T1-weighted images were acquired using a 3.0 Tesla MRI scanner equipped with a 24-channel head coil (Discovery 750w, GE, United States) at the Yancheng School of Clinical Medicine of Nanjing Medical University (Imaging parameters are provided in the\nResting-state fMRI preprocessing: The rs-fMRI data were preprocessed using SPM12 and Data Processing and Analysis for Brain Imaging (DPABI) implemented in MATLAB (R2018b) (\nGraph theoretical analysis of brain network characteristics was performed using the GRETNA software (\nFC networks were constructed using the graph theoretical network analysis toolbox (GRETNA) (\nDifferences in FC between brain regions were analyzed using the connection module of the GRETNA software. Group differences in functional connections were identified using a two-sample\nSPSS 27.0 software was used for statistical analysis. First, normality tests were conducted for age, years of education, PSQI scores, ISI scores, TIV, BAI scores, and BDI-II scores. For normally distributed measurement data, independent sample\nTwo-sample\nDemographic and clinical characteristics of the HCs (\nDemographic and clinical features of all participants.\nBAI, Beck Anxiety Inventory; BDI, Beck Depression Inventory; FC, functional connectivity; F, female; HCs, healthy controls; ISI, Insomnia Severity Index; L, left; M, male; PSQI, Pittsburgh Sleep Quality Index; R, right; SWSD, shift work sleep disorder; TIV, total intracranial volume. Bold values indicate statistical significance (\nGlobal network metrics for the SWSD group and HCs were calculated and compared, utilizing the AUC for each global property (\nGroup comparisons of AUC values of global network properties.\nAUC, area under the curve; C\nAlterations in global network metrics. Compared with HCs, the C\nCompared with the HCs, the NCP of the bilateral calcarine cortex, bilateral lingual gyrus, right cuneus and left superior occipital gyrus was significantly decreased in the SWSD group (all\nNodes showing significant differences in NCP between nurses with SWSD and HCs. The blue circles represent a higher NCP in HCs than in nurses with SWSD (all\nNodes showing significant differences in NLE between nurses with SWSD and HCs. The blue circles represent a higher NLE in HCs than in nurses with SWSD (all\nCompared to HCs, the SWSD group showed significantly altered FC (all\nAltered functional network connectivity.\nANG, angular gyrus; CAL, calcarine cortex; CAU, caudate nucleus; CUN, cuneus; DMN, default mode network; FFG, fusiform gyrus; FPN, frontoparietal network; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; LN, limbic network; LING, lingual gyrus; MFG, middle frontal gyrus; MOG, middle occipital gyrus; OLF, olfactory cortex; ORBsup, superior orbitofrontal gyrus; PCUN, precuneus; PreCG, precentral gyrus; REC, rectus gyrus; SFGmed, superior frontal gyrus, medial part; SMN, somatomotor network; SN, subcortical network; SOG, superior occipital gyrus; VN, visual network; VAN, ventral attention network.\nAlterations in brain functional network connectivity between nurses with SWSD and HCs. Nodes represent specific brain regions grouped by functional networks, including SMN, DMN, VN, VAN, FPN, SN, and LN. Edges indicate significant changes in functional connectivity between nurses with SWSD and HCs, with edge colors reflecting the direction and magnitude of\nAs illustrated in\nCorrelation analysis.\nUsing graph theoretical analysis, this study investigated alterations in brain functional network topology and subnetwork connectivity in nurses with SWSD relative to HCs. These network alterations were subsequently correlated with clinical sleep variables, specifically the PSQI and ISI. The principal findings were as follows: (1) global network metrics, including E\nThe intricate structural and functional organization of the brain arises from the topological configurations of neuronal clusters (\nPrevious extensive research indicates that various sleep disorders exhibit alterations in global brain network topology, including obstructive sleep apnea (OSA) (\nRegarding nodal network metrics, our analysis revealed a significant decrease in NCP in the bilateral calcarine cortex, the bilateral lingual gyrus, and the left superior occipital gyrus compared to HCs. Concurrently, NLE was also significantly reduced in this group, specifically in the bilateral calcarine cortex, bilateral lingual gyrus, right cuneus, and right caudate nucleus. These aforementioned visual network regions (bilateral calcarine cortex, bilateral lingual gyrus, left superior occipital gyrus, and right cuneus) are key components of the visual system (\nBeyond alterations in topological properties, this study also revealed a complex subnetwork of altered FC within and between large-scale brain networks (SMN, SN, VN, LN, FPN, DMN), characterized by both abnormally strong and weak connections. In SWSD, abnormally strong connections may reflect the brain’s compensatory efforts to counteract inefficiencies or a dysregulated hyperconnective state, while weakened connections likely indicate reduced information transfer efficiency or impaired integration between regions (\nBeyond these intra-network disruptions, sleep disorders also profoundly affect FC between distinct brain networks. These disorders disrupt VN interactions with the DMN (\nThe clinical relevance of these network alterations is underscored by our correlation analyses. First, a negative correlation was observed between PSQI scores and NLE in the right cuneus. The cuneus, a core region of the occipital visual cortex, is involved in primary visual information processing, visuospatial processing, and visual attention (\nAlthough this study provided valuable insights into the neural underpinnings of SWSD in nurses, there are several limitations that should be considered. First, our study did not incorporate subjective measures to assess daytime sleepiness (e.g., the Epworth sleepiness scale) or circadian chronotype (e.g., the morningness–eveningness questionnaire). These clinical scales would have allowed us to directly correlate behavioral and circadian phenotypes with the observed alterations in brain networks, thereby enhancing the clinical relevance of our findings. Future studies should incorporate these assessments to build a more comprehensive brain-behavior model of SWSD. Second, the cross-sectional design of our study limits causal inference. It remains unclear whether the observed brain changes are a cause or a consequence of SWSD. A longitudinal study would be invaluable, not only to elucidate the direction of causality but also to map the temporal dynamics of these network alterations. Such a design would enable us to determine, for instance, whether these changes are exacerbated by continued shift work, fluctuate with schedule modifications, or can be ameliorated through intervention. Third, the study focused exclusively on female nurses aged 20–40 years, which may limit the generalizability of the findings to other populations, such as male nurses or nurses in different age groups. Importantly, this precludes the exploration of how SWSD-related brain changes may vary across the lifespan. Future studies should include more diverse samples, particularly across a wider age range and in other populations (e.g., male shift workers), to assess the generalizability of these findings. Fourth, the assessment of sleep disorders and psychological status relied on self-report questionnaires (PSQI, ISI, BAI, and BDI-II), which are subject to recall bias and subjective interpretation, although these are standard instruments in the field. Objective measures of sleep, such as actigraphy or polysomnography, could provide more robust data in future research. Fifth, the focus of the present study is on the topological properties and functional network connectivity in nurses with SWSD. However, this approach cannot resolve the directionality of influence between brain regions. To address this limitation, future investigations should employ methods designed to assess effective connectivity, such as Multivariate granger causality. By utilizing a larger sample and multivariate Granger causality analysis, such studies could elucidate the dynamics of directed functional influence and information flow within and between networks, providing a more comprehensive understanding of how SWSD reconfigures the brain’s communication architecture. Sixth, a further limitation is our inability to perform a stratified analysis by shift work duration. Although we collected these data, the resulting subgroups (e.g., 1–5 years,\nIn conclusion, this study demonstrates that SWSD in female nurses is characterized by significant alterations in brain functional network topology. Key among these are widespread reductions in both global and nodal network efficiency, particularly within visual processing regions and the caudate nucleus, alongside a complex pattern of disrupted (primarily weakened) FC across multiple brain networks. Crucially, these neuroimaging changes correlated significantly with clinical measures of insomnia severity and sleep quality. Identifying these topological and FC alterations advances our understanding of the pathophysiological mechanisms underlying SWSD and provides novel insights for potential prevention and intervention strategies.", "content_for_embedding": "Shift work, prevalent among healthcare workers like nurses, significantly disrupts the endogenous circadian rhythm and acts as a major risk factor for sleep disorders (\nWith rapid advancements in neuroimaging technology, recent studies have indicated that SWSD in nurses is closely associated with alterations in brain function, such as abnormal activity in the default mode network (DMN) and attention-related circuits (\nTo address this knowledge gap, we employed graph theory analysis of resting-state fMRI data to conduct a systematic investigation into the brain functional network topology of female nurses with SWSD. Our study was guided by three primary hypotheses: (1) that female nurses with SWSD would exhibit disrupted brain network organization, manifesting as reduced global and local efficiency compared to HCs; (2) that SWSD would be associated with altered functional connectivity, especially weakened connections within and between key networks like the DMN, visual network (VN), and limbic network (LN); and (3) that these neuroimaging-derived network metrics would correlate with clinical measures of sleep disturbance. By elucidating the neurobiological underpinnings of SWSD, this study aimed to advance our understanding of its pathophysiology and inform the development of novel strategies for prevention and intervention. A flowchart detailing the research process is presented in\nThis study recruited female nurses from the Yancheng School of Clinical Medicine, Nanjing Medical University from May to July 2024. To minimize the acute effects of recent shift work and to capture the chronic neurobiological alterations associated with SWSD, all participants were scanned on a scheduled day off between 6:00 PM and 9:00 PM. Prior to the scan, participants were instructed to lie still in a supine position with their eyes closed, remain awake, and avoid systematic thinking, allowing their minds to wander freely. Foam padding was used to minimize head motion. The resting-state scan was part of a broader imaging protocol, and the scanning conditions were kept consistent for all participants. Inclusion criteria for the SWSD group (\nThe required sample size was determined by an\nPrior to MRI scanning, general demographic information and clinical data were collected, including age, years of education, Beck Anxiety Inventory (BAI) scores, Beck Depression Inventory-II (BDI-II) scores, PSQI scores, and Insomnia Severity Index (ISI) scores.\nRs-fMRI and structural 3D T1-weighted images were acquired using a 3.0 Tesla MRI scanner equipped with a 24-channel head coil (Discovery 750w, GE, United States) at the Yancheng School of Clinical Medicine of Nanjing Medical University (Imaging parameters are provided in the\nResting-state fMRI preprocessing: The rs-fMRI data were preprocessed using SPM12 and Data Processing and Analysis for Brain Imaging (DPABI) implemented in MATLAB (R2018b) (\nGraph theoretical analysis of brain network characteristics was performed using the GRETNA software (\nFC networks were constructed using the graph theoretical network analysis toolbox (GRETNA) (\nDifferences in FC between brain regions were analyzed using the connection module of the GRETNA software. Group differences in functional connections were identified using a two-sample\nSPSS 27.0 software was used for statistical analysis. First, normality tests were conducted for age, years of education, PSQI scores, ISI scores, TIV, BAI scores, and BDI-II scores. For normally distributed measurement data, independent sample\nTwo-sample\nDemographic and clinical characteristics of the HCs (\nDemographic and clinical features of all participants.\nBAI, Beck Anxiety Inventory; BDI, Beck Depression Inventory; FC, functional connectivity; F, female; HCs, healthy controls; ISI, Insomnia Severity Index; L, left; M, male; PSQI, Pittsburgh Sleep Quality Index; R, right; SWSD, shift work sleep disorder; TIV, total intracranial volume. Bold values indicate statistical significance (\nGlobal network metrics for the SWSD group and HCs were calculated and compared, utilizing the AUC for each global property (\nGroup comparisons of AUC values of global network properties.\nAUC, area under the curve; C\nAlterations in global network metrics. Compared with HCs, the C\nCompared with the HCs, the NCP of the bilateral calcarine cortex, bilateral lingual gyrus, right cuneus and left superior occipital gyrus was significantly decreased in the SWSD group (all\nNodes showing significant differences in NCP between nurses with SWSD and HCs. The blue circles represent a higher NCP in HCs than in nurses with SWSD (all\nNodes showing significant differences in NLE between nurses with SWSD and HCs. The blue circles represent a higher NLE in HCs than in nurses with SWSD (all\nCompared to HCs, the SWSD group showed significantly altered FC (all\nAltered functional network connectivity.\nANG, angular gyrus; CAL, calcarine cortex; CAU, caudate nucleus; CUN, cuneus; DMN, default mode network; FFG, fusiform gyrus; FPN, frontoparietal network; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; LN, limbic network; LING, lingual gyrus; MFG, middle frontal gyrus; MOG, middle occipital gyrus; OLF, olfactory cortex; ORBsup, superior orbitofrontal gyrus; PCUN, precuneus; PreCG, precentral gyrus; REC, rectus gyrus; SFGmed, superior frontal gyrus, medial part; SMN, somatomotor network; SN, subcortical network; SOG, superior occipital gyrus; VN, visual network; VAN, ventral attention network.\nAlterations in brain functional network connectivity between nurses with SWSD and HCs. Nodes represent specific brain regions grouped by functional networks, including SMN, DMN, VN, VAN, FPN, SN, and LN. Edges indicate significant changes in functional connectivity between nurses with SWSD and HCs, with edge colors reflecting the direction and magnitude of\nAs illustrated in\nCorrelation analysis.\nUsing graph theoretical analysis, this study investigated alterations in brain functional network topology and subnetwork connectivity in nurses with SWSD relative to HCs. These network alterations were subsequently correlated with clinical sleep variables, specifically the PSQI and ISI. The principal findings were as follows: (1) global network metrics, including E\nThe intricate structural and functional organization of the brain arises from the topological configurations of neuronal clusters (\nPrevious extensive research indicates that various sleep disorders exhibit alterations in global brain network topology, including obstructive sleep apnea (OSA) (\nRegarding nodal network metrics, our analysis revealed a significant decrease in NCP in the bilateral calcarine cortex, the bilateral lingual gyrus, and the left superior occipital gyrus compared to HCs. Concurrently, NLE was also significantly reduced in this group, specifically in the bilateral calcarine cortex, bilateral lingual gyrus, right cuneus, and right caudate nucleus. These aforementioned visual network regions (bilateral calcarine cortex, bilateral lingual gyrus, left superior occipital gyrus, and right cuneus) are key components of the visual system (\nBeyond alterations in topological properties, this study also revealed a complex subnetwork of altered FC within and between large-scale brain networks (SMN, SN, VN, LN, FPN, DMN), characterized by both abnormally strong and weak connections. In SWSD, abnormally strong connections may reflect the brain’s compensatory efforts to counteract inefficiencies or a dysregulated hyperconnective state, while weakened connections likely indicate reduced information transfer efficiency or impaired integration between regions (\nBeyond these intra-network disruptions, sleep disorders also profoundly affect FC between distinct brain networks. These disorders disrupt VN interactions with the DMN (\nThe clinical relevance of these network alterations is underscored by our correlation analyses. First, a negative correlation was observed between PSQI scores and NLE in the right cuneus. The cuneus, a core region of the occipital visual cortex, is involved in primary visual information processing, visuospatial processing, and visual attention (\nAlthough this study provided valuable insights into the neural underpinnings of SWSD in nurses, there are several limitations that should be considered. First, our study did not incorporate subjective measures to assess daytime sleepiness (e.g., the Epworth sleepiness scale) or circadian chronotype (e.g., the morningness–eveningness questionnaire). These clinical scales would have allowed us to directly correlate behavioral and circadian phenotypes with the observed alterations in brain networks, thereby enhancing the clinical relevance of our findings. Future studies should incorporate these assessments to build a more comprehensive brain-behavior model of SWSD. Second, the cross-sectional design of our study limits causal inference. It remains unclear whether the observed brain changes are a cause or a consequence of SWSD. A longitudinal study would be invaluable, not only to elucidate the direction of causality but also to map the temporal dynamics of these network alterations. Such a design would enable us to determine, for instance, whether these changes are exacerbated by continued shift work, fluctuate with schedule modifications, or can be ameliorated through intervention. Third, the study focused exclusively on female nurses aged 20–40 years, which may limit the generalizability of the findings to other populations, such as male nurses or nurses in different age groups. Importantly, this precludes the exploration of how SWSD-related brain changes may vary across the lifespan. Future studies should include more diverse samples, particularly across a wider age range and in other populations (e.g., male shift workers), to assess the generalizability of these findings. Fourth, the assessment of sleep disorders and psychological status relied on self-report questionnaires (PSQI, ISI, BAI, and BDI-II), which are subject to recall bias and subjective interpretation, although these are standard instruments in the field. Objective measures of sleep, such as actigraphy or polysomnography, could provide more robust data in future research. Fifth, the focus of the present study is on the topological properties and functional network connectivity in nurses with SWSD. However, this approach cannot resolve the directionality of influence between brain regions. To address this limitation, future investigations should employ methods designed to assess effective connectivity, such as Multivariate granger causality. By utilizing a larger sample and multivariate Granger causality analysis, such studies could elucidate the dynamics of directed functional influence and information flow within and between networks, providing a more comprehensive understanding of how SWSD reconfigures the brain’s communication architecture. Sixth, a further limitation is our inability to perform a stratified analysis by shift work duration. Although we collected these data, the resulting subgroups (e.g., 1–5 years,\nIn conclusion, this study demonstrates that SWSD in female nurses is characterized by significant alterations in brain functional network topology. Key among these are widespread reductions in both global and nodal network efficiency, particularly within visual processing regions and the caudate nucleus, alongside a complex pattern of disrupted (primarily weakened) FC across multiple brain networks. Crucially, these neuroimaging changes correlated significantly with clinical measures of insomnia severity and sleep quality. Identifying these topological and FC alterations advances our understanding of the pathophysiological mechanisms underlying SWSD and provides novel insights for potential prevention and intervention strategies.", "topic": "Brain"}
{"pmid": "40573479", "pmcid": "12305689", "title": "Radiation-induced changes of reactive astrocyte distribution in mice as a late response to partial-brain proton irradiation", "publication_year": "N/A", "abstract": "", "full_text": "Proton therapy is an increasingly accessible option for highly conformal radiation therapy. The dose of protons increases with increasing depth in tissue and reaches a maximum – the Bragg peak – shortly before the protons stop. Bragg peaks are typically positioned in and adjacent to the target volume, resulting in a substantial reduction in integral dose to healthy tissue. Nevertheless, late radiation-induced side effects were observed months or years after treatment for brain tumor patients as image changes in magnetic resonance imaging (MRI) that could not be explained by the physical dose distribution alone [\nCurrent investigations focus on understanding the development of these lesions and identifying markers for prognostic or therapeutic applications. This requires insights into the tissue changes down to the cellular level with respect to the spatial distribution of induced reactions through experimental in vivo data, as these cannot be gained from human patients. A recently published multimodal dataset involving partial-brain irradiation of mice reproduced the MRI image changes observed in patients [\nAstrocytes are the most abundant glial cells and have important roles during the brain’s development, its homeostasis, and response to injury [\nThe aim of this study was to investigate dose dependence and spatial distribution of reactive astrocytes within mouse brains after focused proton beam irradiation by analyzing brain slices stained for GFAP.\nImages of mouse brain slices originated from a previous study and animal procedures and histological staining protocols are described elsewhere in detail [\nIn this study, four mice from the C57BL/6JRj strain were considered from the dataset: one mouse per prescription dose level (45 Gy, 65 Gy, and 85 Gy) and a sham-irradiated animal. For each mouse, three mid-brain GFAP slices were considered, graded with quality scores of 4–5 and containing the isodose line of 80% of the maximum dose.\nThe simulated dose distribution was spatially correlated and upscaled into the coordinate system of each individual brain slice [\n(a) Cone-beam computed tomography image of mouse head with overlaid simulated dose distribution as isodose lines (dose levels corresponding to 20%, 40%, 60%, and 80% of 45 Gy prescription dose). (b) Brain slice image for the same mouse stained with glial fibrillary acidic protein (GFAP, green) and 4´,6-diamidino-2-phenylindole (DAPI, blue) overlaid with the registered proton dose distribution. The mouse sketch on the bottom indicates the relative orientation of proton beam and mouse. (c, e, and g) Representative images of three tiles (250 μm × 250 μm) from different slices stained for GFAP and DAPI. (d, f, and h) GFAP overlaid with segmentation results for the same tiles. (c) and (d) originate from the cortex region of a sham-irradiated mouse. (e) and (f) show a blood vessel aligned with astrocytes in the region of interest (ROI) of another slice from the sham-irradiated mouse. (g) and (h) originate from the right ROI of an 85 Gy irradiated mouse. White arrows indicate false-positive staining of erythrocytes in the vessels. Red arrows indicate potential false-positive autofluorescent segments. Scale bars: (a–b) 2 mm; (c–h) 50 μm.\nFor GFAP segmentation, microscopy image preprocessing was done using Fiji (version 2.16.0) [\nTo analyze the response of the GFAP signal to radiation, a rectangular region of interest (ROI) of approximately 2.4 mm × 2.4 mm was defined in both the right and left hemispheres of the brain slices, centered along the beam axis (\nExemplary histograms representing the fraction of glial fibrillary acidic protein (GFAP)-positive area per 250 μm × 250 μm tile (color bar) with delineated regions of interest (ROIs) in the right (blue) and left (orange) hemisphere: (a) sham-irradiated and (b) 65 Gy mouse. (c) Mean GFAP-positive area fraction (GFAP+ fraction) of the right and left ROIs for different levels of prescription dose, averaged over three slices per dose level with one standard deviation. Dashed lines show results from linear regression.\nThe spatial variation of the GFAP+ fraction and the simulated dose distribution were compared along the beam direction. For this purpose, the mean values were calculated for all tiles perpendicular to the beam direction within the width of the right ROI (i.e., between the blue dashed lines in\n(a) Representative histogram showing the fraction of glial fibrillary acidic protein (GFAP)-positive area per 250 μm × 250 μm tile (color bar) for a 45 Gy mouse brain slice. Dashed lines indicate integration intervals for the dose and GFAP profile along the beam axis (blue, dashed) and in two areas transverse to the beam axis, which represent the cerebral cortex (red, dotted) and the central brain region (yellow, dash-dotted). (b) Transverse dose and GFAP response profiles in cortex (red) and central brain region (yellow). Depth-dose and GFAP response profiles (blue, dashed) for (c) the 45 Gy and (d) a sham-irradiated mouse brain slice. Background colors indicate anatomical regions. Note that the x-axis of (c) and the y-axis of (b) correspond to the respective x- and y-axes of (a).\nTo investigate how anatomical regions influence the dose response transverse to the beam direction, the tile values along the beam direction were averaged separately within the cerebral cortex (red dotted lines) and in the depth of the target region (central brain, yellow dash-dotted lines).\nGFAP-positive areas were segmented on nine irradiated and three sham-irradiated slices.\nFor the sham-irradiated mouse, the distribution of the GFAP+ fraction was symmetrical between both hemispheres (\nIn general, GFAP expression and its increase with dose were not evenly distributed in the brain slices, with a marked increase in GFAP at some anatomical boundaries (\nThis study investigated the impact of focused proton dose on the spatial distribution of GFAP+ fraction in mice that developed late radiation-induced image changes on MRI due to BBB disruption. Analyses of whole-brain histology revealed a significant increase in GFAP expression on the irradiated hemisphere that followed the spatial distribution of the proton dose in the brain and scaled linearly with prescription dose, while no significant increase in GFAP expression was observed on the contralateral hemisphere. In addition to dose, the respective anatomical region had an impact on the observed level of GFAP+ fraction. In particular, the cerebral cortex showed a lower GFAP expression without irradiation and a less pronounced increase with dose.\nThe known correlation of the quantity of GFAP expression in reactive astrocytes with the severity of injury [\nEarlier work using a single X-ray fraction showed that higher doses (20–45 Gy) led to increased astrogliosis, which persisted for 1 year, whereas lower doses did not affect the number of GFAP-expressing astrocytes [\nIn line with the current work, cortical astrocytes in healthy mouse brains have a lower GFAP content than hippocampal astrocytes [\nA major limitation of this exploratory study is the small animal cohort, with only one mouse per dose group, and thus, the results have to be validated using a bigger sample size. Moreover, the evaluated regions only approximately corresponded to the respective anatomical regions. For example, the regions considered to be cortex also included the anatomical boundary between cortex and hippocampus, resulting in a local peak of GFAP+ fraction near the boundary. In a future study, brain region atlas-based registration could enable improved delineation of anatomy [\nThe presented method enabled segmentation of GFAP-positive cells. While the thin histological slices and the elongated nature of astrocytes prevented direct counting of astrocyte numbers, the GFAP+ fraction of a tile can be considered as a measure of astrogliosis. False-positive signals originating from blood cells, for example, erythrocytes, can be mitigated in future experiments, for example, by co-staining with markers such as Glycophorin A or using antibodies with different wavelengths.\nIn conclusion, a method for GFAP segmentation in whole-brain slices was presented, enabling spatial investigation of astrocyte response to localized radiation-induced brain damage at the microscopic level. The GFAP expression in mice that developed BBB disruption after focused irradiation linearly increased in the high-dose region with the prescription dose. The combination of the presented method with further markers (e.g. microglia) and inclusion of different timepoints and larger sample size will allow comprehensive understanding of the cellular response and regional differences of focal radiation-induced late effects, enabling the development of new therapeutic and preventive strategies.", "content_for_embedding": "Proton therapy is an increasingly accessible option for highly conformal radiation therapy. The dose of protons increases with increasing depth in tissue and reaches a maximum – the Bragg peak – shortly before the protons stop. Bragg peaks are typically positioned in and adjacent to the target volume, resulting in a substantial reduction in integral dose to healthy tissue. Nevertheless, late radiation-induced side effects were observed months or years after treatment for brain tumor patients as image changes in magnetic resonance imaging (MRI) that could not be explained by the physical dose distribution alone [\nCurrent investigations focus on understanding the development of these lesions and identifying markers for prognostic or therapeutic applications. This requires insights into the tissue changes down to the cellular level with respect to the spatial distribution of induced reactions through experimental in vivo data, as these cannot be gained from human patients. A recently published multimodal dataset involving partial-brain irradiation of mice reproduced the MRI image changes observed in patients [\nAstrocytes are the most abundant glial cells and have important roles during the brain’s development, its homeostasis, and response to injury [\nThe aim of this study was to investigate dose dependence and spatial distribution of reactive astrocytes within mouse brains after focused proton beam irradiation by analyzing brain slices stained for GFAP.\nImages of mouse brain slices originated from a previous study and animal procedures and histological staining protocols are described elsewhere in detail [\nIn this study, four mice from the C57BL/6JRj strain were considered from the dataset: one mouse per prescription dose level (45 Gy, 65 Gy, and 85 Gy) and a sham-irradiated animal. For each mouse, three mid-brain GFAP slices were considered, graded with quality scores of 4–5 and containing the isodose line of 80% of the maximum dose.\nThe simulated dose distribution was spatially correlated and upscaled into the coordinate system of each individual brain slice [\n(a) Cone-beam computed tomography image of mouse head with overlaid simulated dose distribution as isodose lines (dose levels corresponding to 20%, 40%, 60%, and 80% of 45 Gy prescription dose). (b) Brain slice image for the same mouse stained with glial fibrillary acidic protein (GFAP, green) and 4´,6-diamidino-2-phenylindole (DAPI, blue) overlaid with the registered proton dose distribution. The mouse sketch on the bottom indicates the relative orientation of proton beam and mouse. (c, e, and g) Representative images of three tiles (250 μm × 250 μm) from different slices stained for GFAP and DAPI. (d, f, and h) GFAP overlaid with segmentation results for the same tiles. (c) and (d) originate from the cortex region of a sham-irradiated mouse. (e) and (f) show a blood vessel aligned with astrocytes in the region of interest (ROI) of another slice from the sham-irradiated mouse. (g) and (h) originate from the right ROI of an 85 Gy irradiated mouse. White arrows indicate false-positive staining of erythrocytes in the vessels. Red arrows indicate potential false-positive autofluorescent segments. Scale bars: (a–b) 2 mm; (c–h) 50 μm.\nFor GFAP segmentation, microscopy image preprocessing was done using Fiji (version 2.16.0) [\nTo analyze the response of the GFAP signal to radiation, a rectangular region of interest (ROI) of approximately 2.4 mm × 2.4 mm was defined in both the right and left hemispheres of the brain slices, centered along the beam axis (\nExemplary histograms representing the fraction of glial fibrillary acidic protein (GFAP)-positive area per 250 μm × 250 μm tile (color bar) with delineated regions of interest (ROIs) in the right (blue) and left (orange) hemisphere: (a) sham-irradiated and (b) 65 Gy mouse. (c) Mean GFAP-positive area fraction (GFAP+ fraction) of the right and left ROIs for different levels of prescription dose, averaged over three slices per dose level with one standard deviation. Dashed lines show results from linear regression.\nThe spatial variation of the GFAP+ fraction and the simulated dose distribution were compared along the beam direction. For this purpose, the mean values were calculated for all tiles perpendicular to the beam direction within the width of the right ROI (i.e., between the blue dashed lines in\n(a) Representative histogram showing the fraction of glial fibrillary acidic protein (GFAP)-positive area per 250 μm × 250 μm tile (color bar) for a 45 Gy mouse brain slice. Dashed lines indicate integration intervals for the dose and GFAP profile along the beam axis (blue, dashed) and in two areas transverse to the beam axis, which represent the cerebral cortex (red, dotted) and the central brain region (yellow, dash-dotted). (b) Transverse dose and GFAP response profiles in cortex (red) and central brain region (yellow). Depth-dose and GFAP response profiles (blue, dashed) for (c) the 45 Gy and (d) a sham-irradiated mouse brain slice. Background colors indicate anatomical regions. Note that the x-axis of (c) and the y-axis of (b) correspond to the respective x- and y-axes of (a).\nTo investigate how anatomical regions influence the dose response transverse to the beam direction, the tile values along the beam direction were averaged separately within the cerebral cortex (red dotted lines) and in the depth of the target region (central brain, yellow dash-dotted lines).\nGFAP-positive areas were segmented on nine irradiated and three sham-irradiated slices.\nFor the sham-irradiated mouse, the distribution of the GFAP+ fraction was symmetrical between both hemispheres (\nIn general, GFAP expression and its increase with dose were not evenly distributed in the brain slices, with a marked increase in GFAP at some anatomical boundaries (\nThis study investigated the impact of focused proton dose on the spatial distribution of GFAP+ fraction in mice that developed late radiation-induced image changes on MRI due to BBB disruption. Analyses of whole-brain histology revealed a significant increase in GFAP expression on the irradiated hemisphere that followed the spatial distribution of the proton dose in the brain and scaled linearly with prescription dose, while no significant increase in GFAP expression was observed on the contralateral hemisphere. In addition to dose, the respective anatomical region had an impact on the observed level of GFAP+ fraction. In particular, the cerebral cortex showed a lower GFAP expression without irradiation and a less pronounced increase with dose.\nThe known correlation of the quantity of GFAP expression in reactive astrocytes with the severity of injury [\nEarlier work using a single X-ray fraction showed that higher doses (20–45 Gy) led to increased astrogliosis, which persisted for 1 year, whereas lower doses did not affect the number of GFAP-expressing astrocytes [\nIn line with the current work, cortical astrocytes in healthy mouse brains have a lower GFAP content than hippocampal astrocytes [\nA major limitation of this exploratory study is the small animal cohort, with only one mouse per dose group, and thus, the results have to be validated using a bigger sample size. Moreover, the evaluated regions only approximately corresponded to the respective anatomical regions. For example, the regions considered to be cortex also included the anatomical boundary between cortex and hippocampus, resulting in a local peak of GFAP+ fraction near the boundary. In a future study, brain region atlas-based registration could enable improved delineation of anatomy [\nThe presented method enabled segmentation of GFAP-positive cells. While the thin histological slices and the elongated nature of astrocytes prevented direct counting of astrocyte numbers, the GFAP+ fraction of a tile can be considered as a measure of astrogliosis. False-positive signals originating from blood cells, for example, erythrocytes, can be mitigated in future experiments, for example, by co-staining with markers such as Glycophorin A or using antibodies with different wavelengths.\nIn conclusion, a method for GFAP segmentation in whole-brain slices was presented, enabling spatial investigation of astrocyte response to localized radiation-induced brain damage at the microscopic level. The GFAP expression in mice that developed BBB disruption after focused irradiation linearly increased in the high-dose region with the prescription dose. The combination of the presented method with further markers (e.g. microglia) and inclusion of different timepoints and larger sample size will allow comprehensive understanding of the cellular response and regional differences of focal radiation-induced late effects, enabling the development of new therapeutic and preventive strategies.", "topic": "Brain"}
{"pmid": "40512723", "pmcid": "12293658", "title": "Association of the Middle Accessory Cerebral Artery With Bihemispheric Anterior Cerebral Artery and Median Artery of the Corpus Callosum", "publication_year": "2025", "abstract": "As part of a microsurgical anatomy study of the recurrent artery of Heubner (RAH), we identified three rare vascular variants in a 45-year-old female cadaver: the right middle accessory cerebral artery (MACA), bihemispheric anterior cerebral artery (Bihem-ACA), and median artery of the corpus callosum (MACC). These anomalies were documented through meticulous dissection and detailed morphometric analysis, underscoring the value of cadaveric studies in elucidating complex cerebral vascular anatomy. The specimen was obtained within 24 hours postmortem. The cerebral arteries were injected with red latex and fixed using a standard protocol: initial perfusion with formaldehyde, followed by immersion in 10% formalin for a total fixation period of two months. Dissection was performed under a Carl Zeiss OPMI™ surgical microscope (Carl Zeiss Meditec AG, Jena, Germany) at 6x-40x magnification. Morphometric measurements were taken using a digital Mitutoyo vernier caliper (Mitutoyo South Asia Pvt. Ltd., New Delhi, India) with 0.01 mm resolution. During dissection, the right MACA was observed originating from the anterior communicating artery (ACoA), proximal to the junction of the A1 segments. It coursed posteriorly, running parallel to the right pre-communicating segment (A1) of the ACA and the sphenoidal (M1) segment of the middle cerebral artery (MCA). This artery measured 67 mm in length and 0.6 mm in diameter, with two perforating branches arising approximately 25 mm from its origin. The right A1 (d-A1) segment was dominant, measuring 14 mm in length, with a 1.5 mm diameter and an 8 mm outer perimeter, continuing distally as a single Bihem-ACA. This trunk extended 27 mm before bifurcating, with a proximal diameter of 1.7 mm and an outer perimeter of 9 mm. In contrast, the left A1 segment was hypoplastic, measuring 16.72 mm in length, 1 mm in diameter, and 5.5 mm in outer perimeter. The MACC originated from the left A3 segment of the ACA and measured 2.1 mm in diameter and 40 mm in length. It gave rise to two right cingulate-callosal branches (0.5 mm and 0.4 mm in diameter, respectively) directed toward the callosal sulcus and cingulate gyrus. These findings highlight an unusual combination of vascular variants not commonly observed together in a single individual. Recognizing such configurations is crucial for surgical planning, particularly in procedures involving the interhemispheric fissure, anterior communicating complex, and corpus callosum. Enhanced anatomical knowledge of these rare variants supports safer neurosurgical interventions and improves intraoperative decision-making in cerebrovascular procedures.", "full_text": "The cerebral vasculature is highly complex and variable, posing significant challenges and opportunities for neurosurgeons and radiologists. Among its critical components, the anterior cerebral artery (ACA) and its branches are vital for supplying blood to the medial and superior parts of the frontal lobes, the corpus callosum, and the cingulate gyrus. Variations in ACA anatomy can significantly impact surgical planning and the management of cerebrovascular diseases [\nThe ACA typically arises from the bifurcation of the internal carotid artery (ICA), near the base of the Sylvian fissure and lateral to the optic chiasm. From there, it travels anteromedially, passing over the optic nerve or chiasm, and enters the interhemispheric fissure. Within this region, it connects with the opposite ACA through the anterior communicating artery (ACoA), forming what is known as the A1 segment [\nThe middle cerebral artery (MCA), initially described by Lautard in 1892 and later renamed by Looten in 1906, also exhibits significant anatomical variations. These include accessory arteries and duplications, which can complicate diagnostic imaging and surgical intervention [\nWe present a rare instance of combined vascular anomalies involving the right middle accessory cerebral artery (MACA), bihemispheric anterior cerebral artery (Bihem-ACA), and median artery of the corpus callosum (MACC). These findings were observed during the microsurgical anatomical study of the recurrent artery of Heubner (RAH). The specimen, a 45-year-old female cadaver, revealed these rare vascular formations, each meticulously measured and documented. The right MACA was noted to originate from the ACoA, running parallel to the pre-communicant segment of the right ACA and the sphenoidal segment of the right MCA, with significant morphometric details recorded.\nOur findings underscore the importance of detailed anatomical studies and cadaveric dissections in accurately identifying and understanding these variations. Cadaveric studies are indispensable for visualizing the complex anatomy of cerebral arteries and planning neurosurgical interventions [\nSpecimen preparation\nA brain specimen was obtained from the Microsurgical Anatomy of the Central Nervous System Laboratory at the Faculty of Medicine, National Autonomous University of Mexico (UNAM), Mexico City, Mexico. The donor was a 45-year-old female cadaver with a postmortem interval of less than 24 hours and no history of neurological disease.\nVascular injection and fixation protocol\nTo preserve the cerebral vascular architecture for microanatomical analysis, a standardized seven-step protocol was followed.\nExtraction\nThe brain was carefully removed from the cranial cavity, ensuring minimal disruption to the vasculature. Cranial nerves, arteries, veins, and venous sinuses were transected close to the skull base. Key vascular structures, including the ICAs, vertebral arteries (V4 segment), and major venous sinuses, were preserved where possible for accurate anatomical correlation.\nVascular Washout\nThe right ICA and right vertebral artery were catheterized using 5 French (Fr) catheters. The contralateral vessels were ligated. A saline solution was gently perfused to remove residual blood and thrombi, preventing obstruction of vascular injection. Sites of leakage, particularly near the internal auditory artery, were identified and sealed to ensure complete system perfusion.\nPerfusion Fixation\nTo achieve optimal tissue preservation and maintain vessel integrity, 15 mL of pure formaldehyde was perfused through both catheterized arteries for five minutes. The brain was then immersed in 3 liters of 10% buffered formalin for 15 minutes, followed by an additional 1-liter perfusion through the ICA while maintaining an open vertebral artery. This protocol ensured thorough fixation of intracerebral and extracerebral vessels.\nLatex Injection\nAfter fixation, the brain was removed and prepared for contrast injection. A white latex compound (Poliformas Plásticas®, Mexico City, Mexico) mixed with carmine 319 acrylic paint (Politec®, Mexico City, Mexico) was used to enhance vascular visibility. Fifteen milliliters of the mixture were injected through the vertebral artery and 20 milliliters through the ICA. Excess latex was carefully washed off to prevent contamination of the leptomeninges.\nPost-injection Immersion Fixation\nThe latex-injected brain was submerged in fresh 10% formalin for 24 hours. The solution was then replaced, and the specimen remained immersed for an additional two-month fixation period to ensure complete tissue stabilization prior to dissection.\nFinal Preservation\nAfter fixation, the brain was rinsed for 24 hours under running water to remove formalin residues and subsequently preserved in a 60% isopropyl alcohol solution to maintain tissue pliability and color contrast during dissection.\nDissection and morphometric analysis\nMicrosurgical dissection was performed under a Carl Zeiss OPMI™ surgical microscope (Carl Zeiss Meditec AG, Jena, Germany) at magnifications ranging from 6x to 40x. Instruments included microsurgical scissors and No. 3 watchmaker’s forceps. The cerebral arterial system was carefully isolated, and each identified vessel variant was documented with high-resolution digital photography.\nMorphometric measurements such as arterial diameter, length, and branching angles were obtained using a Mitutoyo digital Vernier caliper (Model CD-8” CX; Mitutoyo Corporation, Kanagawa, Japan) with a resolution of 0.01 mm. Measurements were taken in situ, ensuring anatomical context relative to cerebral landmarks such as the optic chiasm, corpus callosum, anterior perforated substance, and interhemispheric fissure.\nMeasurements\nMeasurements of the artery's diameter, length, and distances from key anatomical landmarks used for dissection, such as the ICA, anterior perforated substance, and ACoA, were taken using a Mitutoyo digital Vernier caliper (Model CD-8” CX, resolution of 0.0005”/0.01mm, Mitutoyo South Asia Pvt. Ltd., New Delhi, India). Specific measurements included diameter at origin, length from origin to bifurcation, and diameters of medial and lateral branches.\nDistance From ICA Origin to the Posterior Clinoid Process and Morphometric Analysis\nThe number of perforating branches and their diameters were recorded. Detailed morphometric descriptions of segments A1, a-MCA, Bihem-ACA, right A1 (d-A1), right M1 (d-M1), right M2 bifurcation (d-M2B), right ICA (d-ICA), right olfactory bulb (d-OB), right orbitofrontal artery (d-OFA), left A1 (i-A1), RAH, fronto-basal trunk (FBT), and MACC were performed.\nData recording and documentation\nHigh-resolution digital photographs were captured using an Olympus™ μ DIGITAL 800 8.0-megapixel camera (Olympus Corporation, Tokyo, Japan). All findings were meticulously recorded and compared with existing literature to highlight variations and clinical implications [\nResults\nA 45-year-old female brain specimen weighing 1,250 grams was obtained. The A1 segment of the ACA originated bilaterally from the medial face of the ICA, ventral to the olfactory trigone. After a brief horizontal course, it deviated anteromedially, entering rostrally to the chiasm towards the interhemispheric fissure, where it joined its counterpart to form a single arterial trunk (unique A2). The right A1 segment (A1-d) was dominant, with a length of 14 mm, an outer perimeter of 8 mm, and a diameter of 1.5 mm. The left A1 segment (A1-I) was hypoplastic with a length of 16.72 mm, a diameter of 1 mm, and an outer perimeter of 5.5 mm (Figures\nAPS: anterior perforated substance; d-A1: right A1; d-M1: right M1; d-M2B: right M2 bifurcation; d-ICA: right internal carotid artery; d-OB: right olfactory bulb; d-OFA: right orbitofrontal artery; i-A1: left A1; RAH: recurrent artery of Heubner; FBT: fronto-basal trunk\nd-a1: right A1; i-A1: left A1; RAH: recurrent artery of Heubner; i-A2: left A2; d-A2: right A2; FBT: fronto-basal trunk; OFA: orbitofrontal artery; FPA: frontopolar artery\nd-a1: right A1; i-A1: left A1; i-A2: left A2; d-A2: right A2; CC: corpus callosum; RAH: recurrent artery of Heubner; FBT: fronto-basal trunk; OFA: orbitofrontal artery; FPA: frontopolar artery; a-AIFA: anterior internal frontal artery\nR-CC: right corpus callosum\nThe perforating branches originating from the lateral edge of the A1 segment supplied the rostrum of the corpus callosum, lamina terminalis, middle part of the anterior commissure, optic chiasm, and paraolfactory area. The number of perforating branches in d-A1 was 12, with an average diameter of 0.005-1.90 mm, while i-A1 had 18 branches with an average diameter of 0.006-1.10 mm.\nThe duplicated left RAH originated from the lateral edge of A2, directed towards the anterior perforated substance, with a diameter of 1.90 mm and a length of 30.5 mm. A common trunk and bifurcation were observed 9 mm from its origin. On the right side, a-MCA and d-MCA originated at the junction of both precommunicating segments (A1) of the ACA, running retrogradely parallel to the d-A1 segment and then to the sphenoidal segment (M1) of the d-MCA, towards the orbital surface of the frontal lobe. Its length was 67 mm and its diameter was 0.6 mm. During its course, two branches towards the anterior perforated substance were observed 25 mm from its origin, measuring 0.3 mm and 0.4 mm, respectively (Figure\nThe ACoA was not found; instead, a hypoplastic i-A1 was observed with a diameter of 4 mm and a length of 16.72 mm from the bifurcation of the left carotid artery to the junction with the medial edge of A1-d, where its course ended before giving rise to a common trunk for the left RAH and the left fronto-basal artery (FBA). The FBA originated from the left orbito-frontal artery (i-OFA) and the left fronto-polar artery (i-FPA), the latter irrigating the medial surface of the frontal lobe and giving a branch to the medial surface of the contralateral frontal lobe. Its length from the origin of the FBA to d-A1 was 8 mm with a diameter of 1 mm. The segment from the origin of the FBA to the junction of i-A1 with d-A1 presented three perforating branches with a diameter of 0.7 mm (Figure\nThe unique A2 segment began after the union of left A1 (i-A1) to the medial edge of the dominant d-A1, at which point it was named the right Bihem-ACA. It headed towards the interhemispheric fissure, entering the cistern of the corpus callosum, where it coursed rostrally to the lamina terminalis, continuing as segment A3, anterior to the genu of the corpus callosum. Segment A3 of the Bihem-ACA bifurcated into a right A3 (d-A3) and a left A3 (i-A3), with a length of 30 mm and a diameter of 1.5 mm, later giving rise to the pericallosal artery and the callosomarginal artery (CaM) located on the trunk of the CC. The right Bihem-ACA had a length of 27 mm before bifurcation, with an origin diameter of 1.7 mm (Figure\nFrom its i-A3 segment, the right Bihem-ACA gave rise to the MACC, with an origin diameter of 2.1 mm and a length of 40 mm, which originated two right cingulate-callosal branches with diameters of 0.5 mm and 0.4 mm, directed towards the callosal sulcus and cingulate gyrus. The A3 segment bilaterally originated the CaMs, with a diameter of 4.3 mm for the right CaM and 3.5 mm for the left CaM (and the diameters of the pericallosal arteries) (Figure\nThe ACA, a terminal branch of the ICA, plays a pivotal role in supplying the medial and superior portions of the frontal lobes, the corpus callosum, and the cingulate gyrus. Anatomically, the ACA courses anterior to the optic chiasm and olfactory trigone, entering the interhemispheric fissure after joining its contralateral counterpart via the ACoA [\nIn our study, we documented three rare vascular anomalies: a right MACA, a Bihem-ACA, and a MACC. These findings provide a unique anatomical arrangement not previously described in a single specimen and have notable implications for cerebrovascular surgery and neuroimaging.\nThe ACA’s cortical branches-orbital, frontal, and parietal-exhibit a well-documented degree of variability, supplying key regions including the olfactory tract, cingulate gyrus, and precuneus [\nOur observation of a Bihem-ACA, with a dominant d-A1 segment and a hypoplastic i-A1, aligns with previously reported morphological variants. Brandt’s morphometric study identified such asymmetry in up to 21.5% of cases, with left-sided hypoplasia being more frequent [\nThe MACA in our case originated from the ACoA region and coursed parallel to the M1 segment of the MCA. According to Yasargil and Crompton, such arteries are typically classified as a-MCA when arising from the ACA and as d-MCAs when originating from the ICA [\nAlthough our findings were based on a single cadaveric specimen, they highlight critical anatomical nuances relevant to aneurysm surgery, stroke management, and tumor resections involving the corpus callosum and interhemispheric fissure. For example, the presence of a Bihem-ACA can influence collateral circulation dynamics, while the MACC may complicate access to pericallosal or CaMs during surgery. These observations underscore the importance of preoperative vascular imaging and suggest the potential utility of 3D printing technologies to create individualized surgical planning models, particularly when such rare variants are suspected.\nImportance of the study\nUnderstanding ACA variations is crucial for neurosurgeons and radiologists. The ACA supplies critical brain areas, including the medial and superior parts of the frontal lobes, the corpus callosum, and the cingulate gyrus. Anatomical variations can significantly influence surgical planning and the management of cerebrovascular diseases, aiding in avoiding surgical complications and optimizing patient outcomes. Understanding ACA variations is essential for optimizing surgical outcomes in cerebrovascular procedures, and recognizing the anatomical and functional complexity of the region is equally critical, given its key role in motor coordination and its implications in neurological disorders such as hypertrophic olivary degeneration. This study emphasizes the frequency and types of ACA variations, underscoring the necessity for precise preoperative imaging and careful intraoperative navigation [\nAnatomical variations and clinical implications\nThe ACA exhibits several anatomical variants that can impact cerebral pathology presentation and management. Common variations include the absence, bifurcation, or hypoplasia of the A1 segment and the presence of accessory or azygos ACAs. These can alter cerebral hemodynamics, increasing the risk of aneurysm formation at the ACoA complex [\nFor instance, a hypoplastic A1 segment can lead to compensatory increased blood flow through the contralateral ACA, predisposing to aneurysm formation due to altered hemodynamic stress [\nVariations and neurosurgical approaches\nIn aneurysm contexts, ACA anatomical variations are particularly significant. The anterior communicating artery is a common aneurysm site, and variations such as hypoplasia or aplasia of the A1 segment affect both presentation and surgical or endovascular treatment approaches. Preoperative recognition of these variations through advanced imaging is essential to minimize complications during aneurysm clipping or coiling [\nFor glioma treatment, especially in the frontal lobe or corpus callosum, knowledge of ACA variations is vital. Surgical resection of gliomas requires precise navigation to avoid damaging vital vascular structures. Variations like accessory ACA or azygos ACA pose challenges in tumor resection while preserving adequate brain blood supply. Intraoperative mapping and neuro-navigation systems incorporating detailed vascular anatomy help achieve maximal tumor resection with minimal neurological deficits [\nImplications for stroke and other cerebrovascular diseases\nThe ischemic damage in the ACA territory may extend via diaschisis to deeper structures, including the brainstem [\nEnhancing preoperative planning\nFor neurosurgeons, detailed anatomical information from cerebral angiography is invaluable for preoperative planning. Understanding the exact location and configuration of vascular anomalies helps strategize surgical approaches, minimizing intraoperative risks and enhancing the likelihood of successful outcomes. For aneurysm repair, knowing the precise anatomy of the anterior communicating artery complex and its variations guides the surgical approach, reducing complication risks [\nRisk assessment and management\nAnatomical variations in cerebral vasculature significantly impact cerebrovascular disease risk profiles. Cerebral angiography aids early detection of these variations, enabling proactive risk assessment and management. For instance, individuals with hypoplastic A1 segments or azygos ACA may have increased aneurysm formation risks due to altered hemodynamics. Identifying these variations through angiography allows tailored surveillance and intervention strategies, potentially preventing catastrophic events [\nImplications for endovascular procedures\nIn endovascular procedures like stenting and coiling, cerebral angiography is indispensable. It provides a roadmap for navigating complex vascular terrain, ensuring precisely targeted interventions. The ability to visualize small, otherwise occult branches, like the a-MCA or MACC, ensures effective and safe endovascular treatments, avoiding inadvertent damage to vital structures [\nEducational value and training\nCerebral angiography holds significant educational value for neurosurgeons and radiologists in training. Exposing trainees to anatomical variations enhances understanding and prepares them for diverse clinical scenarios. This hands-on experience with real-world anatomical diversity is crucial for developing skills and confidence to manage complex cerebrovascular conditions [\nEmbryological development and variability\nThe embryological development of the ACA as a single interhemispheric plexus differentiating into distinct branches results in wide-ranging vascular configurations, from typical patterns to rare anomalies like the Bihem-ACA or median callosal artery [\nIntegration of 3D printing in neurosurgical planning\n3D printing technology has revolutionized neurosurgery, offering powerful tools for preoperative planning and patient-specific surgical simulations. Accurate, tangible models of the brain's vascular structures, including the rare ACA variations documented in this study, enable better visualization of complex anatomical relationships and more precise planning [\nLimitations of the study\nDespite the valuable insights provided by this study on the rare vascular variants involving the right MCA, Bihem-ACA, and MACC, several limitations must be acknowledged.\nSample Size and Generalizability\nFurther studies involving larger and more diverse sample sizes are necessary to validate these findings and determine their prevalence across different demographic groups [\nMethodological Constraints\nThe methodology employed, including the injection of red latex and fixation with 10% formalin, while standard, may introduce artifacts that could affect the anatomical measurements. The process of dissection under a surgical microscope, though precise, is still subject to human error. Additionally, the use of a digital vernier caliper for measurements, although accurate, might not capture the finest nuances in vascular dimensions, potentially leading to slight inaccuracies [\nAnatomical Variations\nThe study focuses on specific vascular variations within a single individual, which may not capture the full spectrum of anatomical diversity. Vascular anatomy is highly variable, and the presence of unique anomalies in one specimen might not reflect common patterns observed in the general population. This limitation underscores the need for more extensive studies to identify the range and frequency of these variations [\nLack of Functional Correlation\nAnother significant limitation is the lack of correlation with functional or clinical outcomes. While the anatomical descriptions are detailed, the study does not provide information on how these variations might impact cerebral function or predispose individuals to specific pathologies. Future studies should aim to correlate anatomical variations with clinical data to better understand their implications in neurological conditions such as aneurysms, gliomas, and strokes [\nLack of Comparative Analysis\nOne limitation of the study is the absence of a comparative analysis with other specimens or established reference standards. Incorporating such comparisons would offer a more meaningful context for interpreting these rare anatomical variations, helping to clarify how they differ from more common configurations. Comparative research could also shed light on whether these anomalies are isolated findings or part of a wider pattern of vascular variability and limited technological integration.\nWhile the study provides a detailed anatomical account, it does not leverage advanced imaging technologies such as high-resolution MRI or CT angiography, which could offer more precise and comprehensive visualization of vascular structures. Incorporating these technologies in future studies would enhance the accuracy of anatomical descriptions and allow for non-invasive correlation with clinical conditions [\nPotential impact on clinical practice\nDespite these limitations, the findings of this study have important implications for clinical practice, particularly in neurosurgery and radiology. Understanding these rare anatomical variations can aid in preoperative planning, improve surgical outcomes, and reduce the risk of complications. However, the clinical relevance of these findings needs to be validated through further research involving larger cohorts and diverse populations [\nA thorough examination of ACA anatomical variations offers valuable insights for clinical practice. These differences play a crucial role in how conditions like aneurysms, gliomas, strokes, and other cerebrovascular disorders are managed. Utilizing advanced imaging technologies alongside innovations like 3D printing, paired with a deep understanding of anatomical variations, is key to successful treatment planning and better patient outcomes. Moving forward, ongoing research should continue to investigate how these variations impact clinical care and work toward creating clear guidelines for managing them in different neurological scenarios.\nRecognizing and understanding ACA anatomical differences is more than just an academic exercise; it’s essential for real-world applications. As this study shows, these variations can directly influence the success of neurosurgical procedures and the treatment of cerebrovascular conditions.", "content_for_embedding": "The cerebral vasculature is highly complex and variable, posing significant challenges and opportunities for neurosurgeons and radiologists. Among its critical components, the anterior cerebral artery (ACA) and its branches are vital for supplying blood to the medial and superior parts of the frontal lobes, the corpus callosum, and the cingulate gyrus. Variations in ACA anatomy can significantly impact surgical planning and the management of cerebrovascular diseases [\nThe ACA typically arises from the bifurcation of the internal carotid artery (ICA), near the base of the Sylvian fissure and lateral to the optic chiasm. From there, it travels anteromedially, passing over the optic nerve or chiasm, and enters the interhemispheric fissure. Within this region, it connects with the opposite ACA through the anterior communicating artery (ACoA), forming what is known as the A1 segment [\nThe middle cerebral artery (MCA), initially described by Lautard in 1892 and later renamed by Looten in 1906, also exhibits significant anatomical variations. These include accessory arteries and duplications, which can complicate diagnostic imaging and surgical intervention [\nWe present a rare instance of combined vascular anomalies involving the right middle accessory cerebral artery (MACA), bihemispheric anterior cerebral artery (Bihem-ACA), and median artery of the corpus callosum (MACC). These findings were observed during the microsurgical anatomical study of the recurrent artery of Heubner (RAH). The specimen, a 45-year-old female cadaver, revealed these rare vascular formations, each meticulously measured and documented. The right MACA was noted to originate from the ACoA, running parallel to the pre-communicant segment of the right ACA and the sphenoidal segment of the right MCA, with significant morphometric details recorded.\nOur findings underscore the importance of detailed anatomical studies and cadaveric dissections in accurately identifying and understanding these variations. Cadaveric studies are indispensable for visualizing the complex anatomy of cerebral arteries and planning neurosurgical interventions [\nSpecimen preparation\nA brain specimen was obtained from the Microsurgical Anatomy of the Central Nervous System Laboratory at the Faculty of Medicine, National Autonomous University of Mexico (UNAM), Mexico City, Mexico. The donor was a 45-year-old female cadaver with a postmortem interval of less than 24 hours and no history of neurological disease.\nVascular injection and fixation protocol\nTo preserve the cerebral vascular architecture for microanatomical analysis, a standardized seven-step protocol was followed.\nExtraction\nThe brain was carefully removed from the cranial cavity, ensuring minimal disruption to the vasculature. Cranial nerves, arteries, veins, and venous sinuses were transected close to the skull base. Key vascular structures, including the ICAs, vertebral arteries (V4 segment), and major venous sinuses, were preserved where possible for accurate anatomical correlation.\nVascular Washout\nThe right ICA and right vertebral artery were catheterized using 5 French (Fr) catheters. The contralateral vessels were ligated. A saline solution was gently perfused to remove residual blood and thrombi, preventing obstruction of vascular injection. Sites of leakage, particularly near the internal auditory artery, were identified and sealed to ensure complete system perfusion.\nPerfusion Fixation\nTo achieve optimal tissue preservation and maintain vessel integrity, 15 mL of pure formaldehyde was perfused through both catheterized arteries for five minutes. The brain was then immersed in 3 liters of 10% buffered formalin for 15 minutes, followed by an additional 1-liter perfusion through the ICA while maintaining an open vertebral artery. This protocol ensured thorough fixation of intracerebral and extracerebral vessels.\nLatex Injection\nAfter fixation, the brain was removed and prepared for contrast injection. A white latex compound (Poliformas Plásticas®, Mexico City, Mexico) mixed with carmine 319 acrylic paint (Politec®, Mexico City, Mexico) was used to enhance vascular visibility. Fifteen milliliters of the mixture were injected through the vertebral artery and 20 milliliters through the ICA. Excess latex was carefully washed off to prevent contamination of the leptomeninges.\nPost-injection Immersion Fixation\nThe latex-injected brain was submerged in fresh 10% formalin for 24 hours. The solution was then replaced, and the specimen remained immersed for an additional two-month fixation period to ensure complete tissue stabilization prior to dissection.\nFinal Preservation\nAfter fixation, the brain was rinsed for 24 hours under running water to remove formalin residues and subsequently preserved in a 60% isopropyl alcohol solution to maintain tissue pliability and color contrast during dissection.\nDissection and morphometric analysis\nMicrosurgical dissection was performed under a Carl Zeiss OPMI™ surgical microscope (Carl Zeiss Meditec AG, Jena, Germany) at magnifications ranging from 6x to 40x. Instruments included microsurgical scissors and No. 3 watchmaker’s forceps. The cerebral arterial system was carefully isolated, and each identified vessel variant was documented with high-resolution digital photography.\nMorphometric measurements such as arterial diameter, length, and branching angles were obtained using a Mitutoyo digital Vernier caliper (Model CD-8” CX; Mitutoyo Corporation, Kanagawa, Japan) with a resolution of 0.01 mm. Measurements were taken in situ, ensuring anatomical context relative to cerebral landmarks such as the optic chiasm, corpus callosum, anterior perforated substance, and interhemispheric fissure.\nMeasurements\nMeasurements of the artery's diameter, length, and distances from key anatomical landmarks used for dissection, such as the ICA, anterior perforated substance, and ACoA, were taken using a Mitutoyo digital Vernier caliper (Model CD-8” CX, resolution of 0.0005”/0.01mm, Mitutoyo South Asia Pvt. Ltd., New Delhi, India). Specific measurements included diameter at origin, length from origin to bifurcation, and diameters of medial and lateral branches.\nDistance From ICA Origin to the Posterior Clinoid Process and Morphometric Analysis\nThe number of perforating branches and their diameters were recorded. Detailed morphometric descriptions of segments A1, a-MCA, Bihem-ACA, right A1 (d-A1), right M1 (d-M1), right M2 bifurcation (d-M2B), right ICA (d-ICA), right olfactory bulb (d-OB), right orbitofrontal artery (d-OFA), left A1 (i-A1), RAH, fronto-basal trunk (FBT), and MACC were performed.\nData recording and documentation\nHigh-resolution digital photographs were captured using an Olympus™ μ DIGITAL 800 8.0-megapixel camera (Olympus Corporation, Tokyo, Japan). All findings were meticulously recorded and compared with existing literature to highlight variations and clinical implications [\nResults\nA 45-year-old female brain specimen weighing 1,250 grams was obtained. The A1 segment of the ACA originated bilaterally from the medial face of the ICA, ventral to the olfactory trigone. After a brief horizontal course, it deviated anteromedially, entering rostrally to the chiasm towards the interhemispheric fissure, where it joined its counterpart to form a single arterial trunk (unique A2). The right A1 segment (A1-d) was dominant, with a length of 14 mm, an outer perimeter of 8 mm, and a diameter of 1.5 mm. The left A1 segment (A1-I) was hypoplastic with a length of 16.72 mm, a diameter of 1 mm, and an outer perimeter of 5.5 mm (Figures\nAPS: anterior perforated substance; d-A1: right A1; d-M1: right M1; d-M2B: right M2 bifurcation; d-ICA: right internal carotid artery; d-OB: right olfactory bulb; d-OFA: right orbitofrontal artery; i-A1: left A1; RAH: recurrent artery of Heubner; FBT: fronto-basal trunk\nd-a1: right A1; i-A1: left A1; RAH: recurrent artery of Heubner; i-A2: left A2; d-A2: right A2; FBT: fronto-basal trunk; OFA: orbitofrontal artery; FPA: frontopolar artery\nd-a1: right A1; i-A1: left A1; i-A2: left A2; d-A2: right A2; CC: corpus callosum; RAH: recurrent artery of Heubner; FBT: fronto-basal trunk; OFA: orbitofrontal artery; FPA: frontopolar artery; a-AIFA: anterior internal frontal artery\nR-CC: right corpus callosum\nThe perforating branches originating from the lateral edge of the A1 segment supplied the rostrum of the corpus callosum, lamina terminalis, middle part of the anterior commissure, optic chiasm, and paraolfactory area. The number of perforating branches in d-A1 was 12, with an average diameter of 0.005-1.90 mm, while i-A1 had 18 branches with an average diameter of 0.006-1.10 mm.\nThe duplicated left RAH originated from the lateral edge of A2, directed towards the anterior perforated substance, with a diameter of 1.90 mm and a length of 30.5 mm. A common trunk and bifurcation were observed 9 mm from its origin. On the right side, a-MCA and d-MCA originated at the junction of both precommunicating segments (A1) of the ACA, running retrogradely parallel to the d-A1 segment and then to the sphenoidal segment (M1) of the d-MCA, towards the orbital surface of the frontal lobe. Its length was 67 mm and its diameter was 0.6 mm. During its course, two branches towards the anterior perforated substance were observed 25 mm from its origin, measuring 0.3 mm and 0.4 mm, respectively (Figure\nThe ACoA was not found; instead, a hypoplastic i-A1 was observed with a diameter of 4 mm and a length of 16.72 mm from the bifurcation of the left carotid artery to the junction with the medial edge of A1-d, where its course ended before giving rise to a common trunk for the left RAH and the left fronto-basal artery (FBA). The FBA originated from the left orbito-frontal artery (i-OFA) and the left fronto-polar artery (i-FPA), the latter irrigating the medial surface of the frontal lobe and giving a branch to the medial surface of the contralateral frontal lobe. Its length from the origin of the FBA to d-A1 was 8 mm with a diameter of 1 mm. The segment from the origin of the FBA to the junction of i-A1 with d-A1 presented three perforating branches with a diameter of 0.7 mm (Figure\nThe unique A2 segment began after the union of left A1 (i-A1) to the medial edge of the dominant d-A1, at which point it was named the right Bihem-ACA. It headed towards the interhemispheric fissure, entering the cistern of the corpus callosum, where it coursed rostrally to the lamina terminalis, continuing as segment A3, anterior to the genu of the corpus callosum. Segment A3 of the Bihem-ACA bifurcated into a right A3 (d-A3) and a left A3 (i-A3), with a length of 30 mm and a diameter of 1.5 mm, later giving rise to the pericallosal artery and the callosomarginal artery (CaM) located on the trunk of the CC. The right Bihem-ACA had a length of 27 mm before bifurcation, with an origin diameter of 1.7 mm (Figure\nFrom its i-A3 segment, the right Bihem-ACA gave rise to the MACC, with an origin diameter of 2.1 mm and a length of 40 mm, which originated two right cingulate-callosal branches with diameters of 0.5 mm and 0.4 mm, directed towards the callosal sulcus and cingulate gyrus. The A3 segment bilaterally originated the CaMs, with a diameter of 4.3 mm for the right CaM and 3.5 mm for the left CaM (and the diameters of the pericallosal arteries) (Figure\nThe ACA, a terminal branch of the ICA, plays a pivotal role in supplying the medial and superior portions of the frontal lobes, the corpus callosum, and the cingulate gyrus. Anatomically, the ACA courses anterior to the optic chiasm and olfactory trigone, entering the interhemispheric fissure after joining its contralateral counterpart via the ACoA [\nIn our study, we documented three rare vascular anomalies: a right MACA, a Bihem-ACA, and a MACC. These findings provide a unique anatomical arrangement not previously described in a single specimen and have notable implications for cerebrovascular surgery and neuroimaging.\nThe ACA’s cortical branches-orbital, frontal, and parietal-exhibit a well-documented degree of variability, supplying key regions including the olfactory tract, cingulate gyrus, and precuneus [\nOur observation of a Bihem-ACA, with a dominant d-A1 segment and a hypoplastic i-A1, aligns with previously reported morphological variants. Brandt’s morphometric study identified such asymmetry in up to 21.5% of cases, with left-sided hypoplasia being more frequent [\nThe MACA in our case originated from the ACoA region and coursed parallel to the M1 segment of the MCA. According to Yasargil and Crompton, such arteries are typically classified as a-MCA when arising from the ACA and as d-MCAs when originating from the ICA [\nAlthough our findings were based on a single cadaveric specimen, they highlight critical anatomical nuances relevant to aneurysm surgery, stroke management, and tumor resections involving the corpus callosum and interhemispheric fissure. For example, the presence of a Bihem-ACA can influence collateral circulation dynamics, while the MACC may complicate access to pericallosal or CaMs during surgery. These observations underscore the importance of preoperative vascular imaging and suggest the potential utility of 3D printing technologies to create individualized surgical planning models, particularly when such rare variants are suspected.\nImportance of the study\nUnderstanding ACA variations is crucial for neurosurgeons and radiologists. The ACA supplies critical brain areas, including the medial and superior parts of the frontal lobes, the corpus callosum, and the cingulate gyrus. Anatomical variations can significantly influence surgical planning and the management of cerebrovascular diseases, aiding in avoiding surgical complications and optimizing patient outcomes. Understanding ACA variations is essential for optimizing surgical outcomes in cerebrovascular procedures, and recognizing the anatomical and functional complexity of the region is equally critical, given its key role in motor coordination and its implications in neurological disorders such as hypertrophic olivary degeneration. This study emphasizes the frequency and types of ACA variations, underscoring the necessity for precise preoperative imaging and careful intraoperative navigation [\nAnatomical variations and clinical implications\nThe ACA exhibits several anatomical variants that can impact cerebral pathology presentation and management. Common variations include the absence, bifurcation, or hypoplasia of the A1 segment and the presence of accessory or azygos ACAs. These can alter cerebral hemodynamics, increasing the risk of aneurysm formation at the ACoA complex [\nFor instance, a hypoplastic A1 segment can lead to compensatory increased blood flow through the contralateral ACA, predisposing to aneurysm formation due to altered hemodynamic stress [\nVariations and neurosurgical approaches\nIn aneurysm contexts, ACA anatomical variations are particularly significant. The anterior communicating artery is a common aneurysm site, and variations such as hypoplasia or aplasia of the A1 segment affect both presentation and surgical or endovascular treatment approaches. Preoperative recognition of these variations through advanced imaging is essential to minimize complications during aneurysm clipping or coiling [\nFor glioma treatment, especially in the frontal lobe or corpus callosum, knowledge of ACA variations is vital. Surgical resection of gliomas requires precise navigation to avoid damaging vital vascular structures. Variations like accessory ACA or azygos ACA pose challenges in tumor resection while preserving adequate brain blood supply. Intraoperative mapping and neuro-navigation systems incorporating detailed vascular anatomy help achieve maximal tumor resection with minimal neurological deficits [\nImplications for stroke and other cerebrovascular diseases\nThe ischemic damage in the ACA territory may extend via diaschisis to deeper structures, including the brainstem [\nEnhancing preoperative planning\nFor neurosurgeons, detailed anatomical information from cerebral angiography is invaluable for preoperative planning. Understanding the exact location and configuration of vascular anomalies helps strategize surgical approaches, minimizing intraoperative risks and enhancing the likelihood of successful outcomes. For aneurysm repair, knowing the precise anatomy of the anterior communicating artery complex and its variations guides the surgical approach, reducing complication risks [\nRisk assessment and management\nAnatomical variations in cerebral vasculature significantly impact cerebrovascular disease risk profiles. Cerebral angiography aids early detection of these variations, enabling proactive risk assessment and management. For instance, individuals with hypoplastic A1 segments or azygos ACA may have increased aneurysm formation risks due to altered hemodynamics. Identifying these variations through angiography allows tailored surveillance and intervention strategies, potentially preventing catastrophic events [\nImplications for endovascular procedures\nIn endovascular procedures like stenting and coiling, cerebral angiography is indispensable. It provides a roadmap for navigating complex vascular terrain, ensuring precisely targeted interventions. The ability to visualize small, otherwise occult branches, like the a-MCA or MACC, ensures effective and safe endovascular treatments, avoiding inadvertent damage to vital structures [\nEducational value and training\nCerebral angiography holds significant educational value for neurosurgeons and radiologists in training. Exposing trainees to anatomical variations enhances understanding and prepares them for diverse clinical scenarios. This hands-on experience with real-world anatomical diversity is crucial for developing skills and confidence to manage complex cerebrovascular conditions [\nEmbryological development and variability\nThe embryological development of the ACA as a single interhemispheric plexus differentiating into distinct branches results in wide-ranging vascular configurations, from typical patterns to rare anomalies like the Bihem-ACA or median callosal artery [\nIntegration of 3D printing in neurosurgical planning\n3D printing technology has revolutionized neurosurgery, offering powerful tools for preoperative planning and patient-specific surgical simulations. Accurate, tangible models of the brain's vascular structures, including the rare ACA variations documented in this study, enable better visualization of complex anatomical relationships and more precise planning [\nLimitations of the study\nDespite the valuable insights provided by this study on the rare vascular variants involving the right MCA, Bihem-ACA, and MACC, several limitations must be acknowledged.\nSample Size and Generalizability\nFurther studies involving larger and more diverse sample sizes are necessary to validate these findings and determine their prevalence across different demographic groups [\nMethodological Constraints\nThe methodology employed, including the injection of red latex and fixation with 10% formalin, while standard, may introduce artifacts that could affect the anatomical measurements. The process of dissection under a surgical microscope, though precise, is still subject to human error. Additionally, the use of a digital vernier caliper for measurements, although accurate, might not capture the finest nuances in vascular dimensions, potentially leading to slight inaccuracies [\nAnatomical Variations\nThe study focuses on specific vascular variations within a single individual, which may not capture the full spectrum of anatomical diversity. Vascular anatomy is highly variable, and the presence of unique anomalies in one specimen might not reflect common patterns observed in the general population. This limitation underscores the need for more extensive studies to identify the range and frequency of these variations [\nLack of Functional Correlation\nAnother significant limitation is the lack of correlation with functional or clinical outcomes. While the anatomical descriptions are detailed, the study does not provide information on how these variations might impact cerebral function or predispose individuals to specific pathologies. Future studies should aim to correlate anatomical variations with clinical data to better understand their implications in neurological conditions such as aneurysms, gliomas, and strokes [\nLack of Comparative Analysis\nOne limitation of the study is the absence of a comparative analysis with other specimens or established reference standards. Incorporating such comparisons would offer a more meaningful context for interpreting these rare anatomical variations, helping to clarify how they differ from more common configurations. Comparative research could also shed light on whether these anomalies are isolated findings or part of a wider pattern of vascular variability and limited technological integration.\nWhile the study provides a detailed anatomical account, it does not leverage advanced imaging technologies such as high-resolution MRI or CT angiography, which could offer more precise and comprehensive visualization of vascular structures. Incorporating these technologies in future studies would enhance the accuracy of anatomical descriptions and allow for non-invasive correlation with clinical conditions [\nPotential impact on clinical practice\nDespite these limitations, the findings of this study have important implications for clinical practice, particularly in neurosurgery and radiology. Understanding these rare anatomical variations can aid in preoperative planning, improve surgical outcomes, and reduce the risk of complications. However, the clinical relevance of these findings needs to be validated through further research involving larger cohorts and diverse populations [\nA thorough examination of ACA anatomical variations offers valuable insights for clinical practice. These differences play a crucial role in how conditions like aneurysms, gliomas, strokes, and other cerebrovascular disorders are managed. Utilizing advanced imaging technologies alongside innovations like 3D printing, paired with a deep understanding of anatomical variations, is key to successful treatment planning and better patient outcomes. Moving forward, ongoing research should continue to investigate how these variations impact clinical care and work toward creating clear guidelines for managing them in different neurological scenarios.\nRecognizing and understanding ACA anatomical differences is more than just an academic exercise; it’s essential for real-world applications. As this study shows, these variations can directly influence the success of neurosurgical procedures and the treatment of cerebrovascular conditions.", "topic": "Brain"}
{"pmid": "40448387", "pmcid": "12294321", "title": "State Space Correspondence and Cross-Entropy Methods in the Assessment of Bidirectional Cardiorespiratory Coupling in Heart Failure", "publication_year": "N/A", "abstract": "The complex interplay between the cardiac and the respiratory systems, termed cardiorespiratory coupling (CRC), is a bidirectional phenomenon that can be affected by pathologies such as heart failure (HF). In the present work, the potential changes in strength of directional CRC were assessed in HF patients classified according to their cardiac rhythm via two measures of coupling based on k-nearest neighbor (KNN) estimation approaches, cross-entropy (CrossEn) and state space correspondence (SSC), applied on the heart period (HP) and respiratory (RESP) variability series, while also accounting for the complexity of the cardiac and respiratory rhythms. We tested the measures on 25 HF patients with sinus rhythm (SR, age: 58.9 ± 9.7 years; 23 males) and 41 HF patients with ventricular arrhythmia (VA, age 62.2 ± 11.0 years; 30 males). A predominant directionality of interaction from the cardiac to the respiratory rhythm was observed in both cohorts and using both methodologies, with similar statistical power, while a lower complexity for the RESP series compared to HP series was observed in the SR cohort. We conclude that CrossEn and SSC can be considered strictly related to each other when using a KNN technique for the estimation of the cross-predictability markers.", "full_text": "Heart period (HP) is well known to be affected by respiratory phase, according to a mechanism termed respiratory sinus arrhythmia (RSA) [\nHeart failure (HF) is a disease which has been assessed in the context of CRC through both unidirectional and bidirectional markers based on linear and nonlinear measures [\nThe autonomic nervous system (ANS) plays a decisive role in the provocation and maintenance of ventricular arrhythmia (VA) and sympathetic activation may provide both the trigger and substrate of VAs. This interaction between the ANS and VAs is expected in HF, which is a disease with a pronounced ANS imbalance, but has also been shown in the case of idiopathic extrasystoles, such as those from the outflow tract. Namely, it has been proven that patients with frequent outflow tract extrasystoles have impaired heart rate variability (HRV), in terms of significantly reduced values of HRV parameters in the time and frequency domains [\nIn the present work, we evaluate CRC in terms of the changes in the directional interaction of HP and respiratory signal dynamics using two methodologies based on KNN approaches (i.e., CrossEn and SSC), in order to investigate potential differences in either direction of interaction between HF patients with sinus rhythm (SR) and with VA, while accounting for the complexity of the cardiac and respiratory rhythms. To the authors’ knowledge, this study is the first comparison between the two methodologies in the context of CRC of HF patients, by using an extended version of the database in [\nThe data analyzed in this study were collected from patients with HF (reduced left ventricular ejection fraction, LVEF <35%) and indication for implantation of a cardiac resynchronization therapy (CRT) device or implantable cardioverter defibrillator (ICD), a subgroup of a database published in our previous works [\nExperiments were conducted in the morning, between 7 and 8 a.m., in quiet surroundings in a room at the Pacemaker Center of the University Clinical Center of Serbia immediately before device implantation. The electrocardiogram (ECG) and the respiratory signal were acquired during 20 min of recording in relaxed subjects in the supine position and spontaneous breathing frequency without verbalization. Measurements were performed by using the Biopac MP100 system and Acqknowledge 3.9.1 software (BIOPAC System, Inc., Santa Barbara, CA, USA) using a 1 kHz sample rate and a 16-bit amplitude resolution. ECG data were acquired from lead I using the ECG 100C amplifier module, while an RSP 100C respiratory pneumogram amplifier module with TSD 201 strain gauge transducer attached to the belt was used to measure abdominal expansion and contraction as an estimation of the respiratory signal (RESP) [\nR-wave peaks were identified on the ECG using a threshold-based algorithm applied to the first derivative of the ECG, and fixed by parabolic interpolation. Each HP value was estimated as the temporal interval between two consecutively identified R-wave peaks in order to compute the beat-to-beat HP series. A single sequence of 256 consecutive HP values of the beat-to-beat series were randomly selected for analysis for each patient and all identified fiducial points within the chosen window, namely the R-wave peaks, were visually checked. If erroneous detections were present, they were manually corrected using a strict criterion of a maximum of 5% of corrections; otherwise, the segment was discarded and a new one was selected. We did not analyze HP segments corresponding to periods of sustained arrhythmia even in the VA group to better compare similar sinus rhythm regimes in both cohorts and to focus on the chronic effects of arrhythmia on CRC in HF patients. Therefore, if isolated ectopic beats were visually identified, the series was linearly interpolated from the HP values involving the two beats most adjacent to the ectopic beat and classified as sinus rhythm. If any runs of consecutive ectopic beats were identified (i.e., transient arrhythmia), the segment was avoided and a different sequence of 256 HP values was considered. RESP was sampled in correspondence with the identified R-wave peaks in order to obtain a simultaneous beat-to-beat series. Within the beat-to-beat RESP series, the 256 consecutive values corresponding to the previously selected HP series segment were considered. Since the considered methodologies require stationarity, the fulfillment of this prerequisite was verified according to the stability of the mean and variance [\nLet us consider a bivariate random system\nThe information-theoretic measure of CrossEn quantifies the amount of information carried by the present of the target that can be predicted solely by the past of the driver [\nHerein, a non-linear model-free approach relying on\nIn our analyses, we separately took into account both directions of CRC by first setting X as HP and Y as RESP, in order to evaluate the\nUsing the same notation as in\nSpecifically, we consider\nThe cross-predictability function (CPF) is defined over the embedding dimension\nIn the present study, we set\nThe complexity of the two considered series was assessed in order to evaluate the potential bias introduced by changes in the regularity of the driver and the target to the described interaction measures (see\nTo evaluate the regularity and predictability of the time series, we first used the information storage (IS) measure, i.e., the quantity of information held in the current state of the system attributable to its past states, which is defined for a given process\nRegarding the SSC approach, the degree of predictability of\nThe normality of the distributions was verified via the Shapiro–Wilk test. For all clinical variables, t-tests, or Mann–Whitney U tests when appropriate, were carried out over the continuous variables, while chi-square tests, or Fisher Exact tests when appropriate, were applied for the categorical variables in order to evaluate the differences between the two experimental groups (i.e., SR and VA). For the directional CRC indexes, a two-way repeated measure ANOVA with one factor repetition was performed to evaluate the differences between the directions of interaction (i.e.,\nA mixed model analysis was performed to account for the potential effect of the clinical confounding factors on each index of cardiorespiratory interaction (i.e., CrossEn and CPI). Specifically, a linear mixed model for repeated measures was employed, using the experimental group and direction of interaction as fixed effects and the clinical variables that showed a statistically significant difference among the two cohorts as the random effect.\nStatistical analyses were performed using commercial statistical software (Sigmaplot v.14.0, Systat Software, San Jose, CA, USA and IBM SPSS Statistics v.22.0.0.0, Armonk, NY, USA: IBM Corp.). A\nRegarding the linear mixed model analysis, we observed that for all CRC indexes, the random effect of clinical variability within cohorts never reached statistical significance (\nThe differences in the clinical characteristics of the two groups of HF patients were largely expected. Although ventricular extrasystoles can be completely benign in nature, they can also be an excellent indicator of a serious disorder. Especially when extrasystoles are multifocal and prone to clustering, electrolyte disturbances, structural heart disease, or coronary artery disease should be considered. Our results showed that HF patients with VA more often had ischemic heart disease (although this difference was not statistically significant), and were treated significantly longer for HF. In terms of comorbidities and risk factors for the development of cardiovascular disease, the VA patients presented a higher incidence of hypertension, but there was a relative homogeneity for other risk factors, excluding them as a confounding factor for our further results, as confirmed by the linear mixed model analysis. An important finding is that most of these patients had a lower NYHA class, as malignant ventricular arrhythmias, i.e., sudden cardiac death, have been shown to be a greater problem in patients who are not in advanced/terminal HF [\nWhile some previous studies have found that the occurrence of ventricular extrasystoles in HF further reduces the irregularity, or complexity, of the heart rhythm, when measured by sample entropy, our results indicate that the cardiac rhythm complexity is not significantly different in the VA group, with a slight but not statistically significant increase in regularity compared to the SR cohort [\nRespiratory signals consist of oscillatory and deterministic characteristics, but it was unexpectedly discovered, by the results of the respiratory signal analysis, that the complexity of RESP usually was similar to the complexity of the HP time series [\nSeveral factors taken together probably contribute to the findings related to the change in complexity of the respiratory rhythm of HF patients in SR, compared to the complexity of their heart rhythm, as observed via the PI marker. The fundamental mechanism underlying the neural control of breathing is a three-component system which ensures the proper regulation of respiratory patterns. The brainstem central pattern generator, particularly the preBötzinger complex with inhibitory neurons active during inspiratory bursts, plays a crucial role as the primary rhythm driver, generating the basic breathing pattern. The principal role of chemoreceptors in the control of breathing, in the central nervous system and peripheral nervous system, is as feedback detectors of errors in pH and partial pressures of CO\nWhile similar trends can be observed for both IS and PI of both HP and RESP, only PI found a higher predictability of RESP compared to HP, which was not observed with IS. Methodologically, the lower statistical power of IS can be related to the fact that this measure reflects the whole information contained in the past of the target that can be used to predict its present, no matter of the origin of such information. Therefore, IS does not strictly reflect the internal dynamics in the target system but is influenced as well by the causal interactions from source to target [\nIn the present study, we considered the intricate interplay between the cardiac and respiratory systems in the context of HF, and it is apparent that complementary measures of cardiorespiratory coupling highlighted a different influence of the heart rhythm on respiration between patients’ groups. In the group of HF patients with SR, we observed a stronger influence of the heart rhythm on the respiratory rhythm, as evaluated by CrossEn and CPI, than in the other group. We may hypothesize that, in HF patients with VA, some respiratory plasticity-related mechanisms are lost compared to SR, which in return decreases the heart rhythm influence on respiratory patterns. This result was obtained in the absence of any change in PI across experimental groups and only PI of RESP was found to be higher than PI of HP for the SR cohort. It must be taken into account that this last result might introduce a bias in the calculated CRC markers when considering the\nFurthermore, this confirms previous results [\nOf interest, results of bidirectional CRC obtained with the two methodologies yielded the same trends and statistical significance, proving the strong correlation between the two measures, which can be attributed to both methods implementing the concept of cross-predictability with a similar estimation approach. The CrossEn measure represents the cross-information from the driver to the target process, i.e., the overall amount of information carried by the present of the target that can be explained by the past of the driver, including not only the causal interaction from the driver to the target, but also the causal interactions from the target to the driver, and the contribution from the contemporaneous presence of internal dynamics in the target [\nUsually, HF patients exhibit both diminished variability in heart rate [\nClinical practice shows that in patients with HF, especially those with reduced LVEF, there are no sufficiently good predictors or composite risk scores to identify patients with an increased risk of sudden cardiac death or those who will benefit from a certain therapeutic option, such as vagus nerve stimulation or MitraClip implantation, or cardiac resynchronization therapy [\nIn this study, we did not analyze RESP segments corresponding to HP segments with arrhythmias, even in the VA cohort, in order to better compare similar regimes of sinus rhythm in the two cohorts and focus on the chronic effects of arrhythmia on CRC and the complexity of the variability series in HF. Hence, we did not look at the effect of the temporary appearance of VAs in HF patients on the respiratory rhythm and CRC. In the future, the acute effect of arrhythmic epochs and/or atrial fibrillation on CRC in HF should be studied.\nFurthermore, in this exploratory study, the considered techniques of CRC assessment were tested in offline and controlled laboratory conditions. In the future, their validation for real-time clinical use should be performed, aiming for integration into monitoring systems for the clinical assessment of CRC.\nIt must also be noted that the proposed measures of CrossEn and SSC are directional but not causal as defined by the Granger causality (GC) paradigm [\nFinally, it is worth noting that one of the limitations of this study is that we did not involve the group of healthy control subjects who were closer in age to the HF patients. The demographic realities of aging populations, coupled with the high prevalence of subclinical cardiovascular risk factors, render the identification of truly healthy elderly individuals exceptionally rare. Despite this limitation, the present study offers a robust framework for interrogating HF disease-specific alterations in cardiorespiratory dynamics while highlighting the pressing need for innovative approaches to control group selection in translational cardiovascular research.\nFuture studies could focus on the context of heart transplant recipients, whose donor hearts are entirely vagally denervated and consequently lack RSA. In such patients, the absence of vagal influence effectively isolates non-autonomic, intrinsic mechanisms of cardiorespiratory coupling. This unique physiological condition would provide an unambiguous substrate for evaluating the full clinical relevance of our analytical approach. Additionally, quantifying changes in directionality and strength of cardiorespiratory interactions could help differentiate between adaptive and maladaptive responses in individuals with varying degrees of cardiorespiratory compromise. For instance, the restoration or loss of specific feedback loops during apneas, as opposed to normative sleep-breathing cycles, may serve as functional markers for disease progression or therapeutic efficacy. Such analyses might further illuminate how the interplay between parasympathetic withdrawal, sympathetic surges, and altered baroreflex sensitivity modulates the overall cardiorespiratory landscape in both acute and chronic settings.\nCross-predictability methodologies such as cross-entropy and SSC approaches revealed changes in CRC between HF patients with SR and VA, under the hypothesis of loss of respiratory plasticity-related mechanisms in HF patients with VA, compared to HF patients with SR. Moreover, we observed a predominant directionality of interaction from the cardiac rhythm to the respiratory rhythm in both cohorts and using both methodologies, with similar statistical power. We conclude that the two studied approaches can be considered strictly related to each other when using a KNN technique for the estimation of the CrossEn and CPI markers. Furthermore, we stress the importance of considering directionality of interaction by applying causal markers in the assessment of CRC, and of accounting for the complexity of the target in the evaluation of the driver to target relationship, for a more complete evaluation of the dynamics of two weakly coupled systems such as the heart and respiratory system.", "content_for_embedding": "Heart period (HP) is well known to be affected by respiratory phase, according to a mechanism termed respiratory sinus arrhythmia (RSA) [\nHeart failure (HF) is a disease which has been assessed in the context of CRC through both unidirectional and bidirectional markers based on linear and nonlinear measures [\nThe autonomic nervous system (ANS) plays a decisive role in the provocation and maintenance of ventricular arrhythmia (VA) and sympathetic activation may provide both the trigger and substrate of VAs. This interaction between the ANS and VAs is expected in HF, which is a disease with a pronounced ANS imbalance, but has also been shown in the case of idiopathic extrasystoles, such as those from the outflow tract. Namely, it has been proven that patients with frequent outflow tract extrasystoles have impaired heart rate variability (HRV), in terms of significantly reduced values of HRV parameters in the time and frequency domains [\nIn the present work, we evaluate CRC in terms of the changes in the directional interaction of HP and respiratory signal dynamics using two methodologies based on KNN approaches (i.e., CrossEn and SSC), in order to investigate potential differences in either direction of interaction between HF patients with sinus rhythm (SR) and with VA, while accounting for the complexity of the cardiac and respiratory rhythms. To the authors’ knowledge, this study is the first comparison between the two methodologies in the context of CRC of HF patients, by using an extended version of the database in [\nThe data analyzed in this study were collected from patients with HF (reduced left ventricular ejection fraction, LVEF <35%) and indication for implantation of a cardiac resynchronization therapy (CRT) device or implantable cardioverter defibrillator (ICD), a subgroup of a database published in our previous works [\nExperiments were conducted in the morning, between 7 and 8 a.m., in quiet surroundings in a room at the Pacemaker Center of the University Clinical Center of Serbia immediately before device implantation. The electrocardiogram (ECG) and the respiratory signal were acquired during 20 min of recording in relaxed subjects in the supine position and spontaneous breathing frequency without verbalization. Measurements were performed by using the Biopac MP100 system and Acqknowledge 3.9.1 software (BIOPAC System, Inc., Santa Barbara, CA, USA) using a 1 kHz sample rate and a 16-bit amplitude resolution. ECG data were acquired from lead I using the ECG 100C amplifier module, while an RSP 100C respiratory pneumogram amplifier module with TSD 201 strain gauge transducer attached to the belt was used to measure abdominal expansion and contraction as an estimation of the respiratory signal (RESP) [\nR-wave peaks were identified on the ECG using a threshold-based algorithm applied to the first derivative of the ECG, and fixed by parabolic interpolation. Each HP value was estimated as the temporal interval between two consecutively identified R-wave peaks in order to compute the beat-to-beat HP series. A single sequence of 256 consecutive HP values of the beat-to-beat series were randomly selected for analysis for each patient and all identified fiducial points within the chosen window, namely the R-wave peaks, were visually checked. If erroneous detections were present, they were manually corrected using a strict criterion of a maximum of 5% of corrections; otherwise, the segment was discarded and a new one was selected. We did not analyze HP segments corresponding to periods of sustained arrhythmia even in the VA group to better compare similar sinus rhythm regimes in both cohorts and to focus on the chronic effects of arrhythmia on CRC in HF patients. Therefore, if isolated ectopic beats were visually identified, the series was linearly interpolated from the HP values involving the two beats most adjacent to the ectopic beat and classified as sinus rhythm. If any runs of consecutive ectopic beats were identified (i.e., transient arrhythmia), the segment was avoided and a different sequence of 256 HP values was considered. RESP was sampled in correspondence with the identified R-wave peaks in order to obtain a simultaneous beat-to-beat series. Within the beat-to-beat RESP series, the 256 consecutive values corresponding to the previously selected HP series segment were considered. Since the considered methodologies require stationarity, the fulfillment of this prerequisite was verified according to the stability of the mean and variance [\nLet us consider a bivariate random system\nThe information-theoretic measure of CrossEn quantifies the amount of information carried by the present of the target that can be predicted solely by the past of the driver [\nHerein, a non-linear model-free approach relying on\nIn our analyses, we separately took into account both directions of CRC by first setting X as HP and Y as RESP, in order to evaluate the\nUsing the same notation as in\nSpecifically, we consider\nThe cross-predictability function (CPF) is defined over the embedding dimension\nIn the present study, we set\nThe complexity of the two considered series was assessed in order to evaluate the potential bias introduced by changes in the regularity of the driver and the target to the described interaction measures (see\nTo evaluate the regularity and predictability of the time series, we first used the information storage (IS) measure, i.e., the quantity of information held in the current state of the system attributable to its past states, which is defined for a given process\nRegarding the SSC approach, the degree of predictability of\nThe normality of the distributions was verified via the Shapiro–Wilk test. For all clinical variables, t-tests, or Mann–Whitney U tests when appropriate, were carried out over the continuous variables, while chi-square tests, or Fisher Exact tests when appropriate, were applied for the categorical variables in order to evaluate the differences between the two experimental groups (i.e., SR and VA). For the directional CRC indexes, a two-way repeated measure ANOVA with one factor repetition was performed to evaluate the differences between the directions of interaction (i.e.,\nA mixed model analysis was performed to account for the potential effect of the clinical confounding factors on each index of cardiorespiratory interaction (i.e., CrossEn and CPI). Specifically, a linear mixed model for repeated measures was employed, using the experimental group and direction of interaction as fixed effects and the clinical variables that showed a statistically significant difference among the two cohorts as the random effect.\nStatistical analyses were performed using commercial statistical software (Sigmaplot v.14.0, Systat Software, San Jose, CA, USA and IBM SPSS Statistics v.22.0.0.0, Armonk, NY, USA: IBM Corp.). A\nRegarding the linear mixed model analysis, we observed that for all CRC indexes, the random effect of clinical variability within cohorts never reached statistical significance (\nThe differences in the clinical characteristics of the two groups of HF patients were largely expected. Although ventricular extrasystoles can be completely benign in nature, they can also be an excellent indicator of a serious disorder. Especially when extrasystoles are multifocal and prone to clustering, electrolyte disturbances, structural heart disease, or coronary artery disease should be considered. Our results showed that HF patients with VA more often had ischemic heart disease (although this difference was not statistically significant), and were treated significantly longer for HF. In terms of comorbidities and risk factors for the development of cardiovascular disease, the VA patients presented a higher incidence of hypertension, but there was a relative homogeneity for other risk factors, excluding them as a confounding factor for our further results, as confirmed by the linear mixed model analysis. An important finding is that most of these patients had a lower NYHA class, as malignant ventricular arrhythmias, i.e., sudden cardiac death, have been shown to be a greater problem in patients who are not in advanced/terminal HF [\nWhile some previous studies have found that the occurrence of ventricular extrasystoles in HF further reduces the irregularity, or complexity, of the heart rhythm, when measured by sample entropy, our results indicate that the cardiac rhythm complexity is not significantly different in the VA group, with a slight but not statistically significant increase in regularity compared to the SR cohort [\nRespiratory signals consist of oscillatory and deterministic characteristics, but it was unexpectedly discovered, by the results of the respiratory signal analysis, that the complexity of RESP usually was similar to the complexity of the HP time series [\nSeveral factors taken together probably contribute to the findings related to the change in complexity of the respiratory rhythm of HF patients in SR, compared to the complexity of their heart rhythm, as observed via the PI marker. The fundamental mechanism underlying the neural control of breathing is a three-component system which ensures the proper regulation of respiratory patterns. The brainstem central pattern generator, particularly the preBötzinger complex with inhibitory neurons active during inspiratory bursts, plays a crucial role as the primary rhythm driver, generating the basic breathing pattern. The principal role of chemoreceptors in the control of breathing, in the central nervous system and peripheral nervous system, is as feedback detectors of errors in pH and partial pressures of CO\nWhile similar trends can be observed for both IS and PI of both HP and RESP, only PI found a higher predictability of RESP compared to HP, which was not observed with IS. Methodologically, the lower statistical power of IS can be related to the fact that this measure reflects the whole information contained in the past of the target that can be used to predict its present, no matter of the origin of such information. Therefore, IS does not strictly reflect the internal dynamics in the target system but is influenced as well by the causal interactions from source to target [\nIn the present study, we considered the intricate interplay between the cardiac and respiratory systems in the context of HF, and it is apparent that complementary measures of cardiorespiratory coupling highlighted a different influence of the heart rhythm on respiration between patients’ groups. In the group of HF patients with SR, we observed a stronger influence of the heart rhythm on the respiratory rhythm, as evaluated by CrossEn and CPI, than in the other group. We may hypothesize that, in HF patients with VA, some respiratory plasticity-related mechanisms are lost compared to SR, which in return decreases the heart rhythm influence on respiratory patterns. This result was obtained in the absence of any change in PI across experimental groups and only PI of RESP was found to be higher than PI of HP for the SR cohort. It must be taken into account that this last result might introduce a bias in the calculated CRC markers when considering the\nFurthermore, this confirms previous results [\nOf interest, results of bidirectional CRC obtained with the two methodologies yielded the same trends and statistical significance, proving the strong correlation between the two measures, which can be attributed to both methods implementing the concept of cross-predictability with a similar estimation approach. The CrossEn measure represents the cross-information from the driver to the target process, i.e., the overall amount of information carried by the present of the target that can be explained by the past of the driver, including not only the causal interaction from the driver to the target, but also the causal interactions from the target to the driver, and the contribution from the contemporaneous presence of internal dynamics in the target [\nUsually, HF patients exhibit both diminished variability in heart rate [\nClinical practice shows that in patients with HF, especially those with reduced LVEF, there are no sufficiently good predictors or composite risk scores to identify patients with an increased risk of sudden cardiac death or those who will benefit from a certain therapeutic option, such as vagus nerve stimulation or MitraClip implantation, or cardiac resynchronization therapy [\nIn this study, we did not analyze RESP segments corresponding to HP segments with arrhythmias, even in the VA cohort, in order to better compare similar regimes of sinus rhythm in the two cohorts and focus on the chronic effects of arrhythmia on CRC and the complexity of the variability series in HF. Hence, we did not look at the effect of the temporary appearance of VAs in HF patients on the respiratory rhythm and CRC. In the future, the acute effect of arrhythmic epochs and/or atrial fibrillation on CRC in HF should be studied.\nFurthermore, in this exploratory study, the considered techniques of CRC assessment were tested in offline and controlled laboratory conditions. In the future, their validation for real-time clinical use should be performed, aiming for integration into monitoring systems for the clinical assessment of CRC.\nIt must also be noted that the proposed measures of CrossEn and SSC are directional but not causal as defined by the Granger causality (GC) paradigm [\nFinally, it is worth noting that one of the limitations of this study is that we did not involve the group of healthy control subjects who were closer in age to the HF patients. The demographic realities of aging populations, coupled with the high prevalence of subclinical cardiovascular risk factors, render the identification of truly healthy elderly individuals exceptionally rare. Despite this limitation, the present study offers a robust framework for interrogating HF disease-specific alterations in cardiorespiratory dynamics while highlighting the pressing need for innovative approaches to control group selection in translational cardiovascular research.\nFuture studies could focus on the context of heart transplant recipients, whose donor hearts are entirely vagally denervated and consequently lack RSA. In such patients, the absence of vagal influence effectively isolates non-autonomic, intrinsic mechanisms of cardiorespiratory coupling. This unique physiological condition would provide an unambiguous substrate for evaluating the full clinical relevance of our analytical approach. Additionally, quantifying changes in directionality and strength of cardiorespiratory interactions could help differentiate between adaptive and maladaptive responses in individuals with varying degrees of cardiorespiratory compromise. For instance, the restoration or loss of specific feedback loops during apneas, as opposed to normative sleep-breathing cycles, may serve as functional markers for disease progression or therapeutic efficacy. Such analyses might further illuminate how the interplay between parasympathetic withdrawal, sympathetic surges, and altered baroreflex sensitivity modulates the overall cardiorespiratory landscape in both acute and chronic settings.\nCross-predictability methodologies such as cross-entropy and SSC approaches revealed changes in CRC between HF patients with SR and VA, under the hypothesis of loss of respiratory plasticity-related mechanisms in HF patients with VA, compared to HF patients with SR. Moreover, we observed a predominant directionality of interaction from the cardiac rhythm to the respiratory rhythm in both cohorts and using both methodologies, with similar statistical power. We conclude that the two studied approaches can be considered strictly related to each other when using a KNN technique for the estimation of the CrossEn and CPI markers. Furthermore, we stress the importance of considering directionality of interaction by applying causal markers in the assessment of CRC, and of accounting for the complexity of the target in the evaluation of the driver to target relationship, for a more complete evaluation of the dynamics of two weakly coupled systems such as the heart and respiratory system.", "topic": "Brain"}
{"pmid": "40392386", "pmcid": "12308838", "title": "Untangling the multifaceted VTA responses to stress", "publication_year": "N/A", "abstract": "Stress has profound impacts on the ventral tegmental area (VTA). However, the complex and opposing effects of stress on the VTA have limited the ability to reach a clear understanding of how adaptation of the VTA can drive behavior following stress. In this review, we provide an overview of VTA responses to acute and chronic stress, with a primary focus on studies in mice and rats. We propose that divergent responses to stress arise from the heterogeneity of VTA neurons, the multi-dimensional nature of stress, and interactive effects between cumulative stressors. We suggest that the robust and varied plasticity of the VTA in response to stress indicates a role for the VTA as an integrator of homeostatic and affective information during stress to drive flexible and nuanced adjustments in behavioral adaptation.", "full_text": "", "content_for_embedding": "Stress has profound impacts on the ventral tegmental area (VTA). However, the complex and opposing effects of stress on the VTA have limited the ability to reach a clear understanding of how adaptation of the VTA can drive behavior following stress. In this review, we provide an overview of VTA responses to acute and chronic stress, with a primary focus on studies in mice and rats. We propose that divergent responses to stress arise from the heterogeneity of VTA neurons, the multi-dimensional nature of stress, and interactive effects between cumulative stressors. We suggest that the robust and varied plasticity of the VTA in response to stress indicates a role for the VTA as an integrator of homeostatic and affective information during stress to drive flexible and nuanced adjustments in behavioral adaptation.", "topic": "Brain"}
{"pmid": "40327263", "pmcid": "12301324", "title": "Augmenting art crossmodally: possibilities and pitfalls", "publication_year": "N/A", "abstract": "In this narrative historical review, we take a closer look at the question of whether it is possible to augment works of art through crossmodal (specifically audiovisual) means. We start by highlighting an important distinction between three classes of audiovisual crossmodal correspondence: Namely those operating on individual sensory stimuli (so-called basic correspondences), those operating on dynamically-changing stimuli, or else on combinations of unisensory stimuli (so-called mid-level correspondences), and those operating on complex and often aesthetically-meaningful stimuli, such as music and paintings. We also highlight another important distinction between the literature on crossmodal matching and that dedicated to demonstrating crossmodal effects. The latter distinction aligns, in some sense, onto the distinction between crossmodal mapping and crossmodal effects. Although it may not be possible, in any meaningful sense, to translate works of art from one modality into another, that does not deny the possibility of augmenting a work of art by the deliberate addition of stimulation presented to another sensory modality. The aims and objectives of those who have attempted to augment works of art by introducing additional sensory stimulation are discussed. We also draw attention to a number of challenges and/or pitfalls (such as the distraction offered by recourse to the phenomenon of synaesthesia) for those interested in augmenting auditory/visual art crossmodally.", "full_text": "In recent decades, there has been a growing awareness within the cognitive neurosciences that the senses do not operate in isolation (Barlow and Mollon,\nOne of the most exciting recent areas in crossmodal research concerns the crossmodal correspondences. This is the name given to the almost synaesthesia-like surprising connections between features, attributes, or dimensions of experience presented in different senses. Often confused with synaesthesia (in that both phenomena are surprising when first you hear about them), they are fundamentally different (see Deroy and Spence,\nIn this narrative historical review, we first introduce an important distinction between three classes of audiovisual crossmodal correspondence: Namely those operating on individual sensory stimuli (so-called basic correspondences), those operating on dynamically-changing stimuli, or else on combinations of unisensory stimuli (so-called mid-level correspondences), and those operating on complex and often aesthetically-meaningful stimuli, such as music and paintings (Section 1.1). We also highlight an important distinction between the literature on crossmodal matching and that dedicated to demonstrating crossmodal effects. The latter distinction aligns, in some sense, with the distinction between crossmodal mapping and crossmodal effects. In Section 1.2, we briefly consider the kinds of explanations that have been put forward to account for crossmodal correspondences to date. Thereafter, in Sections 2–4, we review the evidence concerning audiovisual crossmodal matching and crossmodal effects operating at the level of basic, mid-level, and complex crossmodal correspondences, respectively.\nIn Section 5, we build on this groundwork by reflecting on the putative existence of crossmodal art. We take a closer look at the question of whether it is possible to augment works of art through crossmodal (specifically audiovisual) means. We show how the phenomenon of synaesthesia, at least as conceptualized by cognitive neuroscientists fails to enlighten theorizing when it comes to thinking about the social/structural construction of meaning in the context of the crossmodal augmentation of art (Dimova,\nAudiovisual crossmodal correspondences can be grouped into different classes based on the (perceived) complexity of the stimuli involved. In particular, while some researchers have chosen to study crossmodal correspondences between basic (i.e., individual) sensory stimuli (i.e., specific features or dimensions), others have chosen to focus on crossmodal correspondences (and Gestalt grouping) in the case of what might be called mid-level correspondences instead (i.e., those correspondences that are experienced between combinations of unisensory stimuli, such as an auditory melody and a sequence of visual stimuli).\nBasic audiovisual crossmodal correspondences operate between discrete unimodal sensory stimuli, such as, for example, pure tones, color patches, and sounds and lights varying in terms of their intensity (i.e., loudness and brightness). It has sometimes been suggested that certain of these basic crossmodal correspondences may reflect the existence of amodal sensory dimensions, such as intensity, shape, texture, brightness, etc. (e.g., Bond and Stevens,\nMid-level correspondences typically operate between structured combinations of unisensory stimuli, such as short sequences of sounds (that may make up a melody or a spatiotemporally arranged pattern of visual stimuli). Numerous studies, often in the Gestalt tradition, have demonstrated how the grouping of sequences of stimuli presented in one sensory modality may influence the presentation of stimuli presented in the other modality, as for example, in the context of the so-called “crossmodal dynamic capture effect” (Soto-Faraco et al.,\nIn parallel, various researchers have explored audiovisual correspondences using more complex and semantically-rich stimuli. In such cases, the complexity of the stimuli involved means that it is harder to explain any crossmodal correspondences that are observed based on specific individual auditory/visual physical stimulus attributes/dimensions (e.g., such as frequency and hue; see Duthie,\nAt the same time, however, a separate distinction can be made between those studies that have merely sought to establish the existence of crossmodal correspondences between the stimuli presented at a particular level (i.e., basic, mid-level, or complex)—crossmodal matching studies—and those studies that have looked to determine whether there are any crossmodal effects, evidenced by the presence of stimuli in one modality having an impact on those presented in the other modality (see\nBroad categorization of different types of crossmodal correspondence.\nWhile crossmodal correspondences operate between sensory stimuli presented at different levels of complexity (e.g., between individual auditory stimuli and complex visual stimuli), the majority of research that has been published to date has tended to present stimuli at the same level of complexity in both the auditory and visual modalities. Nevertheless, a few studies have been published documenting crossmodal correspondences between stimuli at different levels, such as, for example, demonstrating crossmodal correspondences between short snippets of music and isolated color patches (e.g., Palmer et al.,\nThere are two possibly controversial claims (or observations) that we wish to make in this review: The first is that the mere existence of a crossmodal correspondence, as revealed by research on crossmodal matching, has no necessary implications for the existence of a crossmodal effect; The second is that the existence of crossmodal correspondences operating between individual sensory stimuli has no necessary implications as far as the existence of crossmodal effects operating between complex stimuli are concerned. That is, many crossmodal effects have been demonstrated in the literature without any consideration of the crossmodal congruency, or otherwise, of the component unimodal stimuli (see London,\nIt sometimes appears to be implicitly assumed by researchers working in the area (i.e., in crossmodal correspondences research) that the existence of a crossmodal effect can be taken as evidence of the existence of a crossmodal correspondence between one or several of the sensory features or dimensions involved. As such, the view that we wish to advocate here is that the existence of a crossmodal correspondence can only be convincingly demonstrated by the results of crossmodal matching studies (that is, in studies where the participants are explicitly asked to pick the best match of the available options that have been provided to them\nOver the years, a number of different mechanisms have been put forward to try to explain crossmodal correspondences (Motoki et al.,\nSummary of the various different types of crossmodal correspondence that have been proposed that have to connect auditory and visual stimuli and selected literature sources suggested and/or supported them.\nOne of the most extensively studied areas in research on audiovisual correspondences concerns the associations that have been demonstrated between individual sensory stimuli (or attributes), such as pitch, brightness, or loudness. Over the last 50 years or so, a very wide array of crossmodal matching studies has been published. More recently, a number of studies demonstrating crossmodal effects on perceptual binding as well as in a variety of speeded response tasks that are modulated by the crossmodal correspondences that exist between individual sensory stimuli (attributes or sensory dimensions) have also appeared in the literature; it is to this research that we turn next.\nMany published studies have documented crossmodal correspondences between simple stimuli, or stimulus dimensions (see Spence,\nThere has, however, been widespread disagreement in the case of a putative crossmodal correspondence between auditory pitch and visual hue. Some commentators have been convinced that such a correspondence must exist based on the structural similarity of the underlying sensory dimensions (Gombrich,\nMore recent research has drawn attention to the existence of a number of robust crossmodal correspondences between visual features (i.e., surface textures) and auditory timbre, due to the allegedly intrinsic multisensoriality of timbre characterization/semantics (e.g., Reuter et al.,\nOther researchers have demonstrated the existence of audiovisual crossmodal correspondences between musical timbre and visual shapes (Adeli et al.,\nA wide range of crossmodal correspondence effects have been demonstrated across a wide range of experimental paradigms (e.g., Evans and Treisman,\nOther researchers have started to address the question of whether timbre modulates visual perception. So, for example, in a series of two experiments, Wallmark et al. (\nHence, in summary, there is currently robust experimental evidence for the existence of a wide range of audiovisual crossmodal correspondences involving pitch, such as pitch-size, pitch-brightness, pitch-height, with the exception of pitch-hue correspondences. In the latter case, despite the fact that many commentators are convinced of what the most appropriate crossmodal mapping is (see Spence and Di Stefano,\nMid-level audiovisual crossmodal correspondences operate on dynamically-changing stimuli and/or on combinations of unisensory stimuli (attributes or dimensions; Guo et al.,\nTo date, far fewer crossmodal matching studies have been conducted at this level. What is more, the majority of research that has been published to date would appear to involve cross-level correspondences, such as music-to-color associations for single-line piano melodies (Lindborg and Friberg,\nWhen, for instance, the rate of repetitive auditory stimuli is increased or decreased, while a simultaneously-presented visual flicker remains constant, the latter appears to change accordingly with the auditory stimulus, an effect known as “auditory driving” (Gebhard and Mowbray,\nO'Leary and Rhodes (\nConsider here only how the affective meaning of musical scales depends on their direction—namely, whether they are ascending or descending (Collier and Hubbard,\nExperimental psychologists have long been interested in the crossmodal matching of complex auditory and visual stimuli, as well as on the crossmodal effects of one on the other (see Spence and Di Stefano,\nA number of studies have presented a small selection of stimuli in one sensory modality, with participants being required to match from a pre-selected set of stimuli presented in the other modality (see\nBroad categorization of different types of audiovisual crossmodal matching and crossmodal effects involving complex multi-element aesthetically-meaningful sensory stimuli (see Spence and Di Stefano,\nThe majority of research that has been published to date would appear to be consistent with the claim that complex crossmodal associations between music and paintings are primarily mediated by emotion (see also Hung,\nResearchers have demonstrated that music (or musical emotion) can crossmodally influence the perceived (or at least rated) brightness of paintings (Bhattacharya and Lindsen,\nGiven the research that has been reviewed so far, it would appear clear that crossmodal matching occurs between all possible combinations of individual, organized, and more complex, semantically-meaningful stimuli. Having demonstrated such crossmodal correspondences the next question is how such findings inform attempts at sensory translation or sensory augmentation. At the outset here, one might consider whether crossmodal matching is perhaps more relevant when considering sensory translation, whereas the literature on crossmodal effects may be more pertinent for those wanting to augment art crossmodally. Returning to the two important claims that we put forward earlier, we might say that the mere existence of a crossmodal correspondence (as revealed by research on crossmodal matching), has no necessary implications for the existence of a crossmodal effect. A second claim is that the existence of crossmodal correspondences operating between individual sensory stimuli has no necessary implications as far as the existence of crossmodal effects operating between complex stimuli are concerned. Furthermore, it can also be argued that the absence of a demonstrable crossmodal effect cannot be taken as evidence of the absence of a crossmodal correspondence either. That is, it is at least theoretically plausible, that people might consensually rate certain combinations of stimuli as matching vs. mismatching, without there being any crossmodal effects on performance (e.g., in a speeded classification task). All three of these claims would appear consistent with the various sources of evidence that have been reviewed here.\nThere is an extensive literature on “color music”, sometimes called “visual music” (e.g., Adams,\nMany of those interested in this area invoke the notion of synaesthesia when thinking about engaging multiple senses simultaneously (e.g., see Dimova,\nSeparate from any attempt to use synaesthesia in crossmodal compositions, it should be recognized that certain art forms, such as live opera, inherently stimulate multiple senses; Live music performance too, though this varies all the way from simple inevitability of seeing musicians play in a classic music concert through to the visuals that augment more experimental music (see also McDonald et al.,\nSummary of various kinds of audiovisual artistic/entertainment experience, where music complements/augments the visuals, or vice versa.\nNotice how the synchrony between the auditory and visual elements may sometimes exogenously encourage the integration of the audiovisual inputs (e.g., when attending a classical music concert; see also Muller,\nIwamiya (\nThere has long been artistic interest in the possibility of sensory translation (Kargon,\nA somewhat distinct literature has emerged on the question of the crossmodal sensory augmentation of art. Here, one might consider Scriabin's (\nIntriguingly, although formal evaluation has not yet been conducted, those commentators who have experienced such intentional multisensory, or crossmodal, artistic presentations have, on occasion, hinted at an emergent experience, one that supports the role of Gestalt perceptual grouping in a crossmodal artistic context (though note that the rich notion of multisensory emergence likely reaches beyond the insights offered by the Gestalt approach). Just take the following quote from Wells (\nIn principle, multisensory emergence might be just one example of an extraordinary multisensory experience (e.g., Critchley,\nWhile the term multimedia is broad, and covers multiple forms, there is a sense that some multimedia art works fit the description of crossmodal art in that the stimuli presented to eye and ear are in some sense independent, yet the case is different from the addition of music to film, where the music is seemingly always added as an afterthought (Lipscomb,\nHowes (\nThe notion of intersensoriality can thus help to promote a relational view of the senses, one that resonates with Lévi-Strauss' (\nIn this light, mid-level crossmodal correspondences deserve renewed attention—not as mere by-products of low-level sensory matching or high-level conceptual metaphor, but as hybrid, emergent forms of intersensory pattern recognition. One key research question here thus becomes: What sensory feature, structural property, or dynamic pattern drives these mid-level correspondences? They often resist easy categorization because they operate at the cusp of the perceptual and the conceptual, exhibiting characteristics of both. One might refer here to the “collideroscopic sensorium” (Howes,\nIn their 1928 “Statement on Sound”, Eisenstein, Pudovkin, and Alexandrov advocated the role of sound in film and for “the creation of a new orchestral counterpoint of sight-images and sound-images” (see Eisenstein et al.,\nNote that film and advertising are different inasmuch as people assume that the music and visuals have been deliberately paired in order to achieve some emotional effect or convey some form of semantic meaning (see Bolivar et al.,\nBefore concluding this narrative historical review, it is worth noting how there are a number of challenges, theoretical, practical, attentional, and potentially also ethical when it comes to the crossmodal augmentation of art.\nA clear distinction between crossmodal augmentation in art (as in Scriabin,\nRelatedly, one might draw attention to issues related to the sensory nature of artworks by observing that to have either sensory translation or crossmodally augmented art, the mere simultaneous perception of two sensory inputs is insufficient. For example, hearing Beethoven while viewing a Munch exhibit does not constitute sensory translation or augmentation. And what if a contemporary artist\nThe crossmodal augmentation of art can therefore be seen as requiring more than mere than the simultaneous presentation of the relevant auditory and visual stimuli. It involves a deliberate, non-redundant integration of both, framed within an artistic context that invites interpretation. Crucially, the observer's awareness of this intentional combination transforms the experience from mere coexistence into a meaningful, augmented artistic phenomenon.\nIt is worth pausing here to consider Behne's (\nSeparately, should any putative crossmodal augmentation rely on precise crossmodal temporal synchronization then there may be challenges around ensuring that different sensory inputs/elements align perfectly in real time (e.g., matching sound with visual effects). This can be problematic: Interactive or AI-driven experiences sometimes suffer from technology-induced lag in one or more senses, thus reducing the immersiveness of the experience. One might be reminded here of Bruce Naumann's (1969) artwork “Lip Sync” (see\nThe Danish philosopher Kierkegaard (\nOne argument suggested here is that part of what makes art powerful is its ability to isolate, emphasize, or distill specific aspects of perception. Unlike daily life, where we are bombarded with multisensory input that we must constantly filter, artistic experiences often provide a kind of structured reduction—a selective framing of reality that allows for deeper contemplation. For instance, a still life with fruits and bottles strips away movement, sound, odors, and so forces us to focus on composition, color, and form instead. This could explain why we value these aesthetic experiences, because they allow us to maximize sensory engagement. Reintegrating sensory modalities in aesthetic experience to augment them implies adding more sensory information (haptics, olfaction, spatial audio, interactive visuals, etc.). The danger is that this may merely increase cognitive load, making it harder to focus on any one aspect, thus undermining the essence of art and artistic contemplation. Is multisensoriality in art a form of redundancy—where different sensory modalities repeat the same message—or does it truly open new aesthetic dimensions?\nThe argument seemingly has some cognitive implications. Freeing the senses from the burden of managing excessive stimuli implies freeing intellectual resources for deeper reflection and cognitive engagement. A key argument in favor of art as sensory fragmentation is that it does that to free the mind, allowing for more profound conceptual engagement. By narrowing the range of sensory input, art can liberate cognitive capacity for interpretation, imagination, and emotional resonance. In minimalist art, the deliberate reduction of visual complexity forces the viewer into an active intellectual engagement with form and space. By contrast, technologically-augmented multisensory experiences might do the opposite. Consider only the use of virtual/augmented reality (VR/AR) to augment aesthetic experience (e.g., Cho et al.,\nCertainly, not all theorists/critics have necessarily appreciated the desire to engage more senses in art: As Arnheim stated in an essay entitled, with clear reference to the distinction among the arts in Lessing's 18th-century aesthetics, A New Laocoön: “in their attempts to attract the audience, two media are fighting each other instead of capturing it by a united effort” (Arnheim,\nFinally, it is worth briefly considering the ethical question that was raised by David Lomas (from the Art History department at the University of Manchester), in relation to Flying Object's crossmodal augmentation of four works of visual art from the permanent collection at Tate Britain through the use of scent, sound, virtual touch, and even chocolate in 2015 (see Pursey and Lomas,\nOne of the key points to emerge from the present narrative historical review is that it may not be possible to use elemental correspondences (that is, crossmodal correspondences based on simple stimulus properties), given that works of art are mostly complex, multi-elemental, and semantically/emotionally meaningful. This conclusion seemingly contradicting Eisenstein's suggestion that there is no “pervading law of absolute meanings and correspondences between colors and sound.” (as quoted in Harrison,\nOne question for the future that we haven't had the opportunity to consider here is whether musical knowledge/expertise may affect audiovisual crossmodal correspondences and hence the way in which music and visuals interact in experience (Di Stefano et al.,", "content_for_embedding": "In recent decades, there has been a growing awareness within the cognitive neurosciences that the senses do not operate in isolation (Barlow and Mollon,\nOne of the most exciting recent areas in crossmodal research concerns the crossmodal correspondences. This is the name given to the almost synaesthesia-like surprising connections between features, attributes, or dimensions of experience presented in different senses. Often confused with synaesthesia (in that both phenomena are surprising when first you hear about them), they are fundamentally different (see Deroy and Spence,\nIn this narrative historical review, we first introduce an important distinction between three classes of audiovisual crossmodal correspondence: Namely those operating on individual sensory stimuli (so-called basic correspondences), those operating on dynamically-changing stimuli, or else on combinations of unisensory stimuli (so-called mid-level correspondences), and those operating on complex and often aesthetically-meaningful stimuli, such as music and paintings (Section 1.1). We also highlight an important distinction between the literature on crossmodal matching and that dedicated to demonstrating crossmodal effects. The latter distinction aligns, in some sense, with the distinction between crossmodal mapping and crossmodal effects. In Section 1.2, we briefly consider the kinds of explanations that have been put forward to account for crossmodal correspondences to date. Thereafter, in Sections 2–4, we review the evidence concerning audiovisual crossmodal matching and crossmodal effects operating at the level of basic, mid-level, and complex crossmodal correspondences, respectively.\nIn Section 5, we build on this groundwork by reflecting on the putative existence of crossmodal art. We take a closer look at the question of whether it is possible to augment works of art through crossmodal (specifically audiovisual) means. We show how the phenomenon of synaesthesia, at least as conceptualized by cognitive neuroscientists fails to enlighten theorizing when it comes to thinking about the social/structural construction of meaning in the context of the crossmodal augmentation of art (Dimova,\nAudiovisual crossmodal correspondences can be grouped into different classes based on the (perceived) complexity of the stimuli involved. In particular, while some researchers have chosen to study crossmodal correspondences between basic (i.e., individual) sensory stimuli (i.e., specific features or dimensions), others have chosen to focus on crossmodal correspondences (and Gestalt grouping) in the case of what might be called mid-level correspondences instead (i.e., those correspondences that are experienced between combinations of unisensory stimuli, such as an auditory melody and a sequence of visual stimuli).\nBasic audiovisual crossmodal correspondences operate between discrete unimodal sensory stimuli, such as, for example, pure tones, color patches, and sounds and lights varying in terms of their intensity (i.e., loudness and brightness). It has sometimes been suggested that certain of these basic crossmodal correspondences may reflect the existence of amodal sensory dimensions, such as intensity, shape, texture, brightness, etc. (e.g., Bond and Stevens,\nMid-level correspondences typically operate between structured combinations of unisensory stimuli, such as short sequences of sounds (that may make up a melody or a spatiotemporally arranged pattern of visual stimuli). Numerous studies, often in the Gestalt tradition, have demonstrated how the grouping of sequences of stimuli presented in one sensory modality may influence the presentation of stimuli presented in the other modality, as for example, in the context of the so-called “crossmodal dynamic capture effect” (Soto-Faraco et al.,\nIn parallel, various researchers have explored audiovisual correspondences using more complex and semantically-rich stimuli. In such cases, the complexity of the stimuli involved means that it is harder to explain any crossmodal correspondences that are observed based on specific individual auditory/visual physical stimulus attributes/dimensions (e.g., such as frequency and hue; see Duthie,\nAt the same time, however, a separate distinction can be made between those studies that have merely sought to establish the existence of crossmodal correspondences between the stimuli presented at a particular level (i.e., basic, mid-level, or complex)—crossmodal matching studies—and those studies that have looked to determine whether there are any crossmodal effects, evidenced by the presence of stimuli in one modality having an impact on those presented in the other modality (see\nBroad categorization of different types of crossmodal correspondence.\nWhile crossmodal correspondences operate between sensory stimuli presented at different levels of complexity (e.g., between individual auditory stimuli and complex visual stimuli), the majority of research that has been published to date has tended to present stimuli at the same level of complexity in both the auditory and visual modalities. Nevertheless, a few studies have been published documenting crossmodal correspondences between stimuli at different levels, such as, for example, demonstrating crossmodal correspondences between short snippets of music and isolated color patches (e.g., Palmer et al.,\nThere are two possibly controversial claims (or observations) that we wish to make in this review: The first is that the mere existence of a crossmodal correspondence, as revealed by research on crossmodal matching, has no necessary implications for the existence of a crossmodal effect; The second is that the existence of crossmodal correspondences operating between individual sensory stimuli has no necessary implications as far as the existence of crossmodal effects operating between complex stimuli are concerned. That is, many crossmodal effects have been demonstrated in the literature without any consideration of the crossmodal congruency, or otherwise, of the component unimodal stimuli (see London,\nIt sometimes appears to be implicitly assumed by researchers working in the area (i.e., in crossmodal correspondences research) that the existence of a crossmodal effect can be taken as evidence of the existence of a crossmodal correspondence between one or several of the sensory features or dimensions involved. As such, the view that we wish to advocate here is that the existence of a crossmodal correspondence can only be convincingly demonstrated by the results of crossmodal matching studies (that is, in studies where the participants are explicitly asked to pick the best match of the available options that have been provided to them\nOver the years, a number of different mechanisms have been put forward to try to explain crossmodal correspondences (Motoki et al.,\nSummary of the various different types of crossmodal correspondence that have been proposed that have to connect auditory and visual stimuli and selected literature sources suggested and/or supported them.\nOne of the most extensively studied areas in research on audiovisual correspondences concerns the associations that have been demonstrated between individual sensory stimuli (or attributes), such as pitch, brightness, or loudness. Over the last 50 years or so, a very wide array of crossmodal matching studies has been published. More recently, a number of studies demonstrating crossmodal effects on perceptual binding as well as in a variety of speeded response tasks that are modulated by the crossmodal correspondences that exist between individual sensory stimuli (attributes or sensory dimensions) have also appeared in the literature; it is to this research that we turn next.\nMany published studies have documented crossmodal correspondences between simple stimuli, or stimulus dimensions (see Spence,\nThere has, however, been widespread disagreement in the case of a putative crossmodal correspondence between auditory pitch and visual hue. Some commentators have been convinced that such a correspondence must exist based on the structural similarity of the underlying sensory dimensions (Gombrich,\nMore recent research has drawn attention to the existence of a number of robust crossmodal correspondences between visual features (i.e., surface textures) and auditory timbre, due to the allegedly intrinsic multisensoriality of timbre characterization/semantics (e.g., Reuter et al.,\nOther researchers have demonstrated the existence of audiovisual crossmodal correspondences between musical timbre and visual shapes (Adeli et al.,\nA wide range of crossmodal correspondence effects have been demonstrated across a wide range of experimental paradigms (e.g., Evans and Treisman,\nOther researchers have started to address the question of whether timbre modulates visual perception. So, for example, in a series of two experiments, Wallmark et al. (\nHence, in summary, there is currently robust experimental evidence for the existence of a wide range of audiovisual crossmodal correspondences involving pitch, such as pitch-size, pitch-brightness, pitch-height, with the exception of pitch-hue correspondences. In the latter case, despite the fact that many commentators are convinced of what the most appropriate crossmodal mapping is (see Spence and Di Stefano,\nMid-level audiovisual crossmodal correspondences operate on dynamically-changing stimuli and/or on combinations of unisensory stimuli (attributes or dimensions; Guo et al.,\nTo date, far fewer crossmodal matching studies have been conducted at this level. What is more, the majority of research that has been published to date would appear to involve cross-level correspondences, such as music-to-color associations for single-line piano melodies (Lindborg and Friberg,\nWhen, for instance, the rate of repetitive auditory stimuli is increased or decreased, while a simultaneously-presented visual flicker remains constant, the latter appears to change accordingly with the auditory stimulus, an effect known as “auditory driving” (Gebhard and Mowbray,\nO'Leary and Rhodes (\nConsider here only how the affective meaning of musical scales depends on their direction—namely, whether they are ascending or descending (Collier and Hubbard,\nExperimental psychologists have long been interested in the crossmodal matching of complex auditory and visual stimuli, as well as on the crossmodal effects of one on the other (see Spence and Di Stefano,\nA number of studies have presented a small selection of stimuli in one sensory modality, with participants being required to match from a pre-selected set of stimuli presented in the other modality (see\nBroad categorization of different types of audiovisual crossmodal matching and crossmodal effects involving complex multi-element aesthetically-meaningful sensory stimuli (see Spence and Di Stefano,\nThe majority of research that has been published to date would appear to be consistent with the claim that complex crossmodal associations between music and paintings are primarily mediated by emotion (see also Hung,\nResearchers have demonstrated that music (or musical emotion) can crossmodally influence the perceived (or at least rated) brightness of paintings (Bhattacharya and Lindsen,\nGiven the research that has been reviewed so far, it would appear clear that crossmodal matching occurs between all possible combinations of individual, organized, and more complex, semantically-meaningful stimuli. Having demonstrated such crossmodal correspondences the next question is how such findings inform attempts at sensory translation or sensory augmentation. At the outset here, one might consider whether crossmodal matching is perhaps more relevant when considering sensory translation, whereas the literature on crossmodal effects may be more pertinent for those wanting to augment art crossmodally. Returning to the two important claims that we put forward earlier, we might say that the mere existence of a crossmodal correspondence (as revealed by research on crossmodal matching), has no necessary implications for the existence of a crossmodal effect. A second claim is that the existence of crossmodal correspondences operating between individual sensory stimuli has no necessary implications as far as the existence of crossmodal effects operating between complex stimuli are concerned. Furthermore, it can also be argued that the absence of a demonstrable crossmodal effect cannot be taken as evidence of the absence of a crossmodal correspondence either. That is, it is at least theoretically plausible, that people might consensually rate certain combinations of stimuli as matching vs. mismatching, without there being any crossmodal effects on performance (e.g., in a speeded classification task). All three of these claims would appear consistent with the various sources of evidence that have been reviewed here.\nThere is an extensive literature on “color music”, sometimes called “visual music” (e.g., Adams,\nMany of those interested in this area invoke the notion of synaesthesia when thinking about engaging multiple senses simultaneously (e.g., see Dimova,\nSeparate from any attempt to use synaesthesia in crossmodal compositions, it should be recognized that certain art forms, such as live opera, inherently stimulate multiple senses; Live music performance too, though this varies all the way from simple inevitability of seeing musicians play in a classic music concert through to the visuals that augment more experimental music (see also McDonald et al.,\nSummary of various kinds of audiovisual artistic/entertainment experience, where music complements/augments the visuals, or vice versa.\nNotice how the synchrony between the auditory and visual elements may sometimes exogenously encourage the integration of the audiovisual inputs (e.g., when attending a classical music concert; see also Muller,\nIwamiya (\nThere has long been artistic interest in the possibility of sensory translation (Kargon,\nA somewhat distinct literature has emerged on the question of the crossmodal sensory augmentation of art. Here, one might consider Scriabin's (\nIntriguingly, although formal evaluation has not yet been conducted, those commentators who have experienced such intentional multisensory, or crossmodal, artistic presentations have, on occasion, hinted at an emergent experience, one that supports the role of Gestalt perceptual grouping in a crossmodal artistic context (though note that the rich notion of multisensory emergence likely reaches beyond the insights offered by the Gestalt approach). Just take the following quote from Wells (\nIn principle, multisensory emergence might be just one example of an extraordinary multisensory experience (e.g., Critchley,\nWhile the term multimedia is broad, and covers multiple forms, there is a sense that some multimedia art works fit the description of crossmodal art in that the stimuli presented to eye and ear are in some sense independent, yet the case is different from the addition of music to film, where the music is seemingly always added as an afterthought (Lipscomb,\nHowes (\nThe notion of intersensoriality can thus help to promote a relational view of the senses, one that resonates with Lévi-Strauss' (\nIn this light, mid-level crossmodal correspondences deserve renewed attention—not as mere by-products of low-level sensory matching or high-level conceptual metaphor, but as hybrid, emergent forms of intersensory pattern recognition. One key research question here thus becomes: What sensory feature, structural property, or dynamic pattern drives these mid-level correspondences? They often resist easy categorization because they operate at the cusp of the perceptual and the conceptual, exhibiting characteristics of both. One might refer here to the “collideroscopic sensorium” (Howes,\nIn their 1928 “Statement on Sound”, Eisenstein, Pudovkin, and Alexandrov advocated the role of sound in film and for “the creation of a new orchestral counterpoint of sight-images and sound-images” (see Eisenstein et al.,\nNote that film and advertising are different inasmuch as people assume that the music and visuals have been deliberately paired in order to achieve some emotional effect or convey some form of semantic meaning (see Bolivar et al.,\nBefore concluding this narrative historical review, it is worth noting how there are a number of challenges, theoretical, practical, attentional, and potentially also ethical when it comes to the crossmodal augmentation of art.\nA clear distinction between crossmodal augmentation in art (as in Scriabin,\nRelatedly, one might draw attention to issues related to the sensory nature of artworks by observing that to have either sensory translation or crossmodally augmented art, the mere simultaneous perception of two sensory inputs is insufficient. For example, hearing Beethoven while viewing a Munch exhibit does not constitute sensory translation or augmentation. And what if a contemporary artist\nThe crossmodal augmentation of art can therefore be seen as requiring more than mere than the simultaneous presentation of the relevant auditory and visual stimuli. It involves a deliberate, non-redundant integration of both, framed within an artistic context that invites interpretation. Crucially, the observer's awareness of this intentional combination transforms the experience from mere coexistence into a meaningful, augmented artistic phenomenon.\nIt is worth pausing here to consider Behne's (\nSeparately, should any putative crossmodal augmentation rely on precise crossmodal temporal synchronization then there may be challenges around ensuring that different sensory inputs/elements align perfectly in real time (e.g., matching sound with visual effects). This can be problematic: Interactive or AI-driven experiences sometimes suffer from technology-induced lag in one or more senses, thus reducing the immersiveness of the experience. One might be reminded here of Bruce Naumann's (1969) artwork “Lip Sync” (see\nThe Danish philosopher Kierkegaard (\nOne argument suggested here is that part of what makes art powerful is its ability to isolate, emphasize, or distill specific aspects of perception. Unlike daily life, where we are bombarded with multisensory input that we must constantly filter, artistic experiences often provide a kind of structured reduction—a selective framing of reality that allows for deeper contemplation. For instance, a still life with fruits and bottles strips away movement, sound, odors, and so forces us to focus on composition, color, and form instead. This could explain why we value these aesthetic experiences, because they allow us to maximize sensory engagement. Reintegrating sensory modalities in aesthetic experience to augment them implies adding more sensory information (haptics, olfaction, spatial audio, interactive visuals, etc.). The danger is that this may merely increase cognitive load, making it harder to focus on any one aspect, thus undermining the essence of art and artistic contemplation. Is multisensoriality in art a form of redundancy—where different sensory modalities repeat the same message—or does it truly open new aesthetic dimensions?\nThe argument seemingly has some cognitive implications. Freeing the senses from the burden of managing excessive stimuli implies freeing intellectual resources for deeper reflection and cognitive engagement. A key argument in favor of art as sensory fragmentation is that it does that to free the mind, allowing for more profound conceptual engagement. By narrowing the range of sensory input, art can liberate cognitive capacity for interpretation, imagination, and emotional resonance. In minimalist art, the deliberate reduction of visual complexity forces the viewer into an active intellectual engagement with form and space. By contrast, technologically-augmented multisensory experiences might do the opposite. Consider only the use of virtual/augmented reality (VR/AR) to augment aesthetic experience (e.g., Cho et al.,\nCertainly, not all theorists/critics have necessarily appreciated the desire to engage more senses in art: As Arnheim stated in an essay entitled, with clear reference to the distinction among the arts in Lessing's 18th-century aesthetics, A New Laocoön: “in their attempts to attract the audience, two media are fighting each other instead of capturing it by a united effort” (Arnheim,\nFinally, it is worth briefly considering the ethical question that was raised by David Lomas (from the Art History department at the University of Manchester), in relation to Flying Object's crossmodal augmentation of four works of visual art from the permanent collection at Tate Britain through the use of scent, sound, virtual touch, and even chocolate in 2015 (see Pursey and Lomas,\nOne of the key points to emerge from the present narrative historical review is that it may not be possible to use elemental correspondences (that is, crossmodal correspondences based on simple stimulus properties), given that works of art are mostly complex, multi-elemental, and semantically/emotionally meaningful. This conclusion seemingly contradicting Eisenstein's suggestion that there is no “pervading law of absolute meanings and correspondences between colors and sound.” (as quoted in Harrison,\nOne question for the future that we haven't had the opportunity to consider here is whether musical knowledge/expertise may affect audiovisual crossmodal correspondences and hence the way in which music and visuals interact in experience (Di Stefano et al.,", "topic": "Brain"}
{"pmid": "40265989", "pmcid": "12305474", "title": "Two neurocognitive domains identified for patients with myalgic encephalomyelitis/chronic fatigue syndrome and post-acute sequelae of COVID-19", "publication_year": "N/A", "abstract": "Patients with Myalgic Encephalomyelitis/Chronic Fatigue Syndrome (ME/CFS) and Post-Acute Sequelae of COVID-19 (PASC) often have neurocognitive complaints that involve memory and concentration problems and difficulties paying attention. Other neurocognitive domains such as hypersensitivity to noise and light have rarely been included as aspects of neurocognitive impairment for these post-viral conditions. The current study evaluated a more extensive list of neurocognitive items for a group of 2,313 patients with ME/CFS and 299 patients with PASC. Exploratory factor analyses found two factors for each patient group, one involving classic memory and concentration symptoms and the other involving sensory overload phenomena. The findings suggest that researchers might consider expanding the types of self-report neurocognitive symptoms among patients with these post-viral illnesses.", "full_text": "Neurocognitive impairment that results in difficulty remembering, concentrating, and making decisions is a central feature of Myalgic Encephalomyelitis (ME/CFS) (\nMany who have been diagnosed with Post-Acute Sequelae of COVID-19 (PASC) also exhibit neurocognitive problems (\nIn both ME/CFS and PASC, there has been a reported reduction in cerebral blood flow, which may contribute to symptoms of fatigue and cognition (\nTwo ME/CFS case definitions provide a more expanded list of neurocognitive symptoms. The Canadian Consensus Criteria definition ME/CFS (\nBased on the Canadian Case Criteria (\nThe dataset for the current study was aggregated across a variety of international samples and included 2,313 patients with ME/CFS and 299 patients with PASC. The sociodemographic information for this sample is reported in\nSociodemographic information for the samples with ME/CFS and long-COVID.\nDePaul University recruited an international convenience sample of adults who self-identified as having ME/CFS. Eligibility criteria included being at least 18 years of age, having a self-reported and current diagnosis of CFS or ME, and the ability to read and write in English. There was a total of 217 adults with available data, but three were excluded due to incomplete data. This present study included a total of 214 participants who were predominantly female (84/0%), with a mean age of 52.0 years (\nThe BioBank sample was collected by the Solve ME/CFS Initiative\nDue to a suspected diagnosis of CFS, all participants in this sample were referred to the Newcastle-upon-Tyne Royal Victoria Infirmary clinic in Great Britain. An experienced physician conducted a comprehensive examination and medical history. Due to incomplete data, five participants were excluded. With a majority of the 95 participants used in this current study being female (82.1%), 50.0% obtained at least a standard college degree and the mean age was 45.8 years (\nParticipants were recruited from southern Norway and contacted via healthcare professionals, ME/CFS patient organizations, and the waiting list for a patient education program. Individuals living with ME/CFS were invited to participate in a randomized controlled trial for a ME/CFS self-management program. To be eligible, participants were required to have a diagnosis of ME/CFS by a physician or medical specialist, be at least 18 years of age, and be physically able to attend the self-management program. Those who decided to participate were confirmed to have a ME/CFS diagnosis. There was a total of 176 participants and 173 included in the present study (3 participants had incomplete data). Approximately half the sample (50.3%) completed at least a standard college degree. The sample had a mean age of 43.3 years (\nParticipants were recruited from an outpatient clinic at a multidisciplinary ME/CFS center and from an inpatient medical ward for severely ill patients. Patients were required to be able to read and write in Norwegian and to be between 18 and 65 years of age. All participants suspected of a diagnosis of ME/CFS participated in comprehensive medical examination and history conducted by an experienced psychologist and physician. Due to missing data, 60 of the 64 original participants were included in the present study. Less than half the sample (38.3%) had completed at least a standard college degree. The sample was 81.7% female, with a mean age of 35.4 years (\nAll participants, recruited from a tertiary care center specializing in ME/CFS, were examined by an experienced physician and determined to meet ME/CFS criteria. Eligibility criteria included being between 18 and 65 years of age and being able to read and write in Norwegian. There was a total of 175 participants, but the current study included 169 with six being excluded due to incomplete data. The majority were female (81.7%), more than half (57.4%) received at least a standard college degree and the mean age was 38.6 years (\nA convenience sample of adults living with chronic illnesses was collected by DePaul University as a part of a larger study (\nParticipants were recruited from physician clinics specializing in ME/CFS and the ME Japan association\nRecruited by a specialist physician with experience diagnosing ME/CFS from a tertiary referral center in Barcelona, Spain, participants were surveyed using Research Electronic Data Capture (REDCap), a tool used for online data collection (\nParticipants were recruited from a group of individuals with a physician diagnosis of ME/CFS, and the patients had been referred to an outpatient clinic in the Netherlands (the CFS Medical Center in Amsterdam) specializing in ME/CFS. There was a total of 364 participants. After excluding 8 participants due to incomplete date, 356 participants were included in the current study. Less than half the sample (42.1%) obtained at least a standard college degree. The sample had a mean age of 37.0 years (\nIn August 2020, study recruitment information was posted on several social media sites to recruit those who self-reported not recovering from COVID-19. Participants were asked to describe current symptoms (\nParticipants from both datasets completed the DePaul Symptom Questionnaire (DSQ-1) (\nIn the DSQ-1, there is one item that measures patients’ level of impairment by answering a 7-point Likert scale item that assesses the severity of participants’ impairment. This 7-point Likert scale was re-coded into two levels: severe impairment (e.g., bedbound or homebound) and moderate (e.g., able to work part time and leave the house but did not have energy for other activities) impairment.\nParticipants were removed from analyses in the current study if they were missing more than 10% of items from the DSQ-1. Participants could have missing data for either the severity, the frequency, or for both dimensions of a symptom. The missing values of the remaining participants were replaced with a method dependent on the nature of the missing value. For further details on how the missing values were replaced, please see Conroy, Islam, and Jason (\nIBM SPSS Statistics version 25 was used for all analyses (IBM Corps, 2017). Using the 100-point symptom composite scores, an exploratory factor analysis (EFA) was performed. A Promax rotation (kappa = 4) was used to allow the factors to correlate, and the principal axis factoring method was selected to determine the maximum amount of common variance between the factors. To determine the appropriate number of factors to retain, we constructed a parallel analysis using 5,000 iterations of our data using permutations and compared the changes in eigenvalues across consecutive factors to those of our actual data. If the respective eigenvalue exceeded that of the random data based on a 95% confidence interval, factors from the actual data were retained. Among the retained factors, those appearing before the inflection point of the scree plot were assumed to be meaningful. Symptoms that did not load onto any of the factors (rotated loading < 0.3) were dropped, and the analysis was repeated until all symptoms loaded onto a factor.\nLastly, we performed one-way ANOVAs to compare severe versus moderate impairment with the scores of factors from the exploratory factor analysis.\nIn the ME/CFS sample, the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy (0.94) and Bartlett’s Test of Sphericity [χ\nFactor loadings of neurocognitive domain items in participants with ME/CFS.\nN = 2,313.\nIn the PASC sample, the KMO measure of sampling adequacy (0.94) and Bartlett’s Test of Sphericity [χ\nFactor loadings of neurocognitive domain items in participants with PASC.\nIn the ME/CFS sample, we examined whether those with more functional impairment had worse scores on the two neurocognitive factors than those with less impairment (see\nMeans and standard deviations of factor scores and levels of impairment.\nIn both samples, two similar factors emerged. Factor 1 involved neurocognitive symptoms specific to impairments with executive dysfunction, memory, and concentration. Factor 2 contained symptoms more specific to hypersensitivities and sensory and perceptual disturbances. In both ME/CFS and PASC samples, individuals who reported more severe impairment in their physical functioning had worse neurocognitive symptoms for both concentration/memory and sensory/perceptual factors. This finding provides some validation for the notion that these neurocognitive symptoms are related to individuals’ functional status.\nOne possible explanation of the neurocognitive symptoms is Menon’s (\nFactor 2 contains symptoms more specific to hypersensitivities involving sensory and perceptual disturbances, which have been found among patients with PASC (\nThere are several limitations in the current study. The sample sizes were not equal with considerably more patients in the ME/CFS sample. In addition, the findings are based on self-report measures and diagnoses of ME/CFS and PASC, so patients’ diagnoses were not confirmed by medical evaluations. Furthermore, neurocognitive impairment was assessed by a self-report inventory but not by biological measures. Although there is some concurrent validity based on measures of functional status, there is a need to relate these two neurocognitive domains to a wider battery of biobehavioral measures. It is of note that there are several medical conditions comorbid with ME/CFS that have also been related to abnormalities in the salience network (\nClassifying the mechanisms behind the complex neurocognitive symptomology of ME/CFS and PASC could provide physicians and patients with a better understanding of the diseases’ symptoms. Our findings suggest that the neural mechanisms behind the neurocognitive symptoms of post-viral illnesses might involve more areas including those involving sensory overload.", "content_for_embedding": "Neurocognitive impairment that results in difficulty remembering, concentrating, and making decisions is a central feature of Myalgic Encephalomyelitis (ME/CFS) (\nMany who have been diagnosed with Post-Acute Sequelae of COVID-19 (PASC) also exhibit neurocognitive problems (\nIn both ME/CFS and PASC, there has been a reported reduction in cerebral blood flow, which may contribute to symptoms of fatigue and cognition (\nTwo ME/CFS case definitions provide a more expanded list of neurocognitive symptoms. The Canadian Consensus Criteria definition ME/CFS (\nBased on the Canadian Case Criteria (\nThe dataset for the current study was aggregated across a variety of international samples and included 2,313 patients with ME/CFS and 299 patients with PASC. The sociodemographic information for this sample is reported in\nSociodemographic information for the samples with ME/CFS and long-COVID.\nDePaul University recruited an international convenience sample of adults who self-identified as having ME/CFS. Eligibility criteria included being at least 18 years of age, having a self-reported and current diagnosis of CFS or ME, and the ability to read and write in English. There was a total of 217 adults with available data, but three were excluded due to incomplete data. This present study included a total of 214 participants who were predominantly female (84/0%), with a mean age of 52.0 years (\nThe BioBank sample was collected by the Solve ME/CFS Initiative\nDue to a suspected diagnosis of CFS, all participants in this sample were referred to the Newcastle-upon-Tyne Royal Victoria Infirmary clinic in Great Britain. An experienced physician conducted a comprehensive examination and medical history. Due to incomplete data, five participants were excluded. With a majority of the 95 participants used in this current study being female (82.1%), 50.0% obtained at least a standard college degree and the mean age was 45.8 years (\nParticipants were recruited from southern Norway and contacted via healthcare professionals, ME/CFS patient organizations, and the waiting list for a patient education program. Individuals living with ME/CFS were invited to participate in a randomized controlled trial for a ME/CFS self-management program. To be eligible, participants were required to have a diagnosis of ME/CFS by a physician or medical specialist, be at least 18 years of age, and be physically able to attend the self-management program. Those who decided to participate were confirmed to have a ME/CFS diagnosis. There was a total of 176 participants and 173 included in the present study (3 participants had incomplete data). Approximately half the sample (50.3%) completed at least a standard college degree. The sample had a mean age of 43.3 years (\nParticipants were recruited from an outpatient clinic at a multidisciplinary ME/CFS center and from an inpatient medical ward for severely ill patients. Patients were required to be able to read and write in Norwegian and to be between 18 and 65 years of age. All participants suspected of a diagnosis of ME/CFS participated in comprehensive medical examination and history conducted by an experienced psychologist and physician. Due to missing data, 60 of the 64 original participants were included in the present study. Less than half the sample (38.3%) had completed at least a standard college degree. The sample was 81.7% female, with a mean age of 35.4 years (\nAll participants, recruited from a tertiary care center specializing in ME/CFS, were examined by an experienced physician and determined to meet ME/CFS criteria. Eligibility criteria included being between 18 and 65 years of age and being able to read and write in Norwegian. There was a total of 175 participants, but the current study included 169 with six being excluded due to incomplete data. The majority were female (81.7%), more than half (57.4%) received at least a standard college degree and the mean age was 38.6 years (\nA convenience sample of adults living with chronic illnesses was collected by DePaul University as a part of a larger study (\nParticipants were recruited from physician clinics specializing in ME/CFS and the ME Japan association\nRecruited by a specialist physician with experience diagnosing ME/CFS from a tertiary referral center in Barcelona, Spain, participants were surveyed using Research Electronic Data Capture (REDCap), a tool used for online data collection (\nParticipants were recruited from a group of individuals with a physician diagnosis of ME/CFS, and the patients had been referred to an outpatient clinic in the Netherlands (the CFS Medical Center in Amsterdam) specializing in ME/CFS. There was a total of 364 participants. After excluding 8 participants due to incomplete date, 356 participants were included in the current study. Less than half the sample (42.1%) obtained at least a standard college degree. The sample had a mean age of 37.0 years (\nIn August 2020, study recruitment information was posted on several social media sites to recruit those who self-reported not recovering from COVID-19. Participants were asked to describe current symptoms (\nParticipants from both datasets completed the DePaul Symptom Questionnaire (DSQ-1) (\nIn the DSQ-1, there is one item that measures patients’ level of impairment by answering a 7-point Likert scale item that assesses the severity of participants’ impairment. This 7-point Likert scale was re-coded into two levels: severe impairment (e.g., bedbound or homebound) and moderate (e.g., able to work part time and leave the house but did not have energy for other activities) impairment.\nParticipants were removed from analyses in the current study if they were missing more than 10% of items from the DSQ-1. Participants could have missing data for either the severity, the frequency, or for both dimensions of a symptom. The missing values of the remaining participants were replaced with a method dependent on the nature of the missing value. For further details on how the missing values were replaced, please see Conroy, Islam, and Jason (\nIBM SPSS Statistics version 25 was used for all analyses (IBM Corps, 2017). Using the 100-point symptom composite scores, an exploratory factor analysis (EFA) was performed. A Promax rotation (kappa = 4) was used to allow the factors to correlate, and the principal axis factoring method was selected to determine the maximum amount of common variance between the factors. To determine the appropriate number of factors to retain, we constructed a parallel analysis using 5,000 iterations of our data using permutations and compared the changes in eigenvalues across consecutive factors to those of our actual data. If the respective eigenvalue exceeded that of the random data based on a 95% confidence interval, factors from the actual data were retained. Among the retained factors, those appearing before the inflection point of the scree plot were assumed to be meaningful. Symptoms that did not load onto any of the factors (rotated loading < 0.3) were dropped, and the analysis was repeated until all symptoms loaded onto a factor.\nLastly, we performed one-way ANOVAs to compare severe versus moderate impairment with the scores of factors from the exploratory factor analysis.\nIn the ME/CFS sample, the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy (0.94) and Bartlett’s Test of Sphericity [χ\nFactor loadings of neurocognitive domain items in participants with ME/CFS.\nN = 2,313.\nIn the PASC sample, the KMO measure of sampling adequacy (0.94) and Bartlett’s Test of Sphericity [χ\nFactor loadings of neurocognitive domain items in participants with PASC.\nIn the ME/CFS sample, we examined whether those with more functional impairment had worse scores on the two neurocognitive factors than those with less impairment (see\nMeans and standard deviations of factor scores and levels of impairment.\nIn both samples, two similar factors emerged. Factor 1 involved neurocognitive symptoms specific to impairments with executive dysfunction, memory, and concentration. Factor 2 contained symptoms more specific to hypersensitivities and sensory and perceptual disturbances. In both ME/CFS and PASC samples, individuals who reported more severe impairment in their physical functioning had worse neurocognitive symptoms for both concentration/memory and sensory/perceptual factors. This finding provides some validation for the notion that these neurocognitive symptoms are related to individuals’ functional status.\nOne possible explanation of the neurocognitive symptoms is Menon’s (\nFactor 2 contains symptoms more specific to hypersensitivities involving sensory and perceptual disturbances, which have been found among patients with PASC (\nThere are several limitations in the current study. The sample sizes were not equal with considerably more patients in the ME/CFS sample. In addition, the findings are based on self-report measures and diagnoses of ME/CFS and PASC, so patients’ diagnoses were not confirmed by medical evaluations. Furthermore, neurocognitive impairment was assessed by a self-report inventory but not by biological measures. Although there is some concurrent validity based on measures of functional status, there is a need to relate these two neurocognitive domains to a wider battery of biobehavioral measures. It is of note that there are several medical conditions comorbid with ME/CFS that have also been related to abnormalities in the salience network (\nClassifying the mechanisms behind the complex neurocognitive symptomology of ME/CFS and PASC could provide physicians and patients with a better understanding of the diseases’ symptoms. Our findings suggest that the neural mechanisms behind the neurocognitive symptoms of post-viral illnesses might involve more areas including those involving sensory overload.", "topic": "Brain"}
{"pmid": "40205747", "pmcid": "12301348", "title": "Slow-paced breathing reduces anxiety and enhances midfrontal alpha asymmetry, buffering responses to aversive visual stimuli", "publication_year": "N/A", "abstract": "", "full_text": "Breathing is an essential autonomic function for sustaining life and a crucial physiological process that bidirectionally interacts with emotion. For example, slow-paced breathing is used to promote mental calmness and has been clinically applied to alleviate anxiety and stress (\nFrontal alpha asymmetry (FAA) has been investigated as a neurophysiological marker of emotional and cognitive processes. Initially described in relation to approach and avoidance motivation (\nNevertheless, an accumulating body of electrophysiological evidence indicates that respiration modulates cortical α oscillations. A magnetoencephalography study demonstrated that the phase of the respiratory cycle modulates posterior alpha power, and these fluctuations predict near-threshold visual perception (\nSlow-paced breathing reliably attenuates anxiety during and immediately after practice, yet whether this benefit endures once a stressor intervenes remains uncertain. Controlled experiments demonstrate rapid declines in self-reported anxiety and sympathetic activity while participants follow paced-breathing protocols (\nAccordingly, this study aimed to test whether slow-paced breathing modulates state anxiety and rLFA, and whether these effects present immediately after exposure to aversive visual stimuli. We hypothesised that slow-paced breathing would reduce state anxiety and enhance rLFA relative to normal breathing, with these benefits being observable in the period immediately following the stimuli. As an exploratory aim, we examined autonomic and respiratory parameters—heart-rate variability, respiratory rate, respiratory variability, and end-tidal CO₂—to gain a comprehensive view of the underlying physiological mechanisms.\nThis study included 17 healthy undergraduate and graduate students (7 females, 10 males). The mean age of participants was 22.18 years [standard deviation (SD) = 2.77, range: 20–30 years]. All participants were assessed as being right-handed according to the Edinburgh Handedness Inventory (\nSubjective anxiety was assessed using the Japanese version of the State–Trait Anxiety Inventory (STAI-JYZ;\nElectroencephalographic (EEG) data were recorded from 32 scalp sites (see\nHeart rate variability (HRV) was derived from electrocardiogram (ECG) recordings. We used a modified Lead II configuration, with Ag/AgCl electrodes (NE-113A, Nihon Kohden) placed on the right clavicle and the left lower rib cage. The ground electrode was placed on the left clavicle. ECG data were sampled at 1 kHz and synchronized with the EEG recordings.\nThe expired CO\nThe experiment was conducted in an electromagnetically shielded room with controlled temperature and humidity. Environmental conditions were maintained at 22–24°C and 40 ± 5% relative humidity to minimize external environmental interference. All experiments were conducted between 13:00 and 17:00 to account for potential physiological variations due to the time of day.\nParticipants were instructed to abstain from alcohol and caffeine for 12 h before the experiment and to complete their meals at least 2 h before the experiment. Upon arrival at the laboratory, participants completed questionnaires regarding basic attributes (sex, age) and handedness. For physiological measurements, participants were instructed to empty their bladder, after which they were seated in a reclining chair and an experimenter attached the EEG, EOG, and ECG electrodes and the expired CO\nExperimental protocol timeline for investigating the effects of breathing tasks on aversive visual stimuli responses. The experimental protocol constituted two counterbalanced sessions (slow-paced breathing [SB] and resting breathing [RB]), with each session comprising five phases: (1) Pre-task baseline measurement with 2 min crosshair fixation, (2) Breathing task (5 min duration, either 4 s inspiration/6 s expiration for SB or natural breathing rate for RB), (3) Post-task baseline measurement with 2 min crosshair fixation, (4) Aversive visual stimuli presentation (2 min exposure to International Affective Picture System (IAPS) images), and (5) Post-stimuli measurement with 2 min crosshair fixation. The State–Trait Anxiety Inventory-State (STAI-S) was administered after each fixation period. Sessions were separated by a minimum 2 h rest interval, with the order of breathing conditions randomized among participants.\nThe two sessions differed in their breathing task conditions, which were counterbalanced across participants using a randomized block design. Participants in the slow-paced breathing (SB) session followed visual cues to maintain a breathing pattern of 4 s inhalation and 6 s exhalation for 5 min, a duration shown to be sufficient for inducing physiological changes in previous studies (\nThe aversive stimuli comprised 40 images selected from the International Affective Picture System (IAPS). These images were selected from a subset of 69 IAPS images previously validated to induce anxiety (\nPrior to preprocessing, all channels were visually inspected for quality. No channels were excluded or required interpolation in this study. The EEG data were analyzed using Brain Vision Analyzer 2.2 (Brain Products GmbH). The EEG data were recorded at 1,000 Hz, re-referenced using the average reference method, and then down sampled to 250 Hz. A zero-phase fourth-order Butterworth bandpass filter (0.1–30 Hz) and a 50 Hz notch filter were applied.\nEEG data were segmented into 2-min intervals corresponding to the Pre-task, Post-task, and Post-stimuli crosshair fixation periods. Independent Component Analysis (ICA) was performed on these segmented data to remove eye movement artifacts (\nThe corrected EEG data were further segmented into 2-s epochs with 50% overlap. Epochs with remaining artifacts exceeding ±100 μV were excluded. However, no epochs were excluded after ICA-based artifact correction.\nFrontal alpha asymmetry (8–13 Hz) was computed for the midfrontal (F4-F3) and lateral frontal (F8-F7) regions using the natural logarithm transformation: rLFA = ln(right) − ln(left). Power spectral density was estimated using Fast Fourier Transform (FFT) after applying a Hanning window (50% overlap). The employed EEG data processing and analysis procedures were based on established guidelines for frontal EEG asymmetry research (\nHRV analysis was performed using the HRVTool (version 1.07) in MATLAB R2020b (MathWorks) (\nExpired CO₂ waveform data were down sampled to 250 Hz to synchronize with the EEG data and analyzed using custom MATLAB R2020b scripts (MathWorks). A fourth-order Butterworth low-pass filter with a 2 Hz cutoff frequency was applied to reduce noise, followed by zero-phase digital filtering. Moreover, we developed a peak detection algorithm for ETCO₂ detection, with the following parameters: a minimum peak interval of 1.0 s, a minimum peak height equal to the signal mean, and a minimum peak prominence equal to the signal standard deviation. The instantaneous respiratory rate was derived from the time intervals between consecutive ETCO₂ peaks, with values exceeding the median ± 2 standard deviations (SD) removed as outliers. Finally, we computed the mean respiratory rate (RR), the coefficient of variation of respiratory intervals (CVRR), and mean ETCO₂ values from the processed data.\nLinear mixed models (LMM) were employed to analyze psychological measures (STAI-S) and physiological indices (rLFA at F4-F3 and F8-F7, RMSSD, LF/HF ratio, RR, CVRR, and ETCO\nFor each model, the intervention condition (Factor 1: SB, RB) and measurement time point (Factor 2: post-task, post-stimuli) were specified as fixed effects. The corresponding baseline value from the pre-task period was included as a covariate to control for individual baseline differences. We initially attempted to fit models that also included random slopes for the effects of condition and time, but these more complex models failed to converge for some outcomes. Therefore, to ensure model stability and reliable results across all variables, we proceeded with a more parsimonious model that included only random intercepts for participants.\nModel fit was evaluated using marginal and conditional R2 values, and residual diagnostics were assessed using QQ plots. Estimated marginal means (EMM) and 95% confidence intervals (CI) were calculated and visualized. Analysis of variance (ANOVA) was conducted using Type III tests with Satterthwaite’s method for denominator degrees-of-freedom to assess the significance of fixed effects in the LMMs.\nStatistical significance was set at\nThe mean respiratory rate was 6.00 ± 0.11 breaths/min during SB condition and 14.76 ± 3.43 breaths/min during RB condition. These values indicated that participants successfully adhered to the instructed breathing patterns in each condition. The following sections present the results for each physiological and psychological variable. Detailed outputs for the fixed-effects estimates from all linear mixed models are presented in\nThe model fit evaluation indicated a good fit, with R\nEffects of breathing interventions on state anxiety and relative left frontal activity. Temporal changes in State–Trait Anxiety Inventory-State (STAI-S) scores (left), midfrontal (F4-F3) rLFA (middle), and lateral frontal (F8-F7) rLFA (right) during the experimental protocol. Solid purple lines with circles represent the slow-paced breathing (SB) condition; dashed green lines with triangles represent the resting breathing (RB) condition. Data are presented as estimated marginal means (EMM) with 95% confidence intervals (CI) for post-task and post-stimuli periods, while pre-task data show the raw means with 95% CI. The vertical dashed line separates pre-task from intervention periods. Asterisks indicate significant differences between conditions (*\nThe model fit evaluation indicated a good fit, with R\nAs illustrated in\nPower Spectral Density Analysis of Frontal and Frontotemporal EEG Activity. EEG power spectra from frontal and frontotemporal electrodes across three experimental periods (pre-task, post-task, and post-stimuli). The alpha band (8–13 Hz) is highlighted in gray. The rows display data from the slow breathing (SB; rows\nThe model fit evaluation showed R\nExamination of power spectral density patterns in\nThe model fit evaluation indicated a good fit, with R\nEffects of breathing interventions on autonomic and respiratory parameters.\nThe model fit evaluation showed R\nThe model fit evaluation showed R\nThe model fit evaluation showed R\nThe model fit evaluation indicated a good fit, with R\nThis study examined the effects of SB on state anxiety and rLFA compared to RB and assessed whether these effects persisted immediately after exposure to aversive visual stimuli. The key findings were as follows: (1) SB was significantly more effective than RB in reducing state anxiety post-task, an effect that subsequently buffered against the anxiety increase following aversive stimuli; (2) SB led to a significant increase in rLFA at the midfrontal site (F4-F3) compared to RB, and this effect also persisted immediately after aversive visual stimuli exposure; (3) A significant interaction was observed for RMSSD, an index of parasympathetic nervous system activity. Specifically, during the post-task period, RMSSD was lower in SB condition, but following aversive stimuli, it significantly increased only in SB condition.\nThe finding that SB buffered against the anxiety response to aversive stimuli is consistent with previous research on the anxiolytic effects of controlled breathing (\nIn parallel with the reduction in state anxiety, SB condition produced and maintained a left-lateralized shift in midfrontal rLFA. Although early work framed left-frontal dominance within an approach–withdrawal model of emotion (\nSB increased left-lateralized rLFA at the midfrontal site (F4–F3) but did not alter the lateral site (F8–F7), indicating regional specificity in prefrontal responsiveness. The midline electrodes overlie medial premotor and dorsomedial prefrontal cortex, nodes of the central autonomic network that receive respiration-synchronous input from brain-stem nuclei and contribute to interoceptive regulation (\nThe neural mechanisms underlying rLFA changes in response to SB remain unclear, but several possibilities can be considered. Previous studies have shown that rLFA is influenced by dopaminergic system activity (\nAlthough slow breathing usually elevates RMSSD by strengthening cardiac vagal activity, we observed a transient reduction during the immediate post-task phase, followed by a marked increase after the stimuli presentation. This pattern can be understood by considering how respiratory sinus arrhythmia (RSA) influences RMSSD. High CVRR values in the post-task window indicated unstable breathing cycles; such variability disperses RSA amplitude across successive beats and can therefore underestimate RMSSD despite genuine vagal activation (\nETCO\nThis study has several limitations. The lack of pre-registration and\nIn conclusion, this study provides initial evidence that a brief session of slow-paced breathing can reduce state anxiety and increase relative left frontal activity, with effects persisting immediately after aversive stimuli. These findings highlight the practical potential of SB as an accessible and cost-effective intervention for mitigating anxiety and preventing stress response escalation. However, these promising results must be interpreted with caution due to this study’s limitations, including a small sample size and a cross-sectional design. Therefore, future research is essential to confirm these preliminary findings. In particular, longitudinal studies with larger and more diverse clinical populations are warranted. For instance, neurological disorders such as Parkinson’s disease are frequently accompanied by psychiatric symptoms, including anxiety, which can negatively impact motor function (", "content_for_embedding": "Breathing is an essential autonomic function for sustaining life and a crucial physiological process that bidirectionally interacts with emotion. For example, slow-paced breathing is used to promote mental calmness and has been clinically applied to alleviate anxiety and stress (\nFrontal alpha asymmetry (FAA) has been investigated as a neurophysiological marker of emotional and cognitive processes. Initially described in relation to approach and avoidance motivation (\nNevertheless, an accumulating body of electrophysiological evidence indicates that respiration modulates cortical α oscillations. A magnetoencephalography study demonstrated that the phase of the respiratory cycle modulates posterior alpha power, and these fluctuations predict near-threshold visual perception (\nSlow-paced breathing reliably attenuates anxiety during and immediately after practice, yet whether this benefit endures once a stressor intervenes remains uncertain. Controlled experiments demonstrate rapid declines in self-reported anxiety and sympathetic activity while participants follow paced-breathing protocols (\nAccordingly, this study aimed to test whether slow-paced breathing modulates state anxiety and rLFA, and whether these effects present immediately after exposure to aversive visual stimuli. We hypothesised that slow-paced breathing would reduce state anxiety and enhance rLFA relative to normal breathing, with these benefits being observable in the period immediately following the stimuli. As an exploratory aim, we examined autonomic and respiratory parameters—heart-rate variability, respiratory rate, respiratory variability, and end-tidal CO₂—to gain a comprehensive view of the underlying physiological mechanisms.\nThis study included 17 healthy undergraduate and graduate students (7 females, 10 males). The mean age of participants was 22.18 years [standard deviation (SD) = 2.77, range: 20–30 years]. All participants were assessed as being right-handed according to the Edinburgh Handedness Inventory (\nSubjective anxiety was assessed using the Japanese version of the State–Trait Anxiety Inventory (STAI-JYZ;\nElectroencephalographic (EEG) data were recorded from 32 scalp sites (see\nHeart rate variability (HRV) was derived from electrocardiogram (ECG) recordings. We used a modified Lead II configuration, with Ag/AgCl electrodes (NE-113A, Nihon Kohden) placed on the right clavicle and the left lower rib cage. The ground electrode was placed on the left clavicle. ECG data were sampled at 1 kHz and synchronized with the EEG recordings.\nThe expired CO\nThe experiment was conducted in an electromagnetically shielded room with controlled temperature and humidity. Environmental conditions were maintained at 22–24°C and 40 ± 5% relative humidity to minimize external environmental interference. All experiments were conducted between 13:00 and 17:00 to account for potential physiological variations due to the time of day.\nParticipants were instructed to abstain from alcohol and caffeine for 12 h before the experiment and to complete their meals at least 2 h before the experiment. Upon arrival at the laboratory, participants completed questionnaires regarding basic attributes (sex, age) and handedness. For physiological measurements, participants were instructed to empty their bladder, after which they were seated in a reclining chair and an experimenter attached the EEG, EOG, and ECG electrodes and the expired CO\nExperimental protocol timeline for investigating the effects of breathing tasks on aversive visual stimuli responses. The experimental protocol constituted two counterbalanced sessions (slow-paced breathing [SB] and resting breathing [RB]), with each session comprising five phases: (1) Pre-task baseline measurement with 2 min crosshair fixation, (2) Breathing task (5 min duration, either 4 s inspiration/6 s expiration for SB or natural breathing rate for RB), (3) Post-task baseline measurement with 2 min crosshair fixation, (4) Aversive visual stimuli presentation (2 min exposure to International Affective Picture System (IAPS) images), and (5) Post-stimuli measurement with 2 min crosshair fixation. The State–Trait Anxiety Inventory-State (STAI-S) was administered after each fixation period. Sessions were separated by a minimum 2 h rest interval, with the order of breathing conditions randomized among participants.\nThe two sessions differed in their breathing task conditions, which were counterbalanced across participants using a randomized block design. Participants in the slow-paced breathing (SB) session followed visual cues to maintain a breathing pattern of 4 s inhalation and 6 s exhalation for 5 min, a duration shown to be sufficient for inducing physiological changes in previous studies (\nThe aversive stimuli comprised 40 images selected from the International Affective Picture System (IAPS). These images were selected from a subset of 69 IAPS images previously validated to induce anxiety (\nPrior to preprocessing, all channels were visually inspected for quality. No channels were excluded or required interpolation in this study. The EEG data were analyzed using Brain Vision Analyzer 2.2 (Brain Products GmbH). The EEG data were recorded at 1,000 Hz, re-referenced using the average reference method, and then down sampled to 250 Hz. A zero-phase fourth-order Butterworth bandpass filter (0.1–30 Hz) and a 50 Hz notch filter were applied.\nEEG data were segmented into 2-min intervals corresponding to the Pre-task, Post-task, and Post-stimuli crosshair fixation periods. Independent Component Analysis (ICA) was performed on these segmented data to remove eye movement artifacts (\nThe corrected EEG data were further segmented into 2-s epochs with 50% overlap. Epochs with remaining artifacts exceeding ±100 μV were excluded. However, no epochs were excluded after ICA-based artifact correction.\nFrontal alpha asymmetry (8–13 Hz) was computed for the midfrontal (F4-F3) and lateral frontal (F8-F7) regions using the natural logarithm transformation: rLFA = ln(right) − ln(left). Power spectral density was estimated using Fast Fourier Transform (FFT) after applying a Hanning window (50% overlap). The employed EEG data processing and analysis procedures were based on established guidelines for frontal EEG asymmetry research (\nHRV analysis was performed using the HRVTool (version 1.07) in MATLAB R2020b (MathWorks) (\nExpired CO₂ waveform data were down sampled to 250 Hz to synchronize with the EEG data and analyzed using custom MATLAB R2020b scripts (MathWorks). A fourth-order Butterworth low-pass filter with a 2 Hz cutoff frequency was applied to reduce noise, followed by zero-phase digital filtering. Moreover, we developed a peak detection algorithm for ETCO₂ detection, with the following parameters: a minimum peak interval of 1.0 s, a minimum peak height equal to the signal mean, and a minimum peak prominence equal to the signal standard deviation. The instantaneous respiratory rate was derived from the time intervals between consecutive ETCO₂ peaks, with values exceeding the median ± 2 standard deviations (SD) removed as outliers. Finally, we computed the mean respiratory rate (RR), the coefficient of variation of respiratory intervals (CVRR), and mean ETCO₂ values from the processed data.\nLinear mixed models (LMM) were employed to analyze psychological measures (STAI-S) and physiological indices (rLFA at F4-F3 and F8-F7, RMSSD, LF/HF ratio, RR, CVRR, and ETCO\nFor each model, the intervention condition (Factor 1: SB, RB) and measurement time point (Factor 2: post-task, post-stimuli) were specified as fixed effects. The corresponding baseline value from the pre-task period was included as a covariate to control for individual baseline differences. We initially attempted to fit models that also included random slopes for the effects of condition and time, but these more complex models failed to converge for some outcomes. Therefore, to ensure model stability and reliable results across all variables, we proceeded with a more parsimonious model that included only random intercepts for participants.\nModel fit was evaluated using marginal and conditional R2 values, and residual diagnostics were assessed using QQ plots. Estimated marginal means (EMM) and 95% confidence intervals (CI) were calculated and visualized. Analysis of variance (ANOVA) was conducted using Type III tests with Satterthwaite’s method for denominator degrees-of-freedom to assess the significance of fixed effects in the LMMs.\nStatistical significance was set at\nThe mean respiratory rate was 6.00 ± 0.11 breaths/min during SB condition and 14.76 ± 3.43 breaths/min during RB condition. These values indicated that participants successfully adhered to the instructed breathing patterns in each condition. The following sections present the results for each physiological and psychological variable. Detailed outputs for the fixed-effects estimates from all linear mixed models are presented in\nThe model fit evaluation indicated a good fit, with R\nEffects of breathing interventions on state anxiety and relative left frontal activity. Temporal changes in State–Trait Anxiety Inventory-State (STAI-S) scores (left), midfrontal (F4-F3) rLFA (middle), and lateral frontal (F8-F7) rLFA (right) during the experimental protocol. Solid purple lines with circles represent the slow-paced breathing (SB) condition; dashed green lines with triangles represent the resting breathing (RB) condition. Data are presented as estimated marginal means (EMM) with 95% confidence intervals (CI) for post-task and post-stimuli periods, while pre-task data show the raw means with 95% CI. The vertical dashed line separates pre-task from intervention periods. Asterisks indicate significant differences between conditions (*\nThe model fit evaluation indicated a good fit, with R\nAs illustrated in\nPower Spectral Density Analysis of Frontal and Frontotemporal EEG Activity. EEG power spectra from frontal and frontotemporal electrodes across three experimental periods (pre-task, post-task, and post-stimuli). The alpha band (8–13 Hz) is highlighted in gray. The rows display data from the slow breathing (SB; rows\nThe model fit evaluation showed R\nExamination of power spectral density patterns in\nThe model fit evaluation indicated a good fit, with R\nEffects of breathing interventions on autonomic and respiratory parameters.\nThe model fit evaluation showed R\nThe model fit evaluation showed R\nThe model fit evaluation showed R\nThe model fit evaluation indicated a good fit, with R\nThis study examined the effects of SB on state anxiety and rLFA compared to RB and assessed whether these effects persisted immediately after exposure to aversive visual stimuli. The key findings were as follows: (1) SB was significantly more effective than RB in reducing state anxiety post-task, an effect that subsequently buffered against the anxiety increase following aversive stimuli; (2) SB led to a significant increase in rLFA at the midfrontal site (F4-F3) compared to RB, and this effect also persisted immediately after aversive visual stimuli exposure; (3) A significant interaction was observed for RMSSD, an index of parasympathetic nervous system activity. Specifically, during the post-task period, RMSSD was lower in SB condition, but following aversive stimuli, it significantly increased only in SB condition.\nThe finding that SB buffered against the anxiety response to aversive stimuli is consistent with previous research on the anxiolytic effects of controlled breathing (\nIn parallel with the reduction in state anxiety, SB condition produced and maintained a left-lateralized shift in midfrontal rLFA. Although early work framed left-frontal dominance within an approach–withdrawal model of emotion (\nSB increased left-lateralized rLFA at the midfrontal site (F4–F3) but did not alter the lateral site (F8–F7), indicating regional specificity in prefrontal responsiveness. The midline electrodes overlie medial premotor and dorsomedial prefrontal cortex, nodes of the central autonomic network that receive respiration-synchronous input from brain-stem nuclei and contribute to interoceptive regulation (\nThe neural mechanisms underlying rLFA changes in response to SB remain unclear, but several possibilities can be considered. Previous studies have shown that rLFA is influenced by dopaminergic system activity (\nAlthough slow breathing usually elevates RMSSD by strengthening cardiac vagal activity, we observed a transient reduction during the immediate post-task phase, followed by a marked increase after the stimuli presentation. This pattern can be understood by considering how respiratory sinus arrhythmia (RSA) influences RMSSD. High CVRR values in the post-task window indicated unstable breathing cycles; such variability disperses RSA amplitude across successive beats and can therefore underestimate RMSSD despite genuine vagal activation (\nETCO\nThis study has several limitations. The lack of pre-registration and\nIn conclusion, this study provides initial evidence that a brief session of slow-paced breathing can reduce state anxiety and increase relative left frontal activity, with effects persisting immediately after aversive stimuli. These findings highlight the practical potential of SB as an accessible and cost-effective intervention for mitigating anxiety and preventing stress response escalation. However, these promising results must be interpreted with caution due to this study’s limitations, including a small sample size and a cross-sectional design. Therefore, future research is essential to confirm these preliminary findings. In particular, longitudinal studies with larger and more diverse clinical populations are warranted. For instance, neurological disorders such as Parkinson’s disease are frequently accompanied by psychiatric symptoms, including anxiety, which can negatively impact motor function (", "topic": "Brain"}
{"pmid": "40149798", "pmcid": "12309689", "title": "Revealing rhythm categorization in human brain activity", "publication_year": "2025", "abstract": "Humans across cultures show an outstanding capacity to perceive, learn, and produce musical rhythms. These skills rely on mapping the infinite space of possible rhythmic sensory inputs onto a finite set of internal rhythm categories. What is the nature of the brain processes underlying rhythm categorization? We used electroencephalography to measure brain activity as human participants listened to a continuum of rhythmic sequences characterized by repeating patterns of two interonset intervals. Using frequency and representational similarity analyses, we show that brain activity does not merely track the temporal structure of rhythmic inputs but, instead, produces categorical representation of rhythms. These neural rhythm categories arise automatically, independent of any motor- or timing-related tasks, yet exhibit strong similarity with categorization observed in overt behavior. Together, these results and methodological advances constitute a critical step toward understanding the biological roots and diversity of musical behaviors across cultures.\nHuman brain activity shows automatic categorization of rhythm, independent of but similar to categories produced in behavior.", "full_text": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "content_for_embedding": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "topic": "Brain"}
{"pmid": "40094166", "pmcid": "12305239", "title": "Outdoor Air Pollution Is Related to Amygdala Subregion Volume and Apportionment in Early Adolescence", "publication_year": "N/A", "abstract": "Air pollution is known to harm human physical and mental health. In this study, we explored how air pollution affects the amygdala, a brain region responsible for emotional processing, in 4473 9- to 10-year-olds. While no direct associations were identified between air pollutants and total amygdala size, co-exposure to PM\nAir pollution is known to harm human physical and mental health. In this study, we explored how air pollution affects the amygdala, a brain region responsible for emotional processing, in 4473 9- to 10-year-olds. While no direct associations were identified between air pollutants and total amygdala size, co-exposure to PM", "full_text": "Nearly 1 in 5 children in the United States experience mental health concerns (\nThe brain mechanisms that link air pollution and risk for psychopathologies remain unclear. Human research has demonstrated that inhaled pollutants deposit in the nasal cavity and lungs where, depending on the pollutant, they can rapidly enter the bloodstream and circulate throughout the body (\nThe aim of the current cross-sectional study was to fill these gaps by examining associations between ambient air pollutants and amygdala subregion volumes in 9- to 10-year-olds. To accomplish this, we took a 2-pronged approach. First, we used single-pollutant linear mixed-effects (LME) regression to investigate whether each pollutant related to total amygdala volume. Next, considering that individuals are not exposed to pollutants in isolation (\nThe current study utilized a subsample of cross-sectional baseline data from the National Institute of Mental Health (NIMH) Data Archive annual 3.0 (raw imaging data) and 5.0 (all other data) releases (\nAnnual average concentrations of ambient air pollution exposure were estimated at the primary residential address of each child, as previously described (\nAs previously published, a harmonized data protocol was utilized across all ABCD Study sites (\nIn cross-sectional analyses, brain volumes were adjusted using total intracranial volume (ICV) to compensate for differences in head size, as this enables comparisons of brain structures across individuals with different cranial sizes. Therefore, we adjusted for ICV in the total amygdala volume analyses and the amygdala subregion volume analyses. We also calculated amygdala subregion volumes normalized to the total amygdala volume, known as relative volume fractions (RVFs), by dividing each region’s probabilistic volume by the total probabilistic volume of the hemispheric amygdala (\nGiven that the CIT168 atlas was developed and validated using Siemens MRI data (\nCovariates included in our analyses were selected based on previous literature and the construction of a directed acyclic graph (\nAnalyses were conducted using R version 4.3.2 (\nAs part of the revision process, we conducted 2 sets of sensitivity analyses to test the robustness of our results (\nPLSC analyses were conducted as previously described (\nTo test the significant number of latent dimensions for each model, permutation testing was performed where data were resampled 10,000 times without replacement. The probability of significance was determined based on the number of times permuted singular values exceeded the observed singular value, and the percentage of variance explained was assessed visually using scree plots. The percentage of variance explained was used to interpret the strength of the association between the 2 matrices across each latent dimension (\nSociodemographic characteristics of the final study sample are included in\nDemographics for the Study Sample (\nValues are presented as\nBMI, body mass index.\nUsing LME modeling, no significant associations were identified between pollutants and total hemispheric amygdala volumes (\nUsing PLSC, no significant latent dimensions were identified between exposure to criteria pollutants and amygdala subregion volumes (\nVariable loadings for the association between fine particulate matter components and amygdala subregion volumes. Variables passing bootstrap ratio threshold (2.5,\nPLSC analyses of amygdala apportionment revealed one significant latent dimension between criteria pollutants and amygdala subregion RVFs, which explained 82% of the variance (\nVariable loadings for criteria air pollutants, fine particulate matter (PM\nThis study is the first to explore associations between ambient air pollution exposure and amygdala subregion morphology. We aimed to identify novel associations between exposure to criteria pollutants and PM\nWhile total amygdala volumes increase through early adolescence (\nAlthough additional research is needed, it is plausible that the observed associations between air pollution exposure and amygdala subregions may have long-term implications for emotional processing and risk for mental health concerns. The BLA is the main thalamic sensory and cortical input region of the amygdala, involved in emotional regulation and processing (\nSeveral strengths and limitations of the current study should be noted. We implemented the CIT168 atlas, which was created using in vivo Siemens MRI data from healthy young adult brains (\nOverall, the results of the current study show that co-exposures to various components—Ca, K, and OC—are related to differences in amygdala volumes and apportionment, with expansion in subregions involved in fear conditioning, and reduction in subregions responsible for anxiety and fear suppression. These findings support a growing body of literature identifying PM", "content_for_embedding": "Nearly 1 in 5 children in the United States experience mental health concerns (\nThe brain mechanisms that link air pollution and risk for psychopathologies remain unclear. Human research has demonstrated that inhaled pollutants deposit in the nasal cavity and lungs where, depending on the pollutant, they can rapidly enter the bloodstream and circulate throughout the body (\nThe aim of the current cross-sectional study was to fill these gaps by examining associations between ambient air pollutants and amygdala subregion volumes in 9- to 10-year-olds. To accomplish this, we took a 2-pronged approach. First, we used single-pollutant linear mixed-effects (LME) regression to investigate whether each pollutant related to total amygdala volume. Next, considering that individuals are not exposed to pollutants in isolation (\nThe current study utilized a subsample of cross-sectional baseline data from the National Institute of Mental Health (NIMH) Data Archive annual 3.0 (raw imaging data) and 5.0 (all other data) releases (\nAnnual average concentrations of ambient air pollution exposure were estimated at the primary residential address of each child, as previously described (\nAs previously published, a harmonized data protocol was utilized across all ABCD Study sites (\nIn cross-sectional analyses, brain volumes were adjusted using total intracranial volume (ICV) to compensate for differences in head size, as this enables comparisons of brain structures across individuals with different cranial sizes. Therefore, we adjusted for ICV in the total amygdala volume analyses and the amygdala subregion volume analyses. We also calculated amygdala subregion volumes normalized to the total amygdala volume, known as relative volume fractions (RVFs), by dividing each region’s probabilistic volume by the total probabilistic volume of the hemispheric amygdala (\nGiven that the CIT168 atlas was developed and validated using Siemens MRI data (\nCovariates included in our analyses were selected based on previous literature and the construction of a directed acyclic graph (\nAnalyses were conducted using R version 4.3.2 (\nAs part of the revision process, we conducted 2 sets of sensitivity analyses to test the robustness of our results (\nPLSC analyses were conducted as previously described (\nTo test the significant number of latent dimensions for each model, permutation testing was performed where data were resampled 10,000 times without replacement. The probability of significance was determined based on the number of times permuted singular values exceeded the observed singular value, and the percentage of variance explained was assessed visually using scree plots. The percentage of variance explained was used to interpret the strength of the association between the 2 matrices across each latent dimension (\nSociodemographic characteristics of the final study sample are included in\nDemographics for the Study Sample (\nValues are presented as\nBMI, body mass index.\nUsing LME modeling, no significant associations were identified between pollutants and total hemispheric amygdala volumes (\nUsing PLSC, no significant latent dimensions were identified between exposure to criteria pollutants and amygdala subregion volumes (\nVariable loadings for the association between fine particulate matter components and amygdala subregion volumes. Variables passing bootstrap ratio threshold (2.5,\nPLSC analyses of amygdala apportionment revealed one significant latent dimension between criteria pollutants and amygdala subregion RVFs, which explained 82% of the variance (\nVariable loadings for criteria air pollutants, fine particulate matter (PM\nThis study is the first to explore associations between ambient air pollution exposure and amygdala subregion morphology. We aimed to identify novel associations between exposure to criteria pollutants and PM\nWhile total amygdala volumes increase through early adolescence (\nAlthough additional research is needed, it is plausible that the observed associations between air pollution exposure and amygdala subregions may have long-term implications for emotional processing and risk for mental health concerns. The BLA is the main thalamic sensory and cortical input region of the amygdala, involved in emotional regulation and processing (\nSeveral strengths and limitations of the current study should be noted. We implemented the CIT168 atlas, which was created using in vivo Siemens MRI data from healthy young adult brains (\nOverall, the results of the current study show that co-exposures to various components—Ca, K, and OC—are related to differences in amygdala volumes and apportionment, with expansion in subregions involved in fear conditioning, and reduction in subregions responsible for anxiety and fear suppression. These findings support a growing body of literature identifying PM", "topic": "Brain"}
{"pmid": "40039527", "pmcid": "12309616", "title": "Revealing temporal dynamics of the visuomotor system via continuous tracking of position and attribute", "publication_year": "N/A", "abstract": "Continuous tracking is the recently developed psychophysical technique for efficiently estimating human visual temporal characteristics. The standard version of the task, referred to as position tracking (PT), asks participants to track the location of a continuously moving target by a motor response (e.g., mouse movement). Some studies have also used a variant method, attribute tracking (AT), which requires participants to track and reproduce a continuously changing attribute (e.g., luminance) of the target instead of position. For both PT and AT, the temporal dynamics of the entire system from vision to action can be estimated from the cross-correlogram (CCG) of the trajectory between the stimulus and response. The similarities and differences in CCG between PT and AT, however, remain elusive but were examined in this study. Experiment 1 compared the two CCGs using luminance-defined circular patches, color-contrast-defined patches, and luminance-defined patches with various spatial frequencies. The results indicate that the PT response was faster and less affected by the stimulus variables than the AT response. Experiment 2 showed that these differences could be reduced by making the visuomotor mapping of PT less direct by reversing the motor response direction and by making the local stimulus change magnitude comparable between PT and AT. The comparison with the traditional reaction time measures (Experiment 3) further showed that the peak latency of CCG from PT aligned better with the simple reaction time, whereas that from AT aligned better with the choice reaction time. These results indicate that CCG is more sluggish for AT than for PT because AT includes the process of identifying the stimulus content (attribute change direction) and mapping it to a motor response arbitrarily specified by the experimenter, and because the effective stimulus change magnitude for AT is often weaker than that for PT. These findings provide a clearer understanding of the meaning of CCGs measured by the two types of continuous tracking tasks.", "full_text": "The temporal processing of the visual system is crucial for survival in a dynamic world, and it has been extensively studied with regard to the temporal impulse response, temporal integration window, and so on (\nStandard continuous tracking asks participants to track the spatial position of the target stimulus; however, this task can also be extended to track stimulus changes in a dimension other than the spatial position. For example,\nThis study aimed to answer the following questions: Do the temporal dynamics revealed by CCGs differ between PT and AT? If so, where do the differences come from? How much are the underlying visuomotor mechanisms shared between PT and AT? Previous studies often ascribe the differences in CCGs to the temporal difference in the process encoding the sensory signals. For example,\nHere, we report three experiments. The first experiment measured the CCGs of PT and AT for several types of visual attributes. We compared the CCGs of PT and AT for the same attributes and revealed the similarities and differences in the effect of stimulus variables on the CCGs between PT and AT. The second experiment attempted to identify the sensory and motor factors producing the quantitative differences in CCGs between PT and AT. The final experiment introduced a simple stimulus detection task and an attribute choice task to determine the relationship between the traditional reaction time measurements and peak latencies of CCGs for PT and AT tasks.\nTwo of the authors and four naïve participants (six males; mean age, 24.67 years) participated. Participants declared that they had normal or corrected-to-normal vision. They were informed of the nature of the research and provided informed consent before the start of the experiment. The naïve participants received monetary compensation after the experiment was completed. The study protocol followed the tenets of the Declaration of Helsinki, except for registration, and was approved by the Ethics Committee of Kyoto University (KUIS-EAR-2020-003).\nThe stimuli were displayed on a VIEWPixx /3D LCD monitor (VPixx Technologies, Saint-Bruno-de-Montarville, QC, Canada) with a resolution of 1920 × 1080 pixels and a 120-Hz refresh rate. The lowest, mean, and highest display luminance values were 1.8, 48.4, and 96.7 cd/m\nIn the PT task, the participants were shown a horizontally moving circular target, subtending 4° in diameter, with a yellow point (0.2° in diameter) indicating the current mouse location for feedback (\nStimuli for\nThe participants tracked three different types of targets: a 2D luminance Gaussian blob, color-contrast concentric Gabors (red–green, and blue–yellow), and luminance-contrast concentric Gabors (eight spatial frequencies). The general stimuli profile for RGB channels is as follow:\nA minimum motion method was used to measure the equiluminant ratio (in terms of the system intensity value) between red (R) versus green (G) and blue (B) versus yellow (Y) for each participant before the main experiment. During the procedure, participants saw a Gabor drifting rightward. The sinusoidal waves of R and G or B and Y were 180°out of phase. The participants were required to adjust the ratio between the channels to minimize their sense of motion. The mean R:G ratio value was 0.58, and the mean B:Y ratio was 0.20.\nIn AT task, the participants were shown a 4° circular target. Instead of the spatial location, the attribute value varied in the following way:\nBelow the target, we also showed a feedback pattern of the same size as the target, whose attribute value varied according to the participants’ mouse position (\nFor the luminance patch,\nFor the red–green contrast concentric patch,\nFor the blue–yellow contrast concentric patch,\nFor the luminance contrast concentric patch,\nThe\nWe measured the PT and AT CCGs of tracking three attribute classes: luminance, color contrast, and luminance contrast. Within each class, the number of target types differed: one in luminance, two in color contrast (red–green and blue–yellow), and eight in luminance contrast (with spatial frequency ranging from 0.5 cpd to 11 cpd in 1.5-cpd steps). During PT, participants were asked to track the center of the target blob. In contrast, during AT, the participants were asked to match the appearance between the target and the feedback as closely as possible. Notably, participants were instructed that the rightward movement of the mouse would increase the attribute value (i.e., brighter in luminance, higher in contrast), but the leftward movement of the mouse would decrease the attribute value. In each condition, the participants were required to finish 30 tracking trials, each lasting 10 seconds. The participants completed 2 types of tasks × 11 different attribute types × 30 trials = 660 trials, and it took approximately 2 hours to complete the experiment. The sequence of the conditions was randomized.\nWe used the normalized cross-correlation with the time-shifting velocity between the target and response per trial and then took the average over 30 trials to compute the CCGs:\nHere,\nBecause the current study mainly focused on the processing delay of the visual system, we analyzed the peak latency (τ\nThe individual CCGs are shown in\nOne participant's CCGs for PT and AT across different attribute types. The blue line represents position tracking, and the red line represents attribute tracking. The shaded area represents the confidence interval between the 2.5 quantiles and 97.5 quantiles of the bootstrapped CCGs. The first row of the graph array represents the luminance. The second row shows the color contrast; the left shows the red–green contrast, and the right shows the blue–yellow contrast. The third and fourth rows are for luminance contrasts with different spatial frequencies, which are labeled in the title. The other participants’ figures can be found in the\nScatterplot of PT and AT across attribute types. The three colors represent luminance, color contrast, and luminance contrast, respectively. The vertical and horizontal lines attached to the dots are the confidence intervals between the 2.5 quantiles and 97.5 quantiles of the bootstrapped peak latency. The blue diagonal line is the reference line, with a slope of 1.\nPeak latency statistics between PT and AT across attribute types.\nIn addition to the task difference (PT vs. AT), the stimulus attribute influenced the CCG. The peak latency was longer for color contrast than for luminance, and an increase in spatial frequency increased peak latency. The observed trend in the attribute effect was consistent between PT and AT. A significant linear correlation existed between the peak latency of PT and AT across attribute types (\nThe first experiment evaluated the CCGs for various visual stimuli and examined the gap between PT and AT. Regarding the effects of stimulus characteristics on the human visual temporal responses, our findings were consistent with previous reports using non-tracking tasks. First, we observed that peak latencies were longer for color contrast than for luminance. This echoes the consensus that the processing of chromatic stimuli is slower than that of achromatic stimuli, as shown by psychophysics (\nOur findings revealed that tracking the AT took longer time than tracking the target location (PT), but that both tasks showed a qualitatively similar pattern regarding how response latencies changed with the attribute. This observation suggests that PT and AT share a part of the processing from vision to action, at least where the stimulus attribute influences the response speed, although the effect of stimulus attribute is weaker, or more saturated, for PT than for AT. Then, what mechanisms produce the differences between PT and AT?\n\nBoth tasks required the same low-level process to encode the attribute variation (e.g., luminance change) and read out different aspects. In PT, the target movement was estimated; in AT, the target stimulus content was estimated. After the attribute encoder stage, the system had to determine how to relate the stimulus representation with the task-relevant response (S–R mapping). In PT, the positional movement of the target was mapped to the corresponding hand movement, whereas, in AT, the increment/decrement of the target was mapped to the rightward/leftward hand movement. Finally, the hand movement command realized mouse movement.\nIn this framework, we identified two potential sources of AT sluggishness. First, in the stimulus–response (S–R) mapping stage, the mapping was more direct in PT. A rightward stimulus motion was mapped to a rightward hand movement, and a leftward stimulus motion was mapped to a leftward hand movement. In AT, on the other hand, a value increment was arbitrarily mapped to a rightward hand movement, and a value decrement was arbitrarily mapped to a leftward hand movement. The research on reaching limb actions shows that an arm or limb movement is automatically or involuntarily driven by a target motion (\nSecond, in the attribute encoding stage, the local alternation magnitude was generally larger in the PT than in the AT. In the setting of\nAccordingly, in\nIn\nIn order to reduce the magnitude of stimulus intensity in PT, we reduced the peak intensities of the target to 1/10 of the original PT conditions in\n\nVisual inspection of the obtained CCGs for each participant and stimulus condition (\nThe CCG of one participant across three PT modulations and attribute conditions. A dotted line represents the AT results, and a dashed line indicates PT from\n\n(\nThe findings suggest that decreasing the stimulus intensity enhanced the latency difference among stimulus attribute conditions and that voluntary mapping added a constant increase in latency. Furthermore, the combination of the two factors made the PT temporal response fairly close to the AT temporal response. These results suggest that stimulus magnitude and stimulus–response mapping are two major factors that produce differences between PT and AT.\nBecause inverse tracking introduced a novel motor task for participants, we investigated whether the later trials exhibited a learning effect compared to the earlier trials. To quantify the learning effect, we computed the latency change rate by subtracting the median peak latency of the last 15 trials from that of the first 15 trials. We then compared the learning effects across three different tracking conditions: normal, inverse, and attribute. The normal condition included standard tracking tasks (PT and PT_LV), the inverse condition included inverse tracking tasks (PT_Inv and PT_Inv_LV), and the attribute condition corresponded to AT. The results, shown in\nLatency changing rate from the first 15 trials to the later 15 trials across various tracking conditions and attribute types. The horizontal axis indicates three different tracking tasks: normal, corresponding to normal S–R mapping, stands for the PT and PT_LV condition, and inverse, corresponding to inverse S–R mapping, stands for PT_Inv and PT_Inv_LV. Attribute corresponds to AT. The vertical axis is the latency changing rate computed by the change percentage of the first 15 trial peak latency median values compared with the later 15 trials. The error bar attached to each bar is the confident interval with Bonferroni correction (i.e., 99.92% CI).\nFinally, we introduced a simple linear model to decompose the observed peak latency into two components: one from the attribute-specific encoding stage and the other from the S–R mapping stage. This follows the conceptual framework proposed by\nThe first term,\nThe modeling results revealed the following parameter estimates (with the estimate error in the parentheses):\nAdditionally, the magnitude-related encoding latency for AT, captured by\nThe model achieved a high degree of fit to the empirical data (a scatter plot can be referred to\nIn\nModel fit for peak latency data across all experimental conditions. The\nThis experiment had two primary objectives. The first was to examine how the peak latencies of PT and AT correlated with the two RTs, aiming to gain deeper insights into the mechanisms underlying tracking tasks, given that RT interpretation has been extensively studied and is well established. Specifically, because S–R mapping is more direct for simple RTs than for choice RTs, we expected a similarity between the relationship between PT and AT, and the relationship between simple RTs and choice RTs. The second objective was to establish a connection between the current continuous tracking method and RTs, providing a benchmark for future research to compare results across different experimental designs.\nIn the simple RT task, the stimulus was initially presented either with or without a pattern, followed by a random interval between 700 and 2000 ms, and a change in the stimuli occurred. Participants were instructed to detect the pattern change, either from visible to invisible or from invisible to visible, as rapidly as possible. The stimulus size and the number of stimulus attribute types were consistent with those in the original PT task.\nIn the choice RT task, a colored white noise was initially presented, and, after a random interval, a target stimulus was displayed. Specifically, we modified the stimulus in the color choice task from a concentric Gabor pattern to a single-color Gaussian blob, requiring participants to identify the color itself rather than detect color contrast. Participants were required to identify the type of stimulus that appeared by pressing the corresponding key, with the left key representing white/red/yellow and the right key representing black/green/blue. The luminance contrast stimuli were not examined in the choice RT task because low- and high-contrast options in the choice RT task became similar to detectable and undetectable options in the simple RT task.\nThe choice RT task with a low-intensity magnitude followed the same procedure and utilized the same attribute types as the choice RT task, with the exception that the maximum magnitude of the stimulus was reduced to 10% of the maximum, consistent with the settings in PT_LV. For each stimulus and task condition, 30 trials were run. Data from the same six participants were collected in an additional hour.\nAs shown in\nThe two RT paradigms were structured as follows. (\n(\nThe results demonstrated that the latency of tracking tasks aligns with that of the traditional reaction task. Specifically, the high similarity between the PT peak latency and simple RT suggests that the PT latency primarily reflects the processing time required for detecting the occurrence of stimulus change. On the other hand, the high similarity between the AT peak latency and simple RT suggests that the AT latency reflects not only the detection of stimulus change but also the identification of the stimuli content and the selection of the corresponding response.\nThis study directly compared the temporal dynamics of position and attribute tracking (PT and AT) for luminance, color contrast, and luminance contrast. The findings demonstrated that continuously tracking the attribute value of the stimuli (AT) was slower than tracking the location of the stimulus (PT), but the sharing of the encoding process between PT and AT was also suggested by a positive correlation in the effects of stimulus attribute on the peak latency. The subsequent experiment that altered the S–R mapping and local variation magnitude of PT successfully aligned the latencies of PT close to those of AT. The results demonstrated that the disparity in the underlying mechanism between PT and AT comes from a difference in the S–R mapping, in addition to a difference in the effective stimulus strength. The RT measurements provide additional evidence. The simple RT that primarily evaluates the basic speed to detect a stimulus change is closely associated with the peak latency of PT, whereas the choice RT that includes additional processes is closely associated with the peak latency of AT.\nIn the PT task, participants were required to monitor a dynamic change in the position of the target. In the AT task, in contrast, participants were required to monitor a dynamic change in the attribute value of the target stimulus. In spite of the difference in the task objective, our findings revealed clear correlations in the effects of stimulus attribute on the peak latency between AT and PT or its variants (PT_LV, PT_InvLV) except for PT_Inv. This suggests that PT and AT share a mechanism that detects attribute changes (i.e., the attribute detector). This relationship may stem from the partial overlap in task objectives between PT and AT, as both require the visual system to encode the local variation of the attribute value.\nAs shown by the slope of the scatterplots in\nWe propose that one factor producing the slowness of AT, relative to PT, is that the AT task included a voluntary S–R mapping. In contrast, PT can be assisted by a mechanism of involuntary and rapid hand movement following a visual target (\nIn accordance with the similar temporal latency between AT and antireaction PTs, a similar voluntary motor mapping system may be included; however, the underlying mechanism between PT_Inv and AT can be different. For example, when performing the antireaction task in PT_Inv or PT_InvLV, the system may be required to inhibit the original S–R mapping and then voluntarily establish a new one, whereas, in AT, it could be simply related to voluntary S–R mapping. In this regard, however, recent studies have shown that voluntary inhibition of an involuntary motor command involves neural regions common to voluntary motor commands (\nMoreover, we examined whether participants could improve their performance in these more demanding tracking tasks through practice. However, we found no evidence of a learning effect across the 30 trials in each condition, as tracking latency did not significantly decrease over time. This may reflect both the short time scale of the task and the lack of informative feedback, particularly in the inverted mapping condition, where participants did not receive explicit cues to support learning a reversed mapping strategy. One more limitation is that, because the current study used only hand movement to collect the response, it remains unknown whether we can still observe the PT–AT consistency when we use other motor responses, such as eye movement.\nThis study compared the peak latencies for two types of continuous tracking and the RTs for similar stimuli. Their mutual relationships can be approximated as follows:\nDespite being obtained under a specific condition, these functions will be useful for comparison the results obtained with different psychophysical tasks.\nPeak latency is not the sole metric for analyzing CCGs. Other indices such as half-height bandwidth or peak amplitude may also be informative; however, we observed a high level of consistency among these measurements. The Pearson's correlation coefficient was 0.98 between peak latency and bandwidth, –0.83 between peak latency and peak amplitude, and –0.88 between bandwidth and peak amplitude. This result aligns with that of\nFocusing solely on peak latency data might lead to the misconception that the CCG is simply another rapid variant of the RT paradigm. Subsequent research could explore the underlying mechanisms responsible for shaping the CCG function. To achieve this, a comprehensive dynamic model integrating processes from perception to action would be necessary.\nOne methodological point to note is that our trajectory generation process involved rejecting approximately 70% to 80% of the randomly generated sequences, based on criteria ensuring sufficient drifting across the space (spatial or attribute value). Although this filtering step deviated from the original generative model, it did not affect the validity of our CCG analysis, as the retained trajectories still exhibited near-delta autocorrelation functions. However, future modeling efforts that directly rely on the assumed stimulus distribution (for example, those simulating or analyzing behavioral trajectories) should consider the empirical distribution of the final stimuli rather than the idealized generative process.\nWhereas the present study focused on data-driven comparisons between position tracking (PT) and attribute tracking (AT), we recognize that a formal computational model could provide a more unified account of the mechanisms underlying the observed latency differences. Such a model would have to integrate multiple stages (\nExisting models of continuous tracking, such as the Kalman filter framework proposed by\nAs an initial attempt to address this issue, we introduced a simple linear model that decomposes the peak latency into components attributed to stimulus encoding and S–R mapping demands. This model, estimated using least squares on group-averaged data from six participants, successfully captured the relative contributions of attribute type, magnitude, and motor mapping, achieving a high degree of fit (\nA comprehensive future model should explicitly dissociate the contributions of spatial and temporal uncertainty and possibly incorporate individual differences in cost function strategies, as suggested by\nThis research introduces continuous psychophysics paradigm to examine the temporal dynamics of various attributes, aiming to investigate the different processes between position tracking) and attribute tracking. Findings reveal that AT is significantly slower than PT to complete the task, yet the latency difference among different attributes shows a strong correlation, due to the use of a common attribute encoder. Lower stimulus variation magnitude and voluntary stimuli–response mapping can explain the differences between AT and PT. Moreover, the measurements by this new paradigm corresponded well with traditional RT measurements.", "content_for_embedding": "The temporal processing of the visual system is crucial for survival in a dynamic world, and it has been extensively studied with regard to the temporal impulse response, temporal integration window, and so on (\nStandard continuous tracking asks participants to track the spatial position of the target stimulus; however, this task can also be extended to track stimulus changes in a dimension other than the spatial position. For example,\nThis study aimed to answer the following questions: Do the temporal dynamics revealed by CCGs differ between PT and AT? If so, where do the differences come from? How much are the underlying visuomotor mechanisms shared between PT and AT? Previous studies often ascribe the differences in CCGs to the temporal difference in the process encoding the sensory signals. For example,\nHere, we report three experiments. The first experiment measured the CCGs of PT and AT for several types of visual attributes. We compared the CCGs of PT and AT for the same attributes and revealed the similarities and differences in the effect of stimulus variables on the CCGs between PT and AT. The second experiment attempted to identify the sensory and motor factors producing the quantitative differences in CCGs between PT and AT. The final experiment introduced a simple stimulus detection task and an attribute choice task to determine the relationship between the traditional reaction time measurements and peak latencies of CCGs for PT and AT tasks.\nTwo of the authors and four naïve participants (six males; mean age, 24.67 years) participated. Participants declared that they had normal or corrected-to-normal vision. They were informed of the nature of the research and provided informed consent before the start of the experiment. The naïve participants received monetary compensation after the experiment was completed. The study protocol followed the tenets of the Declaration of Helsinki, except for registration, and was approved by the Ethics Committee of Kyoto University (KUIS-EAR-2020-003).\nThe stimuli were displayed on a VIEWPixx /3D LCD monitor (VPixx Technologies, Saint-Bruno-de-Montarville, QC, Canada) with a resolution of 1920 × 1080 pixels and a 120-Hz refresh rate. The lowest, mean, and highest display luminance values were 1.8, 48.4, and 96.7 cd/m\nIn the PT task, the participants were shown a horizontally moving circular target, subtending 4° in diameter, with a yellow point (0.2° in diameter) indicating the current mouse location for feedback (\nStimuli for\nThe participants tracked three different types of targets: a 2D luminance Gaussian blob, color-contrast concentric Gabors (red–green, and blue–yellow), and luminance-contrast concentric Gabors (eight spatial frequencies). The general stimuli profile for RGB channels is as follow:\nA minimum motion method was used to measure the equiluminant ratio (in terms of the system intensity value) between red (R) versus green (G) and blue (B) versus yellow (Y) for each participant before the main experiment. During the procedure, participants saw a Gabor drifting rightward. The sinusoidal waves of R and G or B and Y were 180°out of phase. The participants were required to adjust the ratio between the channels to minimize their sense of motion. The mean R:G ratio value was 0.58, and the mean B:Y ratio was 0.20.\nIn AT task, the participants were shown a 4° circular target. Instead of the spatial location, the attribute value varied in the following way:\nBelow the target, we also showed a feedback pattern of the same size as the target, whose attribute value varied according to the participants’ mouse position (\nFor the luminance patch,\nFor the red–green contrast concentric patch,\nFor the blue–yellow contrast concentric patch,\nFor the luminance contrast concentric patch,\nThe\nWe measured the PT and AT CCGs of tracking three attribute classes: luminance, color contrast, and luminance contrast. Within each class, the number of target types differed: one in luminance, two in color contrast (red–green and blue–yellow), and eight in luminance contrast (with spatial frequency ranging from 0.5 cpd to 11 cpd in 1.5-cpd steps). During PT, participants were asked to track the center of the target blob. In contrast, during AT, the participants were asked to match the appearance between the target and the feedback as closely as possible. Notably, participants were instructed that the rightward movement of the mouse would increase the attribute value (i.e., brighter in luminance, higher in contrast), but the leftward movement of the mouse would decrease the attribute value. In each condition, the participants were required to finish 30 tracking trials, each lasting 10 seconds. The participants completed 2 types of tasks × 11 different attribute types × 30 trials = 660 trials, and it took approximately 2 hours to complete the experiment. The sequence of the conditions was randomized.\nWe used the normalized cross-correlation with the time-shifting velocity between the target and response per trial and then took the average over 30 trials to compute the CCGs:\nHere,\nBecause the current study mainly focused on the processing delay of the visual system, we analyzed the peak latency (τ\nThe individual CCGs are shown in\nOne participant's CCGs for PT and AT across different attribute types. The blue line represents position tracking, and the red line represents attribute tracking. The shaded area represents the confidence interval between the 2.5 quantiles and 97.5 quantiles of the bootstrapped CCGs. The first row of the graph array represents the luminance. The second row shows the color contrast; the left shows the red–green contrast, and the right shows the blue–yellow contrast. The third and fourth rows are for luminance contrasts with different spatial frequencies, which are labeled in the title. The other participants’ figures can be found in the\nScatterplot of PT and AT across attribute types. The three colors represent luminance, color contrast, and luminance contrast, respectively. The vertical and horizontal lines attached to the dots are the confidence intervals between the 2.5 quantiles and 97.5 quantiles of the bootstrapped peak latency. The blue diagonal line is the reference line, with a slope of 1.\nPeak latency statistics between PT and AT across attribute types.\nIn addition to the task difference (PT vs. AT), the stimulus attribute influenced the CCG. The peak latency was longer for color contrast than for luminance, and an increase in spatial frequency increased peak latency. The observed trend in the attribute effect was consistent between PT and AT. A significant linear correlation existed between the peak latency of PT and AT across attribute types (\nThe first experiment evaluated the CCGs for various visual stimuli and examined the gap between PT and AT. Regarding the effects of stimulus characteristics on the human visual temporal responses, our findings were consistent with previous reports using non-tracking tasks. First, we observed that peak latencies were longer for color contrast than for luminance. This echoes the consensus that the processing of chromatic stimuli is slower than that of achromatic stimuli, as shown by psychophysics (\nOur findings revealed that tracking the AT took longer time than tracking the target location (PT), but that both tasks showed a qualitatively similar pattern regarding how response latencies changed with the attribute. This observation suggests that PT and AT share a part of the processing from vision to action, at least where the stimulus attribute influences the response speed, although the effect of stimulus attribute is weaker, or more saturated, for PT than for AT. Then, what mechanisms produce the differences between PT and AT?\n\nBoth tasks required the same low-level process to encode the attribute variation (e.g., luminance change) and read out different aspects. In PT, the target movement was estimated; in AT, the target stimulus content was estimated. After the attribute encoder stage, the system had to determine how to relate the stimulus representation with the task-relevant response (S–R mapping). In PT, the positional movement of the target was mapped to the corresponding hand movement, whereas, in AT, the increment/decrement of the target was mapped to the rightward/leftward hand movement. Finally, the hand movement command realized mouse movement.\nIn this framework, we identified two potential sources of AT sluggishness. First, in the stimulus–response (S–R) mapping stage, the mapping was more direct in PT. A rightward stimulus motion was mapped to a rightward hand movement, and a leftward stimulus motion was mapped to a leftward hand movement. In AT, on the other hand, a value increment was arbitrarily mapped to a rightward hand movement, and a value decrement was arbitrarily mapped to a leftward hand movement. The research on reaching limb actions shows that an arm or limb movement is automatically or involuntarily driven by a target motion (\nSecond, in the attribute encoding stage, the local alternation magnitude was generally larger in the PT than in the AT. In the setting of\nAccordingly, in\nIn\nIn order to reduce the magnitude of stimulus intensity in PT, we reduced the peak intensities of the target to 1/10 of the original PT conditions in\n\nVisual inspection of the obtained CCGs for each participant and stimulus condition (\nThe CCG of one participant across three PT modulations and attribute conditions. A dotted line represents the AT results, and a dashed line indicates PT from\n\n(\nThe findings suggest that decreasing the stimulus intensity enhanced the latency difference among stimulus attribute conditions and that voluntary mapping added a constant increase in latency. Furthermore, the combination of the two factors made the PT temporal response fairly close to the AT temporal response. These results suggest that stimulus magnitude and stimulus–response mapping are two major factors that produce differences between PT and AT.\nBecause inverse tracking introduced a novel motor task for participants, we investigated whether the later trials exhibited a learning effect compared to the earlier trials. To quantify the learning effect, we computed the latency change rate by subtracting the median peak latency of the last 15 trials from that of the first 15 trials. We then compared the learning effects across three different tracking conditions: normal, inverse, and attribute. The normal condition included standard tracking tasks (PT and PT_LV), the inverse condition included inverse tracking tasks (PT_Inv and PT_Inv_LV), and the attribute condition corresponded to AT. The results, shown in\nLatency changing rate from the first 15 trials to the later 15 trials across various tracking conditions and attribute types. The horizontal axis indicates three different tracking tasks: normal, corresponding to normal S–R mapping, stands for the PT and PT_LV condition, and inverse, corresponding to inverse S–R mapping, stands for PT_Inv and PT_Inv_LV. Attribute corresponds to AT. The vertical axis is the latency changing rate computed by the change percentage of the first 15 trial peak latency median values compared with the later 15 trials. The error bar attached to each bar is the confident interval with Bonferroni correction (i.e., 99.92% CI).\nFinally, we introduced a simple linear model to decompose the observed peak latency into two components: one from the attribute-specific encoding stage and the other from the S–R mapping stage. This follows the conceptual framework proposed by\nThe first term,\nThe modeling results revealed the following parameter estimates (with the estimate error in the parentheses):\nAdditionally, the magnitude-related encoding latency for AT, captured by\nThe model achieved a high degree of fit to the empirical data (a scatter plot can be referred to\nIn\nModel fit for peak latency data across all experimental conditions. The\nThis experiment had two primary objectives. The first was to examine how the peak latencies of PT and AT correlated with the two RTs, aiming to gain deeper insights into the mechanisms underlying tracking tasks, given that RT interpretation has been extensively studied and is well established. Specifically, because S–R mapping is more direct for simple RTs than for choice RTs, we expected a similarity between the relationship between PT and AT, and the relationship between simple RTs and choice RTs. The second objective was to establish a connection between the current continuous tracking method and RTs, providing a benchmark for future research to compare results across different experimental designs.\nIn the simple RT task, the stimulus was initially presented either with or without a pattern, followed by a random interval between 700 and 2000 ms, and a change in the stimuli occurred. Participants were instructed to detect the pattern change, either from visible to invisible or from invisible to visible, as rapidly as possible. The stimulus size and the number of stimulus attribute types were consistent with those in the original PT task.\nIn the choice RT task, a colored white noise was initially presented, and, after a random interval, a target stimulus was displayed. Specifically, we modified the stimulus in the color choice task from a concentric Gabor pattern to a single-color Gaussian blob, requiring participants to identify the color itself rather than detect color contrast. Participants were required to identify the type of stimulus that appeared by pressing the corresponding key, with the left key representing white/red/yellow and the right key representing black/green/blue. The luminance contrast stimuli were not examined in the choice RT task because low- and high-contrast options in the choice RT task became similar to detectable and undetectable options in the simple RT task.\nThe choice RT task with a low-intensity magnitude followed the same procedure and utilized the same attribute types as the choice RT task, with the exception that the maximum magnitude of the stimulus was reduced to 10% of the maximum, consistent with the settings in PT_LV. For each stimulus and task condition, 30 trials were run. Data from the same six participants were collected in an additional hour.\nAs shown in\nThe two RT paradigms were structured as follows. (\n(\nThe results demonstrated that the latency of tracking tasks aligns with that of the traditional reaction task. Specifically, the high similarity between the PT peak latency and simple RT suggests that the PT latency primarily reflects the processing time required for detecting the occurrence of stimulus change. On the other hand, the high similarity between the AT peak latency and simple RT suggests that the AT latency reflects not only the detection of stimulus change but also the identification of the stimuli content and the selection of the corresponding response.\nThis study directly compared the temporal dynamics of position and attribute tracking (PT and AT) for luminance, color contrast, and luminance contrast. The findings demonstrated that continuously tracking the attribute value of the stimuli (AT) was slower than tracking the location of the stimulus (PT), but the sharing of the encoding process between PT and AT was also suggested by a positive correlation in the effects of stimulus attribute on the peak latency. The subsequent experiment that altered the S–R mapping and local variation magnitude of PT successfully aligned the latencies of PT close to those of AT. The results demonstrated that the disparity in the underlying mechanism between PT and AT comes from a difference in the S–R mapping, in addition to a difference in the effective stimulus strength. The RT measurements provide additional evidence. The simple RT that primarily evaluates the basic speed to detect a stimulus change is closely associated with the peak latency of PT, whereas the choice RT that includes additional processes is closely associated with the peak latency of AT.\nIn the PT task, participants were required to monitor a dynamic change in the position of the target. In the AT task, in contrast, participants were required to monitor a dynamic change in the attribute value of the target stimulus. In spite of the difference in the task objective, our findings revealed clear correlations in the effects of stimulus attribute on the peak latency between AT and PT or its variants (PT_LV, PT_InvLV) except for PT_Inv. This suggests that PT and AT share a mechanism that detects attribute changes (i.e., the attribute detector). This relationship may stem from the partial overlap in task objectives between PT and AT, as both require the visual system to encode the local variation of the attribute value.\nAs shown by the slope of the scatterplots in\nWe propose that one factor producing the slowness of AT, relative to PT, is that the AT task included a voluntary S–R mapping. In contrast, PT can be assisted by a mechanism of involuntary and rapid hand movement following a visual target (\nIn accordance with the similar temporal latency between AT and antireaction PTs, a similar voluntary motor mapping system may be included; however, the underlying mechanism between PT_Inv and AT can be different. For example, when performing the antireaction task in PT_Inv or PT_InvLV, the system may be required to inhibit the original S–R mapping and then voluntarily establish a new one, whereas, in AT, it could be simply related to voluntary S–R mapping. In this regard, however, recent studies have shown that voluntary inhibition of an involuntary motor command involves neural regions common to voluntary motor commands (\nMoreover, we examined whether participants could improve their performance in these more demanding tracking tasks through practice. However, we found no evidence of a learning effect across the 30 trials in each condition, as tracking latency did not significantly decrease over time. This may reflect both the short time scale of the task and the lack of informative feedback, particularly in the inverted mapping condition, where participants did not receive explicit cues to support learning a reversed mapping strategy. One more limitation is that, because the current study used only hand movement to collect the response, it remains unknown whether we can still observe the PT–AT consistency when we use other motor responses, such as eye movement.\nThis study compared the peak latencies for two types of continuous tracking and the RTs for similar stimuli. Their mutual relationships can be approximated as follows:\nDespite being obtained under a specific condition, these functions will be useful for comparison the results obtained with different psychophysical tasks.\nPeak latency is not the sole metric for analyzing CCGs. Other indices such as half-height bandwidth or peak amplitude may also be informative; however, we observed a high level of consistency among these measurements. The Pearson's correlation coefficient was 0.98 between peak latency and bandwidth, –0.83 between peak latency and peak amplitude, and –0.88 between bandwidth and peak amplitude. This result aligns with that of\nFocusing solely on peak latency data might lead to the misconception that the CCG is simply another rapid variant of the RT paradigm. Subsequent research could explore the underlying mechanisms responsible for shaping the CCG function. To achieve this, a comprehensive dynamic model integrating processes from perception to action would be necessary.\nOne methodological point to note is that our trajectory generation process involved rejecting approximately 70% to 80% of the randomly generated sequences, based on criteria ensuring sufficient drifting across the space (spatial or attribute value). Although this filtering step deviated from the original generative model, it did not affect the validity of our CCG analysis, as the retained trajectories still exhibited near-delta autocorrelation functions. However, future modeling efforts that directly rely on the assumed stimulus distribution (for example, those simulating or analyzing behavioral trajectories) should consider the empirical distribution of the final stimuli rather than the idealized generative process.\nWhereas the present study focused on data-driven comparisons between position tracking (PT) and attribute tracking (AT), we recognize that a formal computational model could provide a more unified account of the mechanisms underlying the observed latency differences. Such a model would have to integrate multiple stages (\nExisting models of continuous tracking, such as the Kalman filter framework proposed by\nAs an initial attempt to address this issue, we introduced a simple linear model that decomposes the peak latency into components attributed to stimulus encoding and S–R mapping demands. This model, estimated using least squares on group-averaged data from six participants, successfully captured the relative contributions of attribute type, magnitude, and motor mapping, achieving a high degree of fit (\nA comprehensive future model should explicitly dissociate the contributions of spatial and temporal uncertainty and possibly incorporate individual differences in cost function strategies, as suggested by\nThis research introduces continuous psychophysics paradigm to examine the temporal dynamics of various attributes, aiming to investigate the different processes between position tracking) and attribute tracking. Findings reveal that AT is significantly slower than PT to complete the task, yet the latency difference among different attributes shows a strong correlation, due to the use of a common attribute encoder. Lower stimulus variation magnitude and voluntary stimuli–response mapping can explain the differences between AT and PT. Moreover, the measurements by this new paradigm corresponded well with traditional RT measurements.", "topic": "Brain"}
{"pmid": "39988284", "pmcid": "12300364", "title": "Memristor-Based Spiking Neuromorphic Systems Toward Brain-Inspired Perception and Computing", "publication_year": "N/A", "abstract": "Threshold-switching memristors (TSMs) are emerging as key enablers for hardware spiking neural networks, offering intrinsic spiking dynamics, sub-pJ energy consumption, and nanoscale footprints ideal for brain-inspired computing at the edge. This review provides a comprehensive examination of how TSMs emulate diverse spiking behaviors—including oscillatory, leaky integrate-and-fire (LIF), Hodgkin–Huxley (H-H), and stochastic dynamics—and how these features enable compact, energy-efficient neuromorphic systems. We analyze the physical switching mechanisms of redox and Mott-type TSMs, discuss their voltage-dependent dynamics, and assess their suitability for spike generation. We review memristor-based neuron circuits regarding architectures, materials, and key performance metrics. At the system level, we summarize bio-inspired neuromorphic platforms integrating TSM neurons with visual, tactile, thermal, and olfactory sensors, achieving real-time edge computation with high accuracy and low power. Finally, we critically examine key challenges—such as stochastic switching origins, device variability, and endurance limits—and propose future directions toward reconfigurable, robust, and scalable memristive neuromorphic architectures.", "full_text": "Edge-AI workloads, such as 4 K/60 fps video (≈12 Gb/s) or sub-10 ms-latency drone navigation [\nTheir two-terminal structure further facilitates seamless integration into neuromorphic systems. Volatile TSMs underpin both (i) the device-level emulation of diverse Neuronal firing behaviors—regular, bursting, adaptive, and stochastic—and (ii) system-level sensory front-ends that transduce the light, pressure, or gas concentration directly into spike trains [\nDespite notable progress in various aspects, memristor-based neuromorphic systems still face several challenges for large-scale deployment, such as significant device variability, limited endurance, and stochastic switching behaviors, all of which may compromise the computational accuracy and scalability. Furthermore, efficient learning algorithms that account for the non-ideal characteristics of memristors are still under active development. Ongoing efforts in approximate computing, CMOS–memristor hybrid design, and the co-optimization of algorithms and hardware aim to overcome these obstacles and accelerate the transition toward practical applications.\nIn this review, we present a comprehensive assessment of memristors in enabling spiking neuromorphic systems targeting brain-like perception and computation. The discussion begins with the biological basis of neural spiking and common computational models, followed by the underlying switching physics of memristors and their utility in mimicking neuronal dynamics. We then summarize key developments in spiking neuron circuit designs based on memristors, highlight applications in vision, touch, and multimodal bioinspired sensing, and conclude by identifying major challenges and future perspectives.\nWhile several prior reviews have examined memristors for neuromorphic computing, most focus predominantly on synaptic functionalities or general device mechanisms without emphasizing spiking neuron implementations. Furthermore, relatively few works systematically analyze how TSMs uniquely enable diverse spiking neuron behaviors—such as leaky integration, oscillations, stochastic firing, and action potential-like dynamics—at the device and circuit levels. Moreover, the interplay between memristive neuron models and multi-modal sensory systems (e.g., vision, touch, and olfaction) remains underexplored.\nThis review seeks to bridge these gaps by providing a focused, structured, and critical evaluation of memristor-based spiking neuromorphic systems, encompassing device mechanisms, circuit implementations, and bio-inspired perception applications. In doing so, it offers a unified perspective on how TSM-based neurons can be harnessed for energy-efficient, near-sensor computing, thereby advancing the state-of-the-art in neuromorphic engineering.\nBiological neurons encode and transmit information through discrete electrical spikes, known as action potentials, rather than the clock-synchronized binary levels of digital logic [\nBiological neurons display diverse spiking patterns to fulfill various functional roles in the brain. This functional diversity underpins the brain’s capacity for temporal encoding, information compression, and stimulus discrimination. Replicating these firing patterns in hardware implementations is essential for achieving biologically faithful neuromorphic architectures. Thus, a thorough understanding of the electrophysiological mechanisms underlying neuronal excitability and information coding is crucial for advancing neuroscience and designing future spike-based hardware systems.\nThe Hodgkin–Huxley (H-H) model provides a quantitative framework for describing the biophysical mechanisms underlying action potential generation in neurons [\nWhile the H-H model provides a highly detailed and biologically realistic simulation of neuronal behavior, its computational complexity is significant. Each time-step simulation typically requires around 1200 floating-point operations, making it computationally expensive and limiting its scalability for large networks or hardware implementations. This high computational cost presents challenges for real-time applications, particularly in neuromorphic systems where low power and efficiency are crucial. To reduce the computational cost while retaining biologically relevant timing, simplified neuron models have been proposed. Notably, the LIF model has become the predominant choice in theoretical neuroscience and neuromorphic implementations [\nBeyond the LIF model, several other neuron models—such as the Izhikevich, FitzHugh–Nagumo (FHN), and Hindmarsh–Rose models—have been introduced to capture more complex neuronal dynamics. The two-variable Izhikevich model matches 20 neuronal firing patterns at 50× less computation than H-H, while FHN captures excitability class-I/II bifurcation with analytical tractability [\nIn conclusion, spiking neuron modeling serves as a critical bridge between biological neuroscience and AI system design. Ranging from the intricate ion–channel kinetics of the H-H model to the computationally efficient abstractions of the LIF and Izhikevich models, the field is evolving toward neuron models that balance biological fidelity with hardware implementability [\nSelecting appropriate spiking neuron models hinges on balancing biological realism and computational efficiency. Therefore, in neuromorphic engineering—where energy efficiency and scalability are paramount—identifying suitable hardware-level implementations of neuron models can significantly simplify computation. Memristors, with their inherent hardware compatibility and intrinsic nonlinearities that emulate ion–channel dynamics, emerge as promising candidates for implementing diverse spiking behaviors in low-power, compact hardware systems. When implemented with memristors, these simplified models benefit from the inherent characteristics of memristive devices, such as threshold-switching and nonlinear behavior, to replicate the action potential dynamics seen in biological neurons. The trade-offs between these models in terms of biological fidelity versus hardware implementation are critical for choosing the right model for a given application. For example, while the H-H model offers high fidelity, it is challenging to implement efficiently in low-power neuromorphic systems due to its high computational demand.\nIn 1962, negative differential resistance (NDR) was first observed in metal–oxide sandwich structures based on Al\nTSMs used in neuron circuit design are mainly classified into two types based on their switching mechanisms: electrochemical redox and Mott memristors [\nMott memristors, depicted in\nResearchers have implemented a variety of TSM-driven neuron circuits to emulate diverse spiking patterns, including oscillatory [\nA key goal of neuromorphic engineering is to design compact, energy-efficient electronic elements that accurately emulate the spiking dynamics of biological neurons. Among emerging devices, TSMs provide a distinctive approach to emulate spiking neuron functionalities. Leveraging their intrinsic nonlinear dynamics, spontaneous threshold switching, and time-dependent behavior, researchers have constructed numerous artificial spiking neurons. This section presents a taxonomy of key neuron circuit architectures, implementation strategies, and performance metrics.\nOscillatory neurons emulate the rhythmic firing behavior observed in biological neurons by generating periodic spikes when driven by either constant voltage or current inputs. This neuron type is particularly suitable for simulating synchronized and rhythmic neuronal activity, commonly found in neuronal oscillations.\nIn a current-driven oscillatory neuron circuit, as illustrated in\nThese memristor-based oscillatory neurons have been extensively applied in neuromorphic computing tasks. Gao et al. demonstrated a self-oscillating neuron based on Pt/NbO\nBeyond periodic spiking, the neural firing mechanism follows a principle of temporal integration: if multiple synaptic inputs collectively exceed the threshold within a certain time window, a spike is fired; otherwise, the integrated input gradually attenuates. This behavior is typically modeled by the LIF neuron, which captures both integration and leakage processes. The “leaky” membrane potential of neurons is analogous to the conductivity decay in volatile memristors, which reflects a key dynamic aspect of memory decay. The volatility of memristors allows artificial neurons to autonomously return to the resting potential after firing, or revert to a quiescent state if the input is insufficient to trigger firing. As shown in\nZhang et al. developed a LIF neuron based on Ag/SiO\nH-H neurons represent one of the most biologically realistic models in computational neuroscience, capable of reproducing a wide range of electrophysiological behaviors such as action potentials, thresholding, and refractory dynamics. Implementing H-H-type neurons with memristive elements requires fine-grained control over ionic conductance, which can be achieved through TSMs that emulate the gating dynamics of voltage-dependent ion channels. A pioneering example was demonstrated by Pickett et al., who utilized Mott-type memristors to mimic the dynamic behaviors of Na\nHowever, while the implementation achieves high fidelity in replicating the H-H model, it also exhibits substantial complexity and overhead. The requirement for multiple discrete components—including separate voltage sources, resistors, and capacitors—limits its scalability and integrability in dense neuromorphic circuits. Additionally, the relatively high power consumption and limited dynamic range of Mott memristors present challenges for practical edge-AI applications that demand energy efficiency and robustness under variability. From a computational perspective, the detailed biophysics modeled by H-H neurons are often overkill for many real-world tasks, where simpler models such as LIF or adaptive spiking suffice with significantly less hardware complexity. Nevertheless, H-H-type implementations are valuable for benchmarks, biological validation, and exploratory modeling, especially when studying complex ion–channel interactions, pharmacological responses, or neuron-level pathologies.\nLooking forward, future H-H-inspired implementations may benefit from compact, hybrid architectures that integrate memristive dynamics with programmable analog/digital modules. This approach could preserve key nonlinear behaviors while reducing the circuit size and improving the power efficiency. Additionally, material engineering in Mott systems—for example, by controlling phase transition thresholds or improving the thermal stability—could further enhance the reproducibility and endurance of these neurons in large-scale neuromorphic platforms.\nThe stochastic behavior of memristors is one of the intriguing characteristics that can play a significant role in neuromorphic computing systems, particularly in randomness-based applications such as true random number generation (TRNG). Stochastic switching in memristors primarily originates from atomic migration, thermal noise, and electron correlation effects, varying by the material type. In redox-based memristors, the movement of metal ions under an applied electric field is a key factor in generating stochasticity. This migration leads to the formation of conductive filaments in the dielectric material, and the random nature of this filament growth contributes to variability in the device’s resistance. As the SET and RESET operations occur, the resistance states can fluctuate due to the inherent randomness in atomic migration. In Mott memristors, stochastic switching originates from critical electron–electron interactions and phase transition dynamics, which are highly sensitive to local temperature and electric field gradients. These phenomena result in observable cycle-to-cycle and device-to-device variations in the switching delay, threshold voltage, and retention characteristics.\nThe inherent stochasticity in memristive devices, particularly in TSMs, offers a valuable mechanism to emulate the probabilistic behavior of biological neurons. Unlike deterministic neurons, which fire in response to precise input conditions, biological neurons often exhibit variable firing thresholds and spike timing due to thermal fluctuations and ion channel noise. Emulating such stochasticity is critical for realizing probabilistic computation, Bayesian inference, and uncertainty quantification in neuromorphic systems.\nThis stochasticity, while traditionally viewed as a source of unreliability, can be purposefully exploited. For instance, Miao et al. developed an electronic stochastic neuron using a CuS/GeSe-based TSM, capable of mimicking random firing behavior observed in biological neurons and enabling Bayesian inference within a spiking neural network (SNN) [\nBeyond probabilistic neural modeling, the stochastic behavior of TSMs is also suitable for hardware-based TRNGs. Phase-change neurons, as demonstrated by Tuma et al., leverage intrinsic thermal noise and threshold dispersion for robust random spike generation, offering potential for secure neuromorphic encryption and stochastic optimization [\nThese studies demonstrate the promise of stochastic memristive neurons in probabilistic inference and unsupervised learning tasks. Nonetheless, these models face notable limitations. The reliance on physical noise sources introduces significant variability not only across devices, but also across time in the same device, making consistent spike-rate modulation and task-specific calibration challenging. Moreover, stochasticity in spike generation, while useful for probabilistic inference, can undermine the performance in applications requiring deterministic or time-locked responses, such as sequence learning or temporal pattern recognition. Additionally, the mapping from input stimuli to desired firing probabilities often lacks linearity or interpretability, complicating training and co-optimization with learning algorithms.\nCompared to LIF and oscillatory neurons, stochastic memristive neurons provide a valuable mechanism for modeling neural noise and probabilistic computation, but their application scope is more suited to ensemble or redundancy-based networks rather than precision-centric tasks. Future work may benefit from integrating tunable stochasticity (e.g., a bias-controlled noise injection) or leveraging noise-aware learning frameworks such as contrastive divergence or spike-based variational inference to enhance the performance and system stability.\nLooking forward, engineering stochasticity at the device level through the material design, geometry optimization, and temperature control could enable tunable randomness, paving the way for neuromorphic hardware platforms with adaptive uncertainty handling. Moreover, hybrid architectures combining deterministic and stochastic neurons may allow for biologically inspired trade-offs between precision and flexibility in spiking neural networks. In summary, the stochastic properties of memristors—once considered detrimental—are increasingly being harnessed to emulate the noisy dynamics of biological neurons. This opens new avenues for energy-efficient probabilistic computing and robust neuromorphic perception systems, especially in real-world environments where uncertainty and variability are inherent.\nBeyond the fundamental neuron types, complex firing patterns including burst firing, frequency adaptation, and neuronal inhibition are also critical for enabling energy-efficient neural computation. Yi et al. demonstrated over twenty spiking behaviors using VO\nWhile a range of memristor-based neuron implementations—from redox and Mott devices to TaO\nIn summary, TSMs provide a solid hardware foundation for building compact, low-power, and biologically plausible spiking neurons due to their pronounced nonlinear switching properties. By integrating TSMs with passive elements such as capacitors and resistors, various spiking behaviors—including oscillatory, LIF, stochastic, and H-H types—have been successfully replicated, with support for reconfigurable neuron implementations. These efforts not only greatly reduce the circuit complexity, but also demonstrate excellent energy efficiency and integration potential, advancing neuromorphic computing from basic units to multifunctional and scalable systems. Memristors offer unparalleled advantages in hardware-level spike encoding, probabilistic reasoning, and multimodal neural dynamics, laying the groundwork for next-generation intelligent and adaptive brain-like systems.\nHumans rely on photoreceptors, thermoreceptors, mechanoreceptors, and olfactory receptors to receive physical signals from the external environment, which are encoded into spikes and transmitted to neural networks for perception and learning [\nWith the rapid development of intelligent robotics, autonomous driving, and wearable electronics, the demand for artificial vision systems with high efficiency, low-power consumption, and real-time responsiveness is growing rapidly [\nIn the biological visual system, retinal photoreceptors detect and integrate optical signals and transmit them to the brain’s visual cortex.\nFurthermore, Li et al. integrated synaptic phototransistors (BPR PT) with NbO\nIn biological systems, tactile neurons like cutaneous mechanoreceptors transduce external pressure into spike signals, which are relayed to the somatosensory cortex for further processing to discern object properties and avoid potentially harmful stimuli [\nYang et al. introduced a neuromorphic tactile system based on coupled VO\nIn the somatosensory system, thermoreceptors play a critical role in regulating metabolic processes and preventing damage from harmful external stimuli [\nBuilding upon the previously discussed unimodal perception systems—covering vision, touch, thermal, and olfactory domains—recent research has emphasized the integration of multiple sensory modalities. In biological somatosensory systems, multimodal integration allows the comprehensive perception of object attributes, enabling precise decision making [\nLi et al. introduced a flexible biomimetic cross-modal spiking neuron (CSSN) based on VO\nFor motion sensing and control, Yang et al. proposed a spike-coding neural circuit driven by emission characteristics, integrating a LiDAR sensor with three H-H neurons based on NbO\nThese systems embody the paradigm of “perception-as-computation”: encoding and processing occur directly at the signal source, reducing redundancy and latency, suitable for resource-constrained edge AI applications. Nevertheless, challenges remain in device uniformity, heterogeneous integration, and stability; future progress will rely on incorporating plasticity (e.g., STDP), vertical stacking, and 3D crossbar architectures for scalable integration and autonomous perception.\nThis review has provided a comprehensive analysis of memristor-based spiking neuromorphic systems, highlighting their unique advantages in enabling brain-inspired perception and computation. By accurately replicating essential neuronal behaviors—such as temporal integration, threshold-triggered firing, spike-frequency adaptation, and stochasticity—memristors offer a compact, scalable, and energy-efficient alternative to conventional CMOS-based approaches. Their intrinsic nonlinear switching dynamics, analog conductance modulation, and compatibility with crossbar arrays further allow memristors to serve as both neurons and synapses within integrated SNN platforms.\nWe systematically summarized the underlying physical mechanisms of TSMs, critically examined representative spiking neuron implementations—including LIF, oscillatory, stochastic, and H-H models—and reviewed their applications across multimodal sensory systems such as vision, tactile sensing, thermosensation, and olfaction. These studies underscore the growing potential of memristor-based hardware in enabling near-sensor, bio-inspired computing at the edge.\nDespite these advances, significant challenges persist, notably addressing variability across devices, enhancing the switching endurance, and mitigating timing dispersion in large-scale arrays. These non-idealities manifest as inconsistent thresholds, retention issues, and timing errors, which can compromise temporal precision in spiking networks. In crossbar architectures, sneak-path currents and read/write disturbances further impair long-term stability and energy efficiency. Addressing these limitations demands a multi-level co-optimization strategy—including material innovation (e.g., doped oxides and 2D heterostructures), selector integration, circuit-level compensation, and training algorithms resilient to hardware imperfections. Standardized benchmarking protocols and the statistical modeling of switching kinetics will also be critical for robust large-scale deployment. To advance toward practical implementation, future work should explore novel material stacks and advanced fabrication methods to mitigate device variability and enhance the cycle stability for large-scale integration. Co-design approaches that align device physics, circuit design, and network algorithms will be essential for achieving a real-time, adaptive performance in neuromorphic platforms.\nAs neuromorphic computing intersects with emerging domains such as intelligent robotics, brain–machine interfaces, and personalized healthcare, demands on low latency, environmental robustness, and on-device learning capabilities will intensify. Memristor-based neuromorphic systems, with their compact form factor and biologically plausible dynamics, could serve as foundational hardware for next-generation cognitive electronics. While broad deployment remains an ongoing challenge, the progress summarized in this review provides both a roadmap and a technical foundation for future innovations in brain-inspired, energy-efficient computation.", "content_for_embedding": "Edge-AI workloads, such as 4 K/60 fps video (≈12 Gb/s) or sub-10 ms-latency drone navigation [\nTheir two-terminal structure further facilitates seamless integration into neuromorphic systems. Volatile TSMs underpin both (i) the device-level emulation of diverse Neuronal firing behaviors—regular, bursting, adaptive, and stochastic—and (ii) system-level sensory front-ends that transduce the light, pressure, or gas concentration directly into spike trains [\nDespite notable progress in various aspects, memristor-based neuromorphic systems still face several challenges for large-scale deployment, such as significant device variability, limited endurance, and stochastic switching behaviors, all of which may compromise the computational accuracy and scalability. Furthermore, efficient learning algorithms that account for the non-ideal characteristics of memristors are still under active development. Ongoing efforts in approximate computing, CMOS–memristor hybrid design, and the co-optimization of algorithms and hardware aim to overcome these obstacles and accelerate the transition toward practical applications.\nIn this review, we present a comprehensive assessment of memristors in enabling spiking neuromorphic systems targeting brain-like perception and computation. The discussion begins with the biological basis of neural spiking and common computational models, followed by the underlying switching physics of memristors and their utility in mimicking neuronal dynamics. We then summarize key developments in spiking neuron circuit designs based on memristors, highlight applications in vision, touch, and multimodal bioinspired sensing, and conclude by identifying major challenges and future perspectives.\nWhile several prior reviews have examined memristors for neuromorphic computing, most focus predominantly on synaptic functionalities or general device mechanisms without emphasizing spiking neuron implementations. Furthermore, relatively few works systematically analyze how TSMs uniquely enable diverse spiking neuron behaviors—such as leaky integration, oscillations, stochastic firing, and action potential-like dynamics—at the device and circuit levels. Moreover, the interplay between memristive neuron models and multi-modal sensory systems (e.g., vision, touch, and olfaction) remains underexplored.\nThis review seeks to bridge these gaps by providing a focused, structured, and critical evaluation of memristor-based spiking neuromorphic systems, encompassing device mechanisms, circuit implementations, and bio-inspired perception applications. In doing so, it offers a unified perspective on how TSM-based neurons can be harnessed for energy-efficient, near-sensor computing, thereby advancing the state-of-the-art in neuromorphic engineering.\nBiological neurons encode and transmit information through discrete electrical spikes, known as action potentials, rather than the clock-synchronized binary levels of digital logic [\nBiological neurons display diverse spiking patterns to fulfill various functional roles in the brain. This functional diversity underpins the brain’s capacity for temporal encoding, information compression, and stimulus discrimination. Replicating these firing patterns in hardware implementations is essential for achieving biologically faithful neuromorphic architectures. Thus, a thorough understanding of the electrophysiological mechanisms underlying neuronal excitability and information coding is crucial for advancing neuroscience and designing future spike-based hardware systems.\nThe Hodgkin–Huxley (H-H) model provides a quantitative framework for describing the biophysical mechanisms underlying action potential generation in neurons [\nWhile the H-H model provides a highly detailed and biologically realistic simulation of neuronal behavior, its computational complexity is significant. Each time-step simulation typically requires around 1200 floating-point operations, making it computationally expensive and limiting its scalability for large networks or hardware implementations. This high computational cost presents challenges for real-time applications, particularly in neuromorphic systems where low power and efficiency are crucial. To reduce the computational cost while retaining biologically relevant timing, simplified neuron models have been proposed. Notably, the LIF model has become the predominant choice in theoretical neuroscience and neuromorphic implementations [\nBeyond the LIF model, several other neuron models—such as the Izhikevich, FitzHugh–Nagumo (FHN), and Hindmarsh–Rose models—have been introduced to capture more complex neuronal dynamics. The two-variable Izhikevich model matches 20 neuronal firing patterns at 50× less computation than H-H, while FHN captures excitability class-I/II bifurcation with analytical tractability [\nIn conclusion, spiking neuron modeling serves as a critical bridge between biological neuroscience and AI system design. Ranging from the intricate ion–channel kinetics of the H-H model to the computationally efficient abstractions of the LIF and Izhikevich models, the field is evolving toward neuron models that balance biological fidelity with hardware implementability [\nSelecting appropriate spiking neuron models hinges on balancing biological realism and computational efficiency. Therefore, in neuromorphic engineering—where energy efficiency and scalability are paramount—identifying suitable hardware-level implementations of neuron models can significantly simplify computation. Memristors, with their inherent hardware compatibility and intrinsic nonlinearities that emulate ion–channel dynamics, emerge as promising candidates for implementing diverse spiking behaviors in low-power, compact hardware systems. When implemented with memristors, these simplified models benefit from the inherent characteristics of memristive devices, such as threshold-switching and nonlinear behavior, to replicate the action potential dynamics seen in biological neurons. The trade-offs between these models in terms of biological fidelity versus hardware implementation are critical for choosing the right model for a given application. For example, while the H-H model offers high fidelity, it is challenging to implement efficiently in low-power neuromorphic systems due to its high computational demand.\nIn 1962, negative differential resistance (NDR) was first observed in metal–oxide sandwich structures based on Al\nTSMs used in neuron circuit design are mainly classified into two types based on their switching mechanisms: electrochemical redox and Mott memristors [\nMott memristors, depicted in\nResearchers have implemented a variety of TSM-driven neuron circuits to emulate diverse spiking patterns, including oscillatory [\nA key goal of neuromorphic engineering is to design compact, energy-efficient electronic elements that accurately emulate the spiking dynamics of biological neurons. Among emerging devices, TSMs provide a distinctive approach to emulate spiking neuron functionalities. Leveraging their intrinsic nonlinear dynamics, spontaneous threshold switching, and time-dependent behavior, researchers have constructed numerous artificial spiking neurons. This section presents a taxonomy of key neuron circuit architectures, implementation strategies, and performance metrics.\nOscillatory neurons emulate the rhythmic firing behavior observed in biological neurons by generating periodic spikes when driven by either constant voltage or current inputs. This neuron type is particularly suitable for simulating synchronized and rhythmic neuronal activity, commonly found in neuronal oscillations.\nIn a current-driven oscillatory neuron circuit, as illustrated in\nThese memristor-based oscillatory neurons have been extensively applied in neuromorphic computing tasks. Gao et al. demonstrated a self-oscillating neuron based on Pt/NbO\nBeyond periodic spiking, the neural firing mechanism follows a principle of temporal integration: if multiple synaptic inputs collectively exceed the threshold within a certain time window, a spike is fired; otherwise, the integrated input gradually attenuates. This behavior is typically modeled by the LIF neuron, which captures both integration and leakage processes. The “leaky” membrane potential of neurons is analogous to the conductivity decay in volatile memristors, which reflects a key dynamic aspect of memory decay. The volatility of memristors allows artificial neurons to autonomously return to the resting potential after firing, or revert to a quiescent state if the input is insufficient to trigger firing. As shown in\nZhang et al. developed a LIF neuron based on Ag/SiO\nH-H neurons represent one of the most biologically realistic models in computational neuroscience, capable of reproducing a wide range of electrophysiological behaviors such as action potentials, thresholding, and refractory dynamics. Implementing H-H-type neurons with memristive elements requires fine-grained control over ionic conductance, which can be achieved through TSMs that emulate the gating dynamics of voltage-dependent ion channels. A pioneering example was demonstrated by Pickett et al., who utilized Mott-type memristors to mimic the dynamic behaviors of Na\nHowever, while the implementation achieves high fidelity in replicating the H-H model, it also exhibits substantial complexity and overhead. The requirement for multiple discrete components—including separate voltage sources, resistors, and capacitors—limits its scalability and integrability in dense neuromorphic circuits. Additionally, the relatively high power consumption and limited dynamic range of Mott memristors present challenges for practical edge-AI applications that demand energy efficiency and robustness under variability. From a computational perspective, the detailed biophysics modeled by H-H neurons are often overkill for many real-world tasks, where simpler models such as LIF or adaptive spiking suffice with significantly less hardware complexity. Nevertheless, H-H-type implementations are valuable for benchmarks, biological validation, and exploratory modeling, especially when studying complex ion–channel interactions, pharmacological responses, or neuron-level pathologies.\nLooking forward, future H-H-inspired implementations may benefit from compact, hybrid architectures that integrate memristive dynamics with programmable analog/digital modules. This approach could preserve key nonlinear behaviors while reducing the circuit size and improving the power efficiency. Additionally, material engineering in Mott systems—for example, by controlling phase transition thresholds or improving the thermal stability—could further enhance the reproducibility and endurance of these neurons in large-scale neuromorphic platforms.\nThe stochastic behavior of memristors is one of the intriguing characteristics that can play a significant role in neuromorphic computing systems, particularly in randomness-based applications such as true random number generation (TRNG). Stochastic switching in memristors primarily originates from atomic migration, thermal noise, and electron correlation effects, varying by the material type. In redox-based memristors, the movement of metal ions under an applied electric field is a key factor in generating stochasticity. This migration leads to the formation of conductive filaments in the dielectric material, and the random nature of this filament growth contributes to variability in the device’s resistance. As the SET and RESET operations occur, the resistance states can fluctuate due to the inherent randomness in atomic migration. In Mott memristors, stochastic switching originates from critical electron–electron interactions and phase transition dynamics, which are highly sensitive to local temperature and electric field gradients. These phenomena result in observable cycle-to-cycle and device-to-device variations in the switching delay, threshold voltage, and retention characteristics.\nThe inherent stochasticity in memristive devices, particularly in TSMs, offers a valuable mechanism to emulate the probabilistic behavior of biological neurons. Unlike deterministic neurons, which fire in response to precise input conditions, biological neurons often exhibit variable firing thresholds and spike timing due to thermal fluctuations and ion channel noise. Emulating such stochasticity is critical for realizing probabilistic computation, Bayesian inference, and uncertainty quantification in neuromorphic systems.\nThis stochasticity, while traditionally viewed as a source of unreliability, can be purposefully exploited. For instance, Miao et al. developed an electronic stochastic neuron using a CuS/GeSe-based TSM, capable of mimicking random firing behavior observed in biological neurons and enabling Bayesian inference within a spiking neural network (SNN) [\nBeyond probabilistic neural modeling, the stochastic behavior of TSMs is also suitable for hardware-based TRNGs. Phase-change neurons, as demonstrated by Tuma et al., leverage intrinsic thermal noise and threshold dispersion for robust random spike generation, offering potential for secure neuromorphic encryption and stochastic optimization [\nThese studies demonstrate the promise of stochastic memristive neurons in probabilistic inference and unsupervised learning tasks. Nonetheless, these models face notable limitations. The reliance on physical noise sources introduces significant variability not only across devices, but also across time in the same device, making consistent spike-rate modulation and task-specific calibration challenging. Moreover, stochasticity in spike generation, while useful for probabilistic inference, can undermine the performance in applications requiring deterministic or time-locked responses, such as sequence learning or temporal pattern recognition. Additionally, the mapping from input stimuli to desired firing probabilities often lacks linearity or interpretability, complicating training and co-optimization with learning algorithms.\nCompared to LIF and oscillatory neurons, stochastic memristive neurons provide a valuable mechanism for modeling neural noise and probabilistic computation, but their application scope is more suited to ensemble or redundancy-based networks rather than precision-centric tasks. Future work may benefit from integrating tunable stochasticity (e.g., a bias-controlled noise injection) or leveraging noise-aware learning frameworks such as contrastive divergence or spike-based variational inference to enhance the performance and system stability.\nLooking forward, engineering stochasticity at the device level through the material design, geometry optimization, and temperature control could enable tunable randomness, paving the way for neuromorphic hardware platforms with adaptive uncertainty handling. Moreover, hybrid architectures combining deterministic and stochastic neurons may allow for biologically inspired trade-offs between precision and flexibility in spiking neural networks. In summary, the stochastic properties of memristors—once considered detrimental—are increasingly being harnessed to emulate the noisy dynamics of biological neurons. This opens new avenues for energy-efficient probabilistic computing and robust neuromorphic perception systems, especially in real-world environments where uncertainty and variability are inherent.\nBeyond the fundamental neuron types, complex firing patterns including burst firing, frequency adaptation, and neuronal inhibition are also critical for enabling energy-efficient neural computation. Yi et al. demonstrated over twenty spiking behaviors using VO\nWhile a range of memristor-based neuron implementations—from redox and Mott devices to TaO\nIn summary, TSMs provide a solid hardware foundation for building compact, low-power, and biologically plausible spiking neurons due to their pronounced nonlinear switching properties. By integrating TSMs with passive elements such as capacitors and resistors, various spiking behaviors—including oscillatory, LIF, stochastic, and H-H types—have been successfully replicated, with support for reconfigurable neuron implementations. These efforts not only greatly reduce the circuit complexity, but also demonstrate excellent energy efficiency and integration potential, advancing neuromorphic computing from basic units to multifunctional and scalable systems. Memristors offer unparalleled advantages in hardware-level spike encoding, probabilistic reasoning, and multimodal neural dynamics, laying the groundwork for next-generation intelligent and adaptive brain-like systems.\nHumans rely on photoreceptors, thermoreceptors, mechanoreceptors, and olfactory receptors to receive physical signals from the external environment, which are encoded into spikes and transmitted to neural networks for perception and learning [\nWith the rapid development of intelligent robotics, autonomous driving, and wearable electronics, the demand for artificial vision systems with high efficiency, low-power consumption, and real-time responsiveness is growing rapidly [\nIn the biological visual system, retinal photoreceptors detect and integrate optical signals and transmit them to the brain’s visual cortex.\nFurthermore, Li et al. integrated synaptic phototransistors (BPR PT) with NbO\nIn biological systems, tactile neurons like cutaneous mechanoreceptors transduce external pressure into spike signals, which are relayed to the somatosensory cortex for further processing to discern object properties and avoid potentially harmful stimuli [\nYang et al. introduced a neuromorphic tactile system based on coupled VO\nIn the somatosensory system, thermoreceptors play a critical role in regulating metabolic processes and preventing damage from harmful external stimuli [\nBuilding upon the previously discussed unimodal perception systems—covering vision, touch, thermal, and olfactory domains—recent research has emphasized the integration of multiple sensory modalities. In biological somatosensory systems, multimodal integration allows the comprehensive perception of object attributes, enabling precise decision making [\nLi et al. introduced a flexible biomimetic cross-modal spiking neuron (CSSN) based on VO\nFor motion sensing and control, Yang et al. proposed a spike-coding neural circuit driven by emission characteristics, integrating a LiDAR sensor with three H-H neurons based on NbO\nThese systems embody the paradigm of “perception-as-computation”: encoding and processing occur directly at the signal source, reducing redundancy and latency, suitable for resource-constrained edge AI applications. Nevertheless, challenges remain in device uniformity, heterogeneous integration, and stability; future progress will rely on incorporating plasticity (e.g., STDP), vertical stacking, and 3D crossbar architectures for scalable integration and autonomous perception.\nThis review has provided a comprehensive analysis of memristor-based spiking neuromorphic systems, highlighting their unique advantages in enabling brain-inspired perception and computation. By accurately replicating essential neuronal behaviors—such as temporal integration, threshold-triggered firing, spike-frequency adaptation, and stochasticity—memristors offer a compact, scalable, and energy-efficient alternative to conventional CMOS-based approaches. Their intrinsic nonlinear switching dynamics, analog conductance modulation, and compatibility with crossbar arrays further allow memristors to serve as both neurons and synapses within integrated SNN platforms.\nWe systematically summarized the underlying physical mechanisms of TSMs, critically examined representative spiking neuron implementations—including LIF, oscillatory, stochastic, and H-H models—and reviewed their applications across multimodal sensory systems such as vision, tactile sensing, thermosensation, and olfaction. These studies underscore the growing potential of memristor-based hardware in enabling near-sensor, bio-inspired computing at the edge.\nDespite these advances, significant challenges persist, notably addressing variability across devices, enhancing the switching endurance, and mitigating timing dispersion in large-scale arrays. These non-idealities manifest as inconsistent thresholds, retention issues, and timing errors, which can compromise temporal precision in spiking networks. In crossbar architectures, sneak-path currents and read/write disturbances further impair long-term stability and energy efficiency. Addressing these limitations demands a multi-level co-optimization strategy—including material innovation (e.g., doped oxides and 2D heterostructures), selector integration, circuit-level compensation, and training algorithms resilient to hardware imperfections. Standardized benchmarking protocols and the statistical modeling of switching kinetics will also be critical for robust large-scale deployment. To advance toward practical implementation, future work should explore novel material stacks and advanced fabrication methods to mitigate device variability and enhance the cycle stability for large-scale integration. Co-design approaches that align device physics, circuit design, and network algorithms will be essential for achieving a real-time, adaptive performance in neuromorphic platforms.\nAs neuromorphic computing intersects with emerging domains such as intelligent robotics, brain–machine interfaces, and personalized healthcare, demands on low latency, environmental robustness, and on-device learning capabilities will intensify. Memristor-based neuromorphic systems, with their compact form factor and biologically plausible dynamics, could serve as foundational hardware for next-generation cognitive electronics. While broad deployment remains an ongoing challenge, the progress summarized in this review provides both a roadmap and a technical foundation for future innovations in brain-inspired, energy-efficient computation.", "topic": "Brain"}
{"pmid": "39930653", "pmcid": "12307468", "title": "Topographic associations of hyperreflective materials in diabetic retinopathy: a multimodal correlation with microvascular pathology, structural remodeling and systemic metabolic dysregulation", "publication_year": "N/A", "abstract": "", "full_text": "Hyperreflective materials (HRMs), alternatively termed hyperreflective foci or spots, represent distinct optical coherence tomography (OCT) findings characterized by intraretinal or subretinal punctate hyperreflectivity (\nThe advent of optical coherence tomography angiography (OCTA) provides a critical technological bridge, enabling simultaneous non-invasive visualization of retinal vasculature and HRMs topography. Leveraging this dual imaging capability, our study systematically investigates: (1) spatial concordance between HRMs distribution and vascular abnormalities in DR. (2) HRMs-macular edema interactions across disease stages. (3) Metabolic correlates of HRMs burden, particularly in relation to glycemic control markers.\nThis is a retrospective, cross-sectional study performed on 105 patients diagnosed with DR by the Department of Ophthalmology of Shenzhen People's Hospital between January 2020 and May 2023. All the research and measurements were conducted in compliance with the tenets of the Declaration of Helsinki. The study was approved by the ethics committee of the hospital and informed consent was obtained from all individuals after a detailed discussion of the nature and possible consequences of the study procedures.\nAll participants underwent standardized multimodal ophthalmic evaluations including: slit-lamp biomicroscopy examination, Fundus Fluorescein Angiography (Topcon, TRC 50IA; Tokyo, Japan), OCTA (Zeiss, CIRRUS AngioPlex, Dublin, CA). Demographic and clinical factors including age, sex, history of hypertension, and serum lipid profile, glucose metabolism, leukocytes number, duration of diabetes and the grade of diabetic retinopathy were also reviewed. Inclusion criteria were as follows: (1) age above 18 years; (2) type 1 or 2 diabetes mellitus; and (3) the grade of DR was based on the international clinical diabetic retinopathy disease severity scale and DME was based on international clinical diabetic macular edema disease severity scale proposed by American Academy of Ophthalmology in 2019. Exclusion criteria were: (1) presence of any other retinal disorder such as retinal vein occlusion, retinal detachment, uveitis or any other maculopathy; (2) other ocular condition that compromises media opacities (i.e., vitreous hemorrhage or mature cataract); (3) previous treatment with any intraocular surgery, laser photocoagulation or intravitreal injection of anti-VEGF or corticosteroids drug; (4) clearly identify patients taking lipid-lowering drugs such as fenofibrate, statins et al.; and (5) poor quality OCTA images defined by the signal strength index of <5/10.\nMacular-centered 6 × 6 mm scan grids (350 × 350 A-scan density) were acquired using Cirrus 5000 Angioplex. Optical microangiography algorithms generated three-dimensional vascular maps through semi-automated segmentation (AngioPlex Metrix v10.0, an integrated analytical software suite for Cirrus 5000 Angioplex and available through two primary channels: directly via the OCT imaging system or through the ZEISS FORUM platform), which is comprise of structural OCT B-scans with flow overlays, superficial/deep vascular plexus en face projections and depth-resolved capillary density maps. Vascular layers were segmented using validated anatomical landmarks: the superficial vascular plexus was defined as extending from the internal limiting membrane to 9 μm above the inner plexiform layer-inner nuclear layer (IPL-INL) junction, and the deep vascular complex encompassed the region from 9 μm above the IPL-INL junction to 9 μm below the outer plexiform layer-outer nuclear layer (OPL-ONL) junction. Corresponding structural en face OCT images were analyzed using identical segmentation slabs to ensure anatomical correspondence. The walls of cystoid spaces are typically formed by adjacent parenchyma or thin, highly reflective lines resulting from optical property differences at the boundaries following hyporeflective fluid accumulation. The border of a serous retinal detachment is defined as the demarcation zone between the detached neurosensory retina and RPE, with underlying hyporeflective subretinal fluid causing an abrupt optical reflectivity transition. All automated segmentations underwent rigorous quality assessment: (1) B-scans with flow overlay were systematically reviewed to identify HRMs or hyperreflective spots exhibiting decorrelation signals; (2) segmentation accuracy was verified through dynamic B-scan navigation using orthogonal (horizontal/vertical) reference lines; and (3) manual corrections were applied when segmentation errors were detected, particularly at layer boundary transitions.\nFour distinct categories of pathological microvascular alterations detectable by OCT angiography (OCTA) were analyzed and their morphological classifications were established based on well-characterized criteria from prior validated studies (\nTwo independent masked retinal specialists (L.Z. and H.S.) performed qualitative and quantitative assessments of HRMs using registered B-scan and en face OCT/OCTA images. A senior retinal specialist (M.M.Y.) served as adjudicator for discordant cases to ensure consensus. Each HRMs was evaluated based on the following standardized criteria: (1) anatomical location [inner retina defined as layers between inner limiting membrane and inner nuclear layer, outer retina defined as layers below the inner boundary of the outer nuclear layer (\nInter-rater reliability was quantified via intraclass correlation coefficients (ICCs) with 95% confidence intervals. To examine the association between HRMs subtypes and retinal vascular pathology, we performed the following analyses: (1) the Kruskal–Wallis test was employed to compare HRMs counts across the four predefined categories of abnormal vascular morphology; (2)\nThe study cohort comprised 105 treatment-naïve diabetic retinopathy (DR) patients (69 males, 36 females; mean age 55.0 ± 8.1 years) undergoing standardized OCTA imaging, yielding 205 evaluable eyes (104 right, 101 left).\nBaseline demographics and ocular characteristics of the whole cohort of patients with hyperreflective materials detected on optical coherence tomography angiography.\nDR, diabetic retinopathy; NPDR, non-proliferative diabetic retinopathy; DME, diabetic macular edema; LogMAR, logarithm of the minimal angle of resolution.\nSix distinct HRMs subtypes were systematically classified through multimodal image analysis (\nCategory of hyperreflective materials based on characters including location, reflectivity, size and presence or absence of back shadowing, decorrelation signal on B-scan and structural en face images (6 × 6 mm scanning area) using OCTA. The scanning position of each image is identified by horizontal (lake blue) or vertical navigation (purple) line. IR is defined as layers between the inner limiting membrane and the OPL; OR is defined as the Henle NFL and outer nuclear layer.\nAssociation of hyperreflective materials distribution with vascular abnormalities including no perfusion area, neovascular membrane and microneurysm.\nQuantitative analysis revealed distinct topographic distributions of hyperreflective materials (HRMs) across four retinal vascular abnormalities visualized by OCTA, with excellent intergrader reliability (\nDistribution of HRMs in the retinal vascular map of optical coherence tomography angiography.\nHRMs, hyperreflective materials; IRHFs, inner retinal hyperreflective spots; ORHFs, outer retinal hyperreflective spots; IRHE, inner retinal hard exudates; ORHE, outer retinal hard exudates; NP, no-perfusion; IRMA, intra-retinal microvascular abnormalities.\nDistribution of hyperreflective materials in the area of intra-retinal microvascular abnormalities (IRMA) detected by OCTA. The scanning position of each image is identified by horizontal (lake blue) or vertical navigation (purple) line.\nTo investigate the potential role of HRMs in diabetic macular edema (DME), we stratified patients with DR into two groups: those with DME and those without DME. Quantitative analysis revealed significantly higher HRMs counts in DME eyes across all vascular abnormalities (all\nComparison of hyperreflective materials number between DR with DME and DR without DME.\nHRMs, hyperreflective materials; IRHFs, inner retinal hyperreflective spots; ORHFs, outer retinal hyperreflective spots; IRHE, inner retinal hard exudates; ORHE, outer retinal hard exudates; NP, no-perfusion; IRMA, intra-retinal microvascular abnormalities, DR, diabetic retinopathy; DME, diabetic macular edema. Bold values indicate statistically and clinically significant data (\nOur generalized estimating equation (GEE) analysis presented in\nCorrelation of systemic metabolic traits with HRMs.\nHRMs, hyperreflective materials; IRHFs, inner retinal hyperreflective spots; ORHFs, outer retinal hyperreflective spots; IRHE, inner retinal hard exudates; ORHE, outer retinal hard exudates; DM, diabetes mellitus; DR, diabetic retinopathy; FBG, fasting blood glucose; TC, total cholesterol; TG, triglyceride; HDL, high-density lipoprotein; LDL, low-density lipoprotein; apolipoprotein a and b, APO a and APO b; WBC, white blood cell; UCR, urinary albumin-creatinine ration; CI, confidence interval. Bold values indicate statistically and clinically significant data (\nThis study systematically investigated the characteristics and spatial distribution of hyperreflective materials (HRMs) in diabetic retinopathy (DR) using OCT angiography (OCTA), while exploring their associations with vascular abnormalities and systemic metabolic dysregulation. By integrating morphological criteria (location, reflectivity, size, back-shadowing, and decorrelation signals) with OCTA features, we identified six distinct HRMs subtypes: inner/outer retinal hyperreflective foci (IRHFs/ORHFs), inner/outer retinal hard exudates (IRHE/ORHE), decorrelation-positive HRMs, and cotton-wool spots. Notably, 75%−94% of HRMs localized to regions of IRMA or microaneurysms, with pronounced enrichment in diabetic macular edema cases. Our findings further linked HRMs prevalence to systemic lipid dysregulation, suggesting their dual origin in blood-retinal barrier (BRB) disruption and metabolic dysfunction.\nThe conceptualization of intraretinal HRMs—termed hyperreflective dots (HRDs) or foci in prior studies—has undergone substantial refinement since their initial characterization by Bolz et al. (\nPrior classification frameworks categorized hyperreflective materials (HRMs) into three subtypes based on OCT-derived morphometrics: microglial-like foci (<30 μm, moderate reflectivity), hard exudates (>30 μm, RPE-comparable reflectivity with shadowing), and microaneurysm-associated deposits (>30 μm, inner retinal localization) (\nThe ontogeny and pathobiological significance of HRMs remain contentious, with three predominant mechanistic hypotheses emerging from prior research. One of the main hypotheses is that HRMs are considered as aggregation of activated microglia, therefore always are proposed as a biomarker of neuro-retinal inflammation (\nThe ultimate aim of studying the HRMs is to direct the individualized treatment of DR. So far, anti-VEGF agents, intravitreal steroid injections or implants, laser and vitrectomy are all considered as the popular therapy (\nThis work has several constraints: (1) cross-sectional design precludes assessment of HRMs evolution or causality; (2) single-timepoint metabolic measures may not capture dynamic interactions with retinal pathology; (3) unmeasured confounders (genetic/epigenetic factors, metabolic memory effects) could influence observed associations. Future studies integrating serial OCTA with multi-omics profiling will clarify HRMs pathophysiology and therapeutic relevance; and (4) the classification of HRMs relies on manual annotation, and although calibrated by expert consensus, there may be interpretation bias particularly for small lesions with poorly defined margins (e.g., intraretinal hyperreflective foci [IRHFs] ≤ 30 μm). Future investigations incorporating serial OCTA imaging with artificial intelligence (AI) image recognition technology will be essential to elucidate the pathophysiological mechanisms and therapeutic implications of HRMs.\nThis study establishes a novel OCTA-guided classification system for HRMs in DR, delineating their spatial association with vascular abnormalities (particularly IRMA and microaneurysms) and systemic lipid dysregulation. The dual pathogenesis of HRMs—rooted in blood-retinal barrier disruption and metabolic dysfunction—highlights their potential as biomarkers for disease staging and therapeutic targeting. Our findings advocate for integrating lipid-lowering strategies with conventional DME therapies, proposing a paradigm shift toward personalized interventions that address both vascular and metabolic drivers of DR progression.", "content_for_embedding": "Hyperreflective materials (HRMs), alternatively termed hyperreflective foci or spots, represent distinct optical coherence tomography (OCT) findings characterized by intraretinal or subretinal punctate hyperreflectivity (\nThe advent of optical coherence tomography angiography (OCTA) provides a critical technological bridge, enabling simultaneous non-invasive visualization of retinal vasculature and HRMs topography. Leveraging this dual imaging capability, our study systematically investigates: (1) spatial concordance between HRMs distribution and vascular abnormalities in DR. (2) HRMs-macular edema interactions across disease stages. (3) Metabolic correlates of HRMs burden, particularly in relation to glycemic control markers.\nThis is a retrospective, cross-sectional study performed on 105 patients diagnosed with DR by the Department of Ophthalmology of Shenzhen People's Hospital between January 2020 and May 2023. All the research and measurements were conducted in compliance with the tenets of the Declaration of Helsinki. The study was approved by the ethics committee of the hospital and informed consent was obtained from all individuals after a detailed discussion of the nature and possible consequences of the study procedures.\nAll participants underwent standardized multimodal ophthalmic evaluations including: slit-lamp biomicroscopy examination, Fundus Fluorescein Angiography (Topcon, TRC 50IA; Tokyo, Japan), OCTA (Zeiss, CIRRUS AngioPlex, Dublin, CA). Demographic and clinical factors including age, sex, history of hypertension, and serum lipid profile, glucose metabolism, leukocytes number, duration of diabetes and the grade of diabetic retinopathy were also reviewed. Inclusion criteria were as follows: (1) age above 18 years; (2) type 1 or 2 diabetes mellitus; and (3) the grade of DR was based on the international clinical diabetic retinopathy disease severity scale and DME was based on international clinical diabetic macular edema disease severity scale proposed by American Academy of Ophthalmology in 2019. Exclusion criteria were: (1) presence of any other retinal disorder such as retinal vein occlusion, retinal detachment, uveitis or any other maculopathy; (2) other ocular condition that compromises media opacities (i.e., vitreous hemorrhage or mature cataract); (3) previous treatment with any intraocular surgery, laser photocoagulation or intravitreal injection of anti-VEGF or corticosteroids drug; (4) clearly identify patients taking lipid-lowering drugs such as fenofibrate, statins et al.; and (5) poor quality OCTA images defined by the signal strength index of <5/10.\nMacular-centered 6 × 6 mm scan grids (350 × 350 A-scan density) were acquired using Cirrus 5000 Angioplex. Optical microangiography algorithms generated three-dimensional vascular maps through semi-automated segmentation (AngioPlex Metrix v10.0, an integrated analytical software suite for Cirrus 5000 Angioplex and available through two primary channels: directly via the OCT imaging system or through the ZEISS FORUM platform), which is comprise of structural OCT B-scans with flow overlays, superficial/deep vascular plexus en face projections and depth-resolved capillary density maps. Vascular layers were segmented using validated anatomical landmarks: the superficial vascular plexus was defined as extending from the internal limiting membrane to 9 μm above the inner plexiform layer-inner nuclear layer (IPL-INL) junction, and the deep vascular complex encompassed the region from 9 μm above the IPL-INL junction to 9 μm below the outer plexiform layer-outer nuclear layer (OPL-ONL) junction. Corresponding structural en face OCT images were analyzed using identical segmentation slabs to ensure anatomical correspondence. The walls of cystoid spaces are typically formed by adjacent parenchyma or thin, highly reflective lines resulting from optical property differences at the boundaries following hyporeflective fluid accumulation. The border of a serous retinal detachment is defined as the demarcation zone between the detached neurosensory retina and RPE, with underlying hyporeflective subretinal fluid causing an abrupt optical reflectivity transition. All automated segmentations underwent rigorous quality assessment: (1) B-scans with flow overlay were systematically reviewed to identify HRMs or hyperreflective spots exhibiting decorrelation signals; (2) segmentation accuracy was verified through dynamic B-scan navigation using orthogonal (horizontal/vertical) reference lines; and (3) manual corrections were applied when segmentation errors were detected, particularly at layer boundary transitions.\nFour distinct categories of pathological microvascular alterations detectable by OCT angiography (OCTA) were analyzed and their morphological classifications were established based on well-characterized criteria from prior validated studies (\nTwo independent masked retinal specialists (L.Z. and H.S.) performed qualitative and quantitative assessments of HRMs using registered B-scan and en face OCT/OCTA images. A senior retinal specialist (M.M.Y.) served as adjudicator for discordant cases to ensure consensus. Each HRMs was evaluated based on the following standardized criteria: (1) anatomical location [inner retina defined as layers between inner limiting membrane and inner nuclear layer, outer retina defined as layers below the inner boundary of the outer nuclear layer (\nInter-rater reliability was quantified via intraclass correlation coefficients (ICCs) with 95% confidence intervals. To examine the association between HRMs subtypes and retinal vascular pathology, we performed the following analyses: (1) the Kruskal–Wallis test was employed to compare HRMs counts across the four predefined categories of abnormal vascular morphology; (2)\nThe study cohort comprised 105 treatment-naïve diabetic retinopathy (DR) patients (69 males, 36 females; mean age 55.0 ± 8.1 years) undergoing standardized OCTA imaging, yielding 205 evaluable eyes (104 right, 101 left).\nBaseline demographics and ocular characteristics of the whole cohort of patients with hyperreflective materials detected on optical coherence tomography angiography.\nDR, diabetic retinopathy; NPDR, non-proliferative diabetic retinopathy; DME, diabetic macular edema; LogMAR, logarithm of the minimal angle of resolution.\nSix distinct HRMs subtypes were systematically classified through multimodal image analysis (\nCategory of hyperreflective materials based on characters including location, reflectivity, size and presence or absence of back shadowing, decorrelation signal on B-scan and structural en face images (6 × 6 mm scanning area) using OCTA. The scanning position of each image is identified by horizontal (lake blue) or vertical navigation (purple) line. IR is defined as layers between the inner limiting membrane and the OPL; OR is defined as the Henle NFL and outer nuclear layer.\nAssociation of hyperreflective materials distribution with vascular abnormalities including no perfusion area, neovascular membrane and microneurysm.\nQuantitative analysis revealed distinct topographic distributions of hyperreflective materials (HRMs) across four retinal vascular abnormalities visualized by OCTA, with excellent intergrader reliability (\nDistribution of HRMs in the retinal vascular map of optical coherence tomography angiography.\nHRMs, hyperreflective materials; IRHFs, inner retinal hyperreflective spots; ORHFs, outer retinal hyperreflective spots; IRHE, inner retinal hard exudates; ORHE, outer retinal hard exudates; NP, no-perfusion; IRMA, intra-retinal microvascular abnormalities.\nDistribution of hyperreflective materials in the area of intra-retinal microvascular abnormalities (IRMA) detected by OCTA. The scanning position of each image is identified by horizontal (lake blue) or vertical navigation (purple) line.\nTo investigate the potential role of HRMs in diabetic macular edema (DME), we stratified patients with DR into two groups: those with DME and those without DME. Quantitative analysis revealed significantly higher HRMs counts in DME eyes across all vascular abnormalities (all\nComparison of hyperreflective materials number between DR with DME and DR without DME.\nHRMs, hyperreflective materials; IRHFs, inner retinal hyperreflective spots; ORHFs, outer retinal hyperreflective spots; IRHE, inner retinal hard exudates; ORHE, outer retinal hard exudates; NP, no-perfusion; IRMA, intra-retinal microvascular abnormalities, DR, diabetic retinopathy; DME, diabetic macular edema. Bold values indicate statistically and clinically significant data (\nOur generalized estimating equation (GEE) analysis presented in\nCorrelation of systemic metabolic traits with HRMs.\nHRMs, hyperreflective materials; IRHFs, inner retinal hyperreflective spots; ORHFs, outer retinal hyperreflective spots; IRHE, inner retinal hard exudates; ORHE, outer retinal hard exudates; DM, diabetes mellitus; DR, diabetic retinopathy; FBG, fasting blood glucose; TC, total cholesterol; TG, triglyceride; HDL, high-density lipoprotein; LDL, low-density lipoprotein; apolipoprotein a and b, APO a and APO b; WBC, white blood cell; UCR, urinary albumin-creatinine ration; CI, confidence interval. Bold values indicate statistically and clinically significant data (\nThis study systematically investigated the characteristics and spatial distribution of hyperreflective materials (HRMs) in diabetic retinopathy (DR) using OCT angiography (OCTA), while exploring their associations with vascular abnormalities and systemic metabolic dysregulation. By integrating morphological criteria (location, reflectivity, size, back-shadowing, and decorrelation signals) with OCTA features, we identified six distinct HRMs subtypes: inner/outer retinal hyperreflective foci (IRHFs/ORHFs), inner/outer retinal hard exudates (IRHE/ORHE), decorrelation-positive HRMs, and cotton-wool spots. Notably, 75%−94% of HRMs localized to regions of IRMA or microaneurysms, with pronounced enrichment in diabetic macular edema cases. Our findings further linked HRMs prevalence to systemic lipid dysregulation, suggesting their dual origin in blood-retinal barrier (BRB) disruption and metabolic dysfunction.\nThe conceptualization of intraretinal HRMs—termed hyperreflective dots (HRDs) or foci in prior studies—has undergone substantial refinement since their initial characterization by Bolz et al. (\nPrior classification frameworks categorized hyperreflective materials (HRMs) into three subtypes based on OCT-derived morphometrics: microglial-like foci (<30 μm, moderate reflectivity), hard exudates (>30 μm, RPE-comparable reflectivity with shadowing), and microaneurysm-associated deposits (>30 μm, inner retinal localization) (\nThe ontogeny and pathobiological significance of HRMs remain contentious, with three predominant mechanistic hypotheses emerging from prior research. One of the main hypotheses is that HRMs are considered as aggregation of activated microglia, therefore always are proposed as a biomarker of neuro-retinal inflammation (\nThe ultimate aim of studying the HRMs is to direct the individualized treatment of DR. So far, anti-VEGF agents, intravitreal steroid injections or implants, laser and vitrectomy are all considered as the popular therapy (\nThis work has several constraints: (1) cross-sectional design precludes assessment of HRMs evolution or causality; (2) single-timepoint metabolic measures may not capture dynamic interactions with retinal pathology; (3) unmeasured confounders (genetic/epigenetic factors, metabolic memory effects) could influence observed associations. Future studies integrating serial OCTA with multi-omics profiling will clarify HRMs pathophysiology and therapeutic relevance; and (4) the classification of HRMs relies on manual annotation, and although calibrated by expert consensus, there may be interpretation bias particularly for small lesions with poorly defined margins (e.g., intraretinal hyperreflective foci [IRHFs] ≤ 30 μm). Future investigations incorporating serial OCTA imaging with artificial intelligence (AI) image recognition technology will be essential to elucidate the pathophysiological mechanisms and therapeutic implications of HRMs.\nThis study establishes a novel OCTA-guided classification system for HRMs in DR, delineating their spatial association with vascular abnormalities (particularly IRMA and microaneurysms) and systemic lipid dysregulation. The dual pathogenesis of HRMs—rooted in blood-retinal barrier disruption and metabolic dysfunction—highlights their potential as biomarkers for disease staging and therapeutic targeting. Our findings advocate for integrating lipid-lowering strategies with conventional DME therapies, proposing a paradigm shift toward personalized interventions that address both vascular and metabolic drivers of DR progression.", "topic": "Brain"}
{"pmid": "39874276", "pmcid": "12307413", "title": "A bibliometric analysis of synaptic plasticity and epilepsy from 2003 to 2023", "publication_year": "N/A", "abstract": "", "full_text": "Epilepsy is a devastating and complex neurological disease that impacts approximately 70 million individuals globally (\nEpilepsy frequently causes cognitive deficits, particularly in learning and memory, with synaptic plasticity serving as the foundational structure for these functions (\nBibliometric analysis serves as a crucial statistical technique for quantitatively examining extensive and diverse sets of publications (\nIn the last 20 years, numerous studies have demonstrated the link between synaptic plasticity and epilepsy. However, to the best of our knowledge, there has not been a bibliometric analysis on this topic. To address this gap, this study utilized the Web of Science Core Collection (WoSCC). We retrieved bibliometric data (annual articles, countries/regions, authors, institutions, disciplines, journals, references, and keywords) for each synaptic plasticity and epilepsy research field and performed descriptive statistics. This article provides a comprehensive overview of the research landscape on synaptic plasticity and epilepsy from 2003 to 2023, utilizing CiteSpace and VOSviewer to create knowledge maps that systematically analyze publication trends, citation patterns, and authorship networks. This study aims to identify key research areas, emerging trends, and influential contributions in the field.\nWe conducted a comprehensive search of research published between January 1, 2003, and December 31, 2023, covering relevant literature within this period from the Science Citation Index Expanded (SCI-Expanded) in the Web of Science Core Collection (WoSCC). The search criteria were as follows: TS = (“Epilepsy” OR “Seizure” OR “Convulsion” OR “Epileptic”) AND TS = (“synaptic plasticity” OR “synapse plasticity”). To ensure a thorough analysis, only peer-reviewed English-language articles were considered. After a detailed review of all the documents, a total of 1,369 valid references were identified (see\nBibliometric data for the review and original research were analyzed separately using VOSviewer (version 1.6.20) and CiteSpace (version 6.1. R3).\nA total of 1,369 publications, comprising 1,060 articles and 309 reviews, met the inclusion criteria. The annual distribution of review and original research publications on synaptic plasticity in epilepsy is illustrated in\nTo better understand the international distribution of academic output, the dataset was reanalyzed by categorizing the publications into two types: reviews and original research articles. Publication counts were then assessed separately for each category. The leading ten nations with the highest publication output were evaluated according to the total number of works produced by all contributors. The number of publications was indicated by the size of the circles, so countries with a higher volume of articles typically had larger circles. Connections between the two nations have led to the joint publication of articles.\nThe United States published the highest number of both review and original research articles, with 120 and 405 publications, respectively. Italy and the United Kingdom ranked second and third in terms of review article output, contributing 39 and 20 publications, respectively. China was the second-largest contributor of original research articles, with 151 publications, followed by Germany with 129 (\nLeading ten nations with the highest volume of review publications on synaptic plasticity in epilepsy.\nLeading ten nations with the highest volume of original research publications on synaptic plasticity in epilepsy.\nCooperation map of countries/regions on synaptic plasticity in epilepsy.\nJohns Hopkins University ranked first in the number of review articles published, with a total of 8 papers, followed by the Russian Academy of Sciences and the University of Genoa, each with 6 review articles (\nLeading ten institutions with the highest volume of review publications on synaptic plasticity in epilepsy.\nLeading ten institutions with the highest volume of original research publications on synaptic plasticity in epilepsy.\nThe leading researchers of review publications on synaptic plasticity in epilepsy.\nThe leading researchers in the area of original research publications on synaptic plasticity in epilepsy.\nA co-authorship network was visualized for the top 45 authors of review articles by including those with at least two published reviews and merging author name variants using VOSviewer. Among these 45 authors, 19 had no connections with others in the network. The largest group, shown in red, consisted of 4 authors (\nDistribution of authors.\nSimilarly, a co-authorship network was constructed for the top 42 authors of original research articles, including those with at least five published articles and merging synonyms in VOSviewer. Among these 42 authors, 7 had no connections with other authors. The largest group, represented in red, comprised 12 authors and formed the most substantial cluster, demonstrating strong collaborative potential with a total of 86 publications (\nBibliometric analysis of reviews identified 159 distinct journals publishing review articles related to synaptic plasticity in epilepsy.\nLists the leading 10 primary journals of review publications on synaptic plasticity in epilepsy.\nBibliometric analysis of original research identified 200 distinct journals publishing original research articles related to synaptic plasticity in epilepsy.\nLists the leading 10 primary journals of original research publications on synaptic plasticity in epilepsy.\nA co-citation map of review articles was generated using VOSviewer. In the subsequent citation analysis, a total of 60 references were identified.\nSubsequently, a co-citation map of original research articles was created. In this citation analysis, a total of 86 references were identified.\nFor the review articles, we selected 78 journals for a co-citation analysis. The results yielded a network map comprising three distinct clusters (\nFrom among the original research articles we selected 98 journals for a co-citation analysis. This study generated a network map comprising four distinct clusters (\nA total of 101 keywords were extracted from the 309 review articles. Only 34.7% of these keywords appeared more than 10 times, while a large proportion (approximately 25.7%) appeared only five times. This stark contrast indicates that only a small subset of keywords were used frequently. Based on the network visualization of the 101 most frequently used keywords, six distinct clusters were identified (\nAll keywords that experienced citation bursts first appeared in 2003 (\nThe 20 most frequent keywords of review publications on synaptic plasticity in epilepsy.\nFrom the 1,060 original research articles, 65 keywords were extracted. Only 13.9% of these keywords appeared more than 14 times, whereas a large portion (approximately 38.7%) appeared only twice. This again illustrates that only a limited number of keywords were frequently used. Based on the network visualization of the 65 most common keywords, five distinct clusters were identified (\nAll keywords with citation bursts also initially appeared in 2003 (\nThe 20 most frequent keywords in studies of original research publications on synaptic plasticity in epilepsy.\nThis research gathered studies from the WoSCC database that concentrate on studying synaptic plasticity in the context of epilepsy. A total of 1,369 references from 587 journals were included and analyzed. In American journals, numerous research papers have been published and cited simultaneously. Over the last 20 years, bibliometric analysis of studies on synaptic plasticity in epilepsy revealed a steady rise in publications, suggesting growing interest in this area.\nA bibliometric study on synaptic plasticity’s involvement in epilepsy research over the last 20 years revealed a steady rise in the number of articles published, suggesting growing interest in this area. The findings indicate that the primary contributors in this area are the United States, China, Italy, and Germany (\nWe evaluated the impact of authors in the domain by ranking them according to their publication count and overall citations. Data from WoSCC revealed that Benfenati Fabio secured the top position with the highest citation count, followed by Valtorta Flavia and Reddy Doodipala Samba. According to the number of articles published, Wang Xuefeng ranked first with 19 articles.\nExamining well-known publications can offer scientists a clear path for investigation in this field of study. Our research revealed that the Journal of Neuroscience has the highest number of publications and leads in citation rankings. Researchers can easily identify appropriate journals for their papers by referring to the journals’ rankings.\nBy examining the chronological distribution of relevant references and the sudden increase in keyword citations, we can identify key areas, emerging trends, and developments within this scientific research field. A comparison of burst keywords from review articles and original research revealed a notable pattern: certain keywords, such as “long-term potentiation” and “astrocytes,” showed citation bursts in review articles as early as 2003 or earlier, whereas in original research, these same keywords did not exhibit bursts until 2020.\nThis discrepancy may be attributed to the nature of review articles, which are fundamentally designed to summarize, synthesize, and anticipate research trends based on existing studies. For example, although empirical research on LTP was still limited in the early stages, its potential significance as a core mechanism in synaptic plasticity may have already been highlighted and disseminated by expert authors through review publications. Moreover, high-impact reviews are often authored by leading experts or well-established research teams in the field, who typically possess a forward-looking academic vision. These authors are capable of promoting novel concepts through reviews even before such ideas are widely accepted or substantiated by experimental data. In addition, early-stage empirical studies on LTP may have been constrained by technical limitations—such as challenges in electrophysiology or optogenetics—which made direct investigation difficult. Only with the advancement of experimental tools (e.g., refined methods for precisely monitoring synaptic activity in the hippocampus) in recent years did a substantial volume of research emerge, resulting in the delayed burst of these keywords in original research literature.\nAn intriguing finding of this bibliometric analysis is the emergence of “Alzheimer’s disease” as a burst keyword in review articles related to synaptic plasticity and epilepsy. Although Alzheimer’s disease (AD) is not traditionally categorized within the domain of epilepsy research, its appearance as a frequently cited term highlights a growing interdisciplinary convergence between neurodegenerative and epileptic disorders. One plausible explanation lies in the shared pathophysiological mechanisms between the two conditions. Both epilepsy and AD are closely linked to disruptions in synaptic plasticity, particularly in hippocampal circuits. (\nAn important trend emerging from our bibliometric analysis is the increasing convergence of both review articles and original research on the molecular underpinnings of synaptic plasticity, particularly those involving neurotransmitters and their receptors. This is evidenced by the recent appearance of “glutamate” and “glutamate receptors” as burst keywords in review articles, and “NMDA” and “NMDA receptors” in original research articles. This pattern suggests a strategic shift in research emphasis. While earlier phases of epilepsy and synaptic plasticity research may have been dominated by descriptive neuroanatomy, electrophysiological observations, or systems-level models, the current focus is increasingly aligned with molecular and receptor-level mechanisms. Glutamate, as the principal excitatory neurotransmitter in the central nervous system, and NMDA receptors, as its critical mediators in synaptic plasticity and excitotoxicity, have emerged as central research targets (\nIn review literature, the burst of keywords like “glutamate” reflects a thematic consolidation, where scholars synthesize emerging data into neurochemical frameworks that connect synaptic plasticity with disease states such as epilepsy and neurodegeneration. These reviews often serve to highlight the relevance of neurotransmitter systems and predict future research directions. In original research, the shift toward specific receptor subtypes—particularly NMDA receptors—indicates that experimental investigations are now probing deeper into the molecular cascades involved in LTP, seizure initiation, and synaptic remodeling. The increasing frequency of terms like “NMDA” and “NMDA receptors” reflects not only technological advancements but also a growing emphasis on precision neuroscience, where identifying therapeutic targets at the receptor level is a priority. These findings point to a unified thematic evolution: both review and original research are progressively gravitating toward neurotransmitter signaling and receptor dynamics as core mechanisms underlying synaptic plasticity and epilepsy. This convergence may signal a maturation of the field, where interdisciplinary approaches—from cellular neurobiology to systems pharmacology—are increasingly integrated around common molecular denominators.\nHebbian and homeostatic plasticity are two fundamental mechanisms that play crucial roles in the regulation of neuronal excitability and synaptic strength. These processes are particularly relevant in the context of epilepsy. In epilepsy, the balance between excitatory and inhibitory signals in the brain is often disrupted, leading to excessive neuronal firing (\nHebbian plasticity, often summarized by the phrase “cells that fire together wire together,” involves the strengthening of synapses based on the correlation of pre- and postsynaptic activity. This form of plasticity can lead to increased excitability in neuronal networks, potentially contributing to the hyperexcitability observed in epileptic seizures. On the other hand, homeostatic plasticity acts as a stabilizing force, adjusting the intrinsic excitability of neurons to maintain overall network stability. This mechanism can counteract the effects of Hebbian plasticity by reducing synaptic strength or increasing inhibitory inputs when neuronal activity becomes too high (\nResearch has shown that homeostatic plasticity can manifest through various mechanisms, such as the regulation of ion channel expression and function. For example, in the striatum, dopamine depletion leads to an increase in the intrinsic excitability of medium spiny neurons through the modulation of A-type potassium currents, representing a form of homeostatic plasticity that compensates for synaptic perturbations (\nIn the context of epilepsy, these plasticity mechanisms may become dysregulated, leading to persistent changes in neuronal excitability and synaptic connectivity. Understanding the interplay between Hebbian and homeostatic plasticity in epilepsy could pave the way for novel therapeutic approaches aimed at restoring the balance of excitatory and inhibitory signals in the brain, potentially reducing seizure frequency and severity.\nThis study offers several unique advantages. Initially, it provides the inaugural systematic review of synaptic plasticity studies in epilepsy through bibliometric methods, delivering extensive insights for researchers focused on this area. Secondly, we utilized two bibliometric instruments concurrently for the study, such as VOSviewer and CiteSpace, which are well-known in the bibliometric domain, to guarantee an impartial data analysis procedure. In conclusion, bibliometric analysis provides deeper understanding of key areas and emerging trends than conventional reviews.\nNaturally, this study has some limitations. First, it depends exclusively on information from the WoSCC database, which might miss pertinent research available in other databases. Second, we focused exclusively on studies published in English, thereby excluding potentially significant articles in other languages. Additionally, our study focused solely on two types of documents—reviews and articles—excluding other forms of publications such as books or conference papers from our bibliometric analysis.\nThis research offers insights into synaptic plasticity in epilepsy through visualization and bibliometric analysis. We examined the key areas of publication trends and emerging research topics. Regarding publication patterns, studies on synaptic plasticity related to epilepsy have shown a consistent rise. At present, the focus and emerging areas of review articles have shifted from long-term potentiation, astrocytes, and depression to gamma-aminobutyric acid, amyloid beta peptide, and glutamate receptors. Meanwhile, the focus and emerging areas of original research have shifted from synaptic reorganization, dentate granule cells, and messenger RNA to astrocytes, NMDA receptors, and long-term potentiation.", "content_for_embedding": "Epilepsy is a devastating and complex neurological disease that impacts approximately 70 million individuals globally (\nEpilepsy frequently causes cognitive deficits, particularly in learning and memory, with synaptic plasticity serving as the foundational structure for these functions (\nBibliometric analysis serves as a crucial statistical technique for quantitatively examining extensive and diverse sets of publications (\nIn the last 20 years, numerous studies have demonstrated the link between synaptic plasticity and epilepsy. However, to the best of our knowledge, there has not been a bibliometric analysis on this topic. To address this gap, this study utilized the Web of Science Core Collection (WoSCC). We retrieved bibliometric data (annual articles, countries/regions, authors, institutions, disciplines, journals, references, and keywords) for each synaptic plasticity and epilepsy research field and performed descriptive statistics. This article provides a comprehensive overview of the research landscape on synaptic plasticity and epilepsy from 2003 to 2023, utilizing CiteSpace and VOSviewer to create knowledge maps that systematically analyze publication trends, citation patterns, and authorship networks. This study aims to identify key research areas, emerging trends, and influential contributions in the field.\nWe conducted a comprehensive search of research published between January 1, 2003, and December 31, 2023, covering relevant literature within this period from the Science Citation Index Expanded (SCI-Expanded) in the Web of Science Core Collection (WoSCC). The search criteria were as follows: TS = (“Epilepsy” OR “Seizure” OR “Convulsion” OR “Epileptic”) AND TS = (“synaptic plasticity” OR “synapse plasticity”). To ensure a thorough analysis, only peer-reviewed English-language articles were considered. After a detailed review of all the documents, a total of 1,369 valid references were identified (see\nBibliometric data for the review and original research were analyzed separately using VOSviewer (version 1.6.20) and CiteSpace (version 6.1. R3).\nA total of 1,369 publications, comprising 1,060 articles and 309 reviews, met the inclusion criteria. The annual distribution of review and original research publications on synaptic plasticity in epilepsy is illustrated in\nTo better understand the international distribution of academic output, the dataset was reanalyzed by categorizing the publications into two types: reviews and original research articles. Publication counts were then assessed separately for each category. The leading ten nations with the highest publication output were evaluated according to the total number of works produced by all contributors. The number of publications was indicated by the size of the circles, so countries with a higher volume of articles typically had larger circles. Connections between the two nations have led to the joint publication of articles.\nThe United States published the highest number of both review and original research articles, with 120 and 405 publications, respectively. Italy and the United Kingdom ranked second and third in terms of review article output, contributing 39 and 20 publications, respectively. China was the second-largest contributor of original research articles, with 151 publications, followed by Germany with 129 (\nLeading ten nations with the highest volume of review publications on synaptic plasticity in epilepsy.\nLeading ten nations with the highest volume of original research publications on synaptic plasticity in epilepsy.\nCooperation map of countries/regions on synaptic plasticity in epilepsy.\nJohns Hopkins University ranked first in the number of review articles published, with a total of 8 papers, followed by the Russian Academy of Sciences and the University of Genoa, each with 6 review articles (\nLeading ten institutions with the highest volume of review publications on synaptic plasticity in epilepsy.\nLeading ten institutions with the highest volume of original research publications on synaptic plasticity in epilepsy.\nThe leading researchers of review publications on synaptic plasticity in epilepsy.\nThe leading researchers in the area of original research publications on synaptic plasticity in epilepsy.\nA co-authorship network was visualized for the top 45 authors of review articles by including those with at least two published reviews and merging author name variants using VOSviewer. Among these 45 authors, 19 had no connections with others in the network. The largest group, shown in red, consisted of 4 authors (\nDistribution of authors.\nSimilarly, a co-authorship network was constructed for the top 42 authors of original research articles, including those with at least five published articles and merging synonyms in VOSviewer. Among these 42 authors, 7 had no connections with other authors. The largest group, represented in red, comprised 12 authors and formed the most substantial cluster, demonstrating strong collaborative potential with a total of 86 publications (\nBibliometric analysis of reviews identified 159 distinct journals publishing review articles related to synaptic plasticity in epilepsy.\nLists the leading 10 primary journals of review publications on synaptic plasticity in epilepsy.\nBibliometric analysis of original research identified 200 distinct journals publishing original research articles related to synaptic plasticity in epilepsy.\nLists the leading 10 primary journals of original research publications on synaptic plasticity in epilepsy.\nA co-citation map of review articles was generated using VOSviewer. In the subsequent citation analysis, a total of 60 references were identified.\nSubsequently, a co-citation map of original research articles was created. In this citation analysis, a total of 86 references were identified.\nFor the review articles, we selected 78 journals for a co-citation analysis. The results yielded a network map comprising three distinct clusters (\nFrom among the original research articles we selected 98 journals for a co-citation analysis. This study generated a network map comprising four distinct clusters (\nA total of 101 keywords were extracted from the 309 review articles. Only 34.7% of these keywords appeared more than 10 times, while a large proportion (approximately 25.7%) appeared only five times. This stark contrast indicates that only a small subset of keywords were used frequently. Based on the network visualization of the 101 most frequently used keywords, six distinct clusters were identified (\nAll keywords that experienced citation bursts first appeared in 2003 (\nThe 20 most frequent keywords of review publications on synaptic plasticity in epilepsy.\nFrom the 1,060 original research articles, 65 keywords were extracted. Only 13.9% of these keywords appeared more than 14 times, whereas a large portion (approximately 38.7%) appeared only twice. This again illustrates that only a limited number of keywords were frequently used. Based on the network visualization of the 65 most common keywords, five distinct clusters were identified (\nAll keywords with citation bursts also initially appeared in 2003 (\nThe 20 most frequent keywords in studies of original research publications on synaptic plasticity in epilepsy.\nThis research gathered studies from the WoSCC database that concentrate on studying synaptic plasticity in the context of epilepsy. A total of 1,369 references from 587 journals were included and analyzed. In American journals, numerous research papers have been published and cited simultaneously. Over the last 20 years, bibliometric analysis of studies on synaptic plasticity in epilepsy revealed a steady rise in publications, suggesting growing interest in this area.\nA bibliometric study on synaptic plasticity’s involvement in epilepsy research over the last 20 years revealed a steady rise in the number of articles published, suggesting growing interest in this area. The findings indicate that the primary contributors in this area are the United States, China, Italy, and Germany (\nWe evaluated the impact of authors in the domain by ranking them according to their publication count and overall citations. Data from WoSCC revealed that Benfenati Fabio secured the top position with the highest citation count, followed by Valtorta Flavia and Reddy Doodipala Samba. According to the number of articles published, Wang Xuefeng ranked first with 19 articles.\nExamining well-known publications can offer scientists a clear path for investigation in this field of study. Our research revealed that the Journal of Neuroscience has the highest number of publications and leads in citation rankings. Researchers can easily identify appropriate journals for their papers by referring to the journals’ rankings.\nBy examining the chronological distribution of relevant references and the sudden increase in keyword citations, we can identify key areas, emerging trends, and developments within this scientific research field. A comparison of burst keywords from review articles and original research revealed a notable pattern: certain keywords, such as “long-term potentiation” and “astrocytes,” showed citation bursts in review articles as early as 2003 or earlier, whereas in original research, these same keywords did not exhibit bursts until 2020.\nThis discrepancy may be attributed to the nature of review articles, which are fundamentally designed to summarize, synthesize, and anticipate research trends based on existing studies. For example, although empirical research on LTP was still limited in the early stages, its potential significance as a core mechanism in synaptic plasticity may have already been highlighted and disseminated by expert authors through review publications. Moreover, high-impact reviews are often authored by leading experts or well-established research teams in the field, who typically possess a forward-looking academic vision. These authors are capable of promoting novel concepts through reviews even before such ideas are widely accepted or substantiated by experimental data. In addition, early-stage empirical studies on LTP may have been constrained by technical limitations—such as challenges in electrophysiology or optogenetics—which made direct investigation difficult. Only with the advancement of experimental tools (e.g., refined methods for precisely monitoring synaptic activity in the hippocampus) in recent years did a substantial volume of research emerge, resulting in the delayed burst of these keywords in original research literature.\nAn intriguing finding of this bibliometric analysis is the emergence of “Alzheimer’s disease” as a burst keyword in review articles related to synaptic plasticity and epilepsy. Although Alzheimer’s disease (AD) is not traditionally categorized within the domain of epilepsy research, its appearance as a frequently cited term highlights a growing interdisciplinary convergence between neurodegenerative and epileptic disorders. One plausible explanation lies in the shared pathophysiological mechanisms between the two conditions. Both epilepsy and AD are closely linked to disruptions in synaptic plasticity, particularly in hippocampal circuits. (\nAn important trend emerging from our bibliometric analysis is the increasing convergence of both review articles and original research on the molecular underpinnings of synaptic plasticity, particularly those involving neurotransmitters and their receptors. This is evidenced by the recent appearance of “glutamate” and “glutamate receptors” as burst keywords in review articles, and “NMDA” and “NMDA receptors” in original research articles. This pattern suggests a strategic shift in research emphasis. While earlier phases of epilepsy and synaptic plasticity research may have been dominated by descriptive neuroanatomy, electrophysiological observations, or systems-level models, the current focus is increasingly aligned with molecular and receptor-level mechanisms. Glutamate, as the principal excitatory neurotransmitter in the central nervous system, and NMDA receptors, as its critical mediators in synaptic plasticity and excitotoxicity, have emerged as central research targets (\nIn review literature, the burst of keywords like “glutamate” reflects a thematic consolidation, where scholars synthesize emerging data into neurochemical frameworks that connect synaptic plasticity with disease states such as epilepsy and neurodegeneration. These reviews often serve to highlight the relevance of neurotransmitter systems and predict future research directions. In original research, the shift toward specific receptor subtypes—particularly NMDA receptors—indicates that experimental investigations are now probing deeper into the molecular cascades involved in LTP, seizure initiation, and synaptic remodeling. The increasing frequency of terms like “NMDA” and “NMDA receptors” reflects not only technological advancements but also a growing emphasis on precision neuroscience, where identifying therapeutic targets at the receptor level is a priority. These findings point to a unified thematic evolution: both review and original research are progressively gravitating toward neurotransmitter signaling and receptor dynamics as core mechanisms underlying synaptic plasticity and epilepsy. This convergence may signal a maturation of the field, where interdisciplinary approaches—from cellular neurobiology to systems pharmacology—are increasingly integrated around common molecular denominators.\nHebbian and homeostatic plasticity are two fundamental mechanisms that play crucial roles in the regulation of neuronal excitability and synaptic strength. These processes are particularly relevant in the context of epilepsy. In epilepsy, the balance between excitatory and inhibitory signals in the brain is often disrupted, leading to excessive neuronal firing (\nHebbian plasticity, often summarized by the phrase “cells that fire together wire together,” involves the strengthening of synapses based on the correlation of pre- and postsynaptic activity. This form of plasticity can lead to increased excitability in neuronal networks, potentially contributing to the hyperexcitability observed in epileptic seizures. On the other hand, homeostatic plasticity acts as a stabilizing force, adjusting the intrinsic excitability of neurons to maintain overall network stability. This mechanism can counteract the effects of Hebbian plasticity by reducing synaptic strength or increasing inhibitory inputs when neuronal activity becomes too high (\nResearch has shown that homeostatic plasticity can manifest through various mechanisms, such as the regulation of ion channel expression and function. For example, in the striatum, dopamine depletion leads to an increase in the intrinsic excitability of medium spiny neurons through the modulation of A-type potassium currents, representing a form of homeostatic plasticity that compensates for synaptic perturbations (\nIn the context of epilepsy, these plasticity mechanisms may become dysregulated, leading to persistent changes in neuronal excitability and synaptic connectivity. Understanding the interplay between Hebbian and homeostatic plasticity in epilepsy could pave the way for novel therapeutic approaches aimed at restoring the balance of excitatory and inhibitory signals in the brain, potentially reducing seizure frequency and severity.\nThis study offers several unique advantages. Initially, it provides the inaugural systematic review of synaptic plasticity studies in epilepsy through bibliometric methods, delivering extensive insights for researchers focused on this area. Secondly, we utilized two bibliometric instruments concurrently for the study, such as VOSviewer and CiteSpace, which are well-known in the bibliometric domain, to guarantee an impartial data analysis procedure. In conclusion, bibliometric analysis provides deeper understanding of key areas and emerging trends than conventional reviews.\nNaturally, this study has some limitations. First, it depends exclusively on information from the WoSCC database, which might miss pertinent research available in other databases. Second, we focused exclusively on studies published in English, thereby excluding potentially significant articles in other languages. Additionally, our study focused solely on two types of documents—reviews and articles—excluding other forms of publications such as books or conference papers from our bibliometric analysis.\nThis research offers insights into synaptic plasticity in epilepsy through visualization and bibliometric analysis. We examined the key areas of publication trends and emerging research topics. Regarding publication patterns, studies on synaptic plasticity related to epilepsy have shown a consistent rise. At present, the focus and emerging areas of review articles have shifted from long-term potentiation, astrocytes, and depression to gamma-aminobutyric acid, amyloid beta peptide, and glutamate receptors. Meanwhile, the focus and emerging areas of original research have shifted from synaptic reorganization, dentate granule cells, and messenger RNA to astrocytes, NMDA receptors, and long-term potentiation.", "topic": "Brain"}
{"pmid": "39819873", "pmcid": "12304901", "title": "Representational drift and learning-induced stabilization in the piriform cortex", "publication_year": "2025", "abstract": "The neural response to a specific stimulus often evolves over time in a phenomenon known as representational drift (RD). However, the underlying mechanisms driving this widespread phenomenon remain poorly understood. By employing a realistic neural network model to investigate RD in the piriform cortex, we demonstrate that the experimentally observed RD can be quantitatively attributed to slow spontaneous fluctuations in synaptic weights. Furthermore, our model shows that a fast-learning process drives neural activity toward a lower-dimensional representational manifold, effectively reducing the dimensionality of the diffusive RD and thus reducing RD as seen in recent experiments. In addition to quantitatively explaining recent experiments in the piriform cortex, our slow-fluctuation-fast-learning model offers a comprehensive framework for understanding representation dynamics in the brain.\nThe brain encodes external stimuli through patterns of neural activity, forming internal representations of the world. Increasing experimental evidence showed that neural representations for a specific stimulus can change over time in a phenomenon called “representational drift” (RD). However, the underlying mechanisms for this widespread phenomenon remain poorly understood. Here, we study RD in the piriform cortex of the olfactory system with a realistic neural network model that incorporates two general mechanisms for synaptic weight dynamics operating at two well-separated timescales: spontaneous multiplicative fluctuations on a scale of days and spike-timing-dependent plasticity (STDP) effects on a scale of seconds. We show that the slow multiplicative fluctuations in synaptic sizes, which lead to a steady-state distribution of synaptic weights consistent with experiments, can induce RD effects that are in quantitative agreement with recent empirical evidence. Furthermore, our model reveals that the fast STDP learning dynamics during presentation of a given odor drives the system toward a low-dimensional representational manifold, which effectively reduces the dimensionality of synaptic weight fluctuations and thus suppresses RD. Specifically, our model explains why representations of already “learned” odors drift slower than unfamiliar ones, as well as the dependence of the drift rate with the frequency of stimulus presentation—both of which align with recent experimental data. The proposed model not only offers a simple explanation for the emergence of RD and its relation to learning in the piriform cortex, but also provides a general theoretical framework for studying representation dynamics in other neural systems.", "full_text": "", "content_for_embedding": "The neural response to a specific stimulus often evolves over time in a phenomenon known as representational drift (RD). However, the underlying mechanisms driving this widespread phenomenon remain poorly understood. By employing a realistic neural network model to investigate RD in the piriform cortex, we demonstrate that the experimentally observed RD can be quantitatively attributed to slow spontaneous fluctuations in synaptic weights. Furthermore, our model shows that a fast-learning process drives neural activity toward a lower-dimensional representational manifold, effectively reducing the dimensionality of the diffusive RD and thus reducing RD as seen in recent experiments. In addition to quantitatively explaining recent experiments in the piriform cortex, our slow-fluctuation-fast-learning model offers a comprehensive framework for understanding representation dynamics in the brain.\nThe brain encodes external stimuli through patterns of neural activity, forming internal representations of the world. Increasing experimental evidence showed that neural representations for a specific stimulus can change over time in a phenomenon called “representational drift” (RD). However, the underlying mechanisms for this widespread phenomenon remain poorly understood. Here, we study RD in the piriform cortex of the olfactory system with a realistic neural network model that incorporates two general mechanisms for synaptic weight dynamics operating at two well-separated timescales: spontaneous multiplicative fluctuations on a scale of days and spike-timing-dependent plasticity (STDP) effects on a scale of seconds. We show that the slow multiplicative fluctuations in synaptic sizes, which lead to a steady-state distribution of synaptic weights consistent with experiments, can induce RD effects that are in quantitative agreement with recent empirical evidence. Furthermore, our model reveals that the fast STDP learning dynamics during presentation of a given odor drives the system toward a low-dimensional representational manifold, which effectively reduces the dimensionality of synaptic weight fluctuations and thus suppresses RD. Specifically, our model explains why representations of already “learned” odors drift slower than unfamiliar ones, as well as the dependence of the drift rate with the frequency of stimulus presentation—both of which align with recent experimental data. The proposed model not only offers a simple explanation for the emergence of RD and its relation to learning in the piriform cortex, but also provides a general theoretical framework for studying representation dynamics in other neural systems.", "topic": "Brain"}
{"pmid": "39761735", "pmcid": "12308846", "title": "Circular causality in volition", "publication_year": "N/A", "abstract": "Conventional scientific paradigms predominantly emphasize upward causality, often overlooking or dismissing the role of downward causality. This approach is also prevalent in neuroscience, where cortical neurodynamics and higher cognitive functions are typically viewed as consequences of neuronal or even ion channel activity. Conversely, mental phenomena are generally assumed to lack causal efficacy over neural processes—an assumption that is increasingly being questioned. The causality associated with volition may be analyzed at three organizational levels: (1) neuronal interactions within cortical networks, (2) interregional dynamics between distinct brain areas, and (3) the reciprocal relationship between the nervous system and its environmental context. Across all these domains, circular rather than strictly linear causality appears to be at play. This paper examines the implications of such circular causality for volition and the longstanding problem of free will, with particular reference to insights derived from neurocomputational modeling.", "full_text": "A central challenge in the study of\nFree will, commonly defined as the capacity to make choices not strictly determined by prior causes, remains a core issue in\nWe conceptualize the brain as a complex system, with activity reflected in complex neurodynamics, including oscillations and fluctuations observable in EEG recordings. These dynamics result from interactions among multiple organizational levels, from ion channels and neurons to networks, each governed by distinct time scales and regulatory mechanisms. Also input from the environment affect the brain dynamics in ways hard to control.\nComplex systems typically exhibit non-linear interdependencies, thresholds, and feedback loops, which contribute to unexpected or non-intuitive behaviors. These properties demand a multiscale perspective to properly capture causal interactions (\nHermann Haken’s concept of\nHaken applied this approach to the human brain-mind system (\nIn addressing volition, we aim to clarify the intricate causal relationships within brain dynamics, as determined by network physiology and connectivity. Neural activity is typically assumed to cause mental phenomena (upward causation), while the reverse—mental states influencing neural processes (downward causation)—is often neglected, with some insightful exceptions (see\nOur neurocomputational models simulate various cortical regions, examining how their neurodynamics depend on structural factors such as connectivity and cell types, as well as intrinsic and external signals. We also explore how the complex neurodynamics may influence neuronal populations and relate to mental functions. This is particularly relevant for the\nThe action-perception cycle is at the heart of Freeman’s theory of cortical dynamics, describing an organism’s interaction with its environment through a continuous loop of action and perception in search of food or social partners. Volitional actions are typically preceded by intentions, while perceptions are shaped by attention. These processes may not be sequential but parallel, involving distinct neural hierarchies. Thus, intention and attention, as key aspects of consciousness, should be viewed as interdependent and equally vital for adaptive behavior (\nIn the sections that follow, we explore how circular causality manifests at three interrelated levels relevant to volition: (1) within cortical neural networks, (2) across different brain regions, and (3) in interactions between the individual and the external environment.\nMesoscopic brain dynamics emerge from a delicate balance between excitation and inhibition, often producing oscillatory or more complex activity patterns (\nIt is evident that intrinsic noise can induce various phase transitions within network dynamics, potentially influencing higher-order functions. For instance, an elevated noise level localized to a small subset of network nodes can trigger widespread spatio-temporal oscillations across model networks, thereby recruiting otherwise inactive nodes into global network activity (\nThe complex neurodynamics of cortical networks can also exhibit chaotic behavior, with high sensitivity to initial conditions. Even small variations in firing patterns can lead to drastically different network outputs. Both experimental research (\nOur simulations show how microscopic events at the level of single neurons, or neuronal populations, can shape mesoscopic dynamics, which in turn relate to macroscopic cognitive functions. The modeled cortical structure includes one excitatory layer flanked by two inhibitory layers, representing pyramidal cells and two types of inhibitory interneurons, respectively. Excitatory network nodes are recurrently connected, and are also exciting the inhibitory nodes, while the inhibitory nodes are not interconnected. A bidirectional excitatory-inhibitory balance is maintained through distinct feedback loops (see\nModel of the cortical network structures used in our study. The upper and lower layers are each composed of 25 inhibitory nodes, corresponding to inhibitory interneurons, and the middle layer is composed of 100 excitatory nodes, corresponding to populations of pyramidal cells. External inputs stimulate a subset of the excitatory nodes, and all the excitatory nodes may excite the inhibitory ones. The model simulates a 10 × 10 mm square of each cortical structure.\nNetwork dynamics depend mainly on synaptic weight strengths and an order parameter,\n\nTo investigate how different brain regions interact during DM and volition, we developed neurocomputational models that simulate various experimental scenarios (\nOur DM model (\nSchematic flow chart of the subsystems and information flow in the modelled decision making process, including feedback within and without the brain-mind system (Adopted from\nNumerous internal and external factors influence DM, including desires, risks, and environmental uncertainty. For example, a large expected reward might drive behavior despite high costs, while uncertainty can lead to increased risk-taking and exploratory choices. In contrast, predictable situations support long-term planning.\nTime is a key parameter here: in intertemporal DM, future rewards are often undervalued—a tendency that leads to prioritizing immediate over delayed benefits, although this generally leads to a net loss (\nEffective DM is adaptive, influenced by individual experiences, preferences, attitudes, and social context. While attitudes are generally relatively stable, they can evolve at a longer time-scale through learning and social interaction. Our models also consider these influences (see next section and\nDecisions to act are closely linked to the readiness potential (RP) observed in Libet-type experiments (\nWe hypothesize that decisions arise from competition between System I (Amygdala/OFC) and System II (LPFC) dynamic cell assemblies. The “winning” signal proceeds to the supplementary motor area (SMA), initiating motor output. Our IC model investigates how RP might originate from earlier activity in LPFC, linked to rational decision-making (\nThe contribution of ACC to both rational and emotional aspects of human behavior makes it a hub for cortico-cortical and cortico-limbic connections (\nOur neurocomputational models illustrate the complex, dynamic interactions between brain areas involved in volition, where circular causality plays a central role. A key challenge is identifying where intentions originate and how they relate to earlier and subsequent decisions and actions. While models cannot fully explain brain processes, they can offer valuable insights.\nVolitional DM involves selecting among alternatives through conscious control. This process may be underpinned by complex, possibly chaotic, neurodynamics that eventually stabilize into more ordered, oscillatory activity associated with decision resolution. Our model-generated EEG-like signals help explore such oscillatory shifts, such as the transition from beta to gamma rhythms in the prefrontal cortex.\nWe have also simulated the evolution of neural attractor states and shown how intention may emerge through hierarchical processing of external and internal stimuli, as well as goal retrieval. These attractor states are modulated through feedback loops at multiple levels (\nThe choice of action, deciding among several options, is shaped by a range of internal and external factors. Whether consciously or unconsciously, we are influenced by others, both past and present. While personal experience often drives our behavior, social learning also plays a key role (\nFor instance, to encourage environmentally responsible behavior, authorities may offer incentives like subsidies for installing solar panels, buying electric cars, or organize recycling systems. A municipality and bus company might jointly offer free monthly tickets to commuters who leave their cars at home and instead take the bus to work. Often, early adopters inspire others, neighbors or coworkers, to follow suit, and over time, such behaviors become community norms, even after the incentives are removed (\nPositive or negative behaviors observed in others tend to impact our own decisions. Observing others (including figures of authority) can alter trust and inspire behavioral shifts (\nThe link between learning and goal-directed behavior extends to\nInterpersonal trust is closely tied to societal influence. Observing trusted individuals or authorities can shift one’s behavior, depending on the perceived reliability and consistency of their actions. In essence, observing others forms a foundation for learning and trust-building. As individuals interpret action-outcome relationships, predictability fosters trust, shaping both emotional and rational evaluations. Trust and observational learning thus emerge as key social forces that shape behavior and attitudes.\nIn the context of volition, it is especially important to trace the neural pathways involved, from intention formation to decision-making and action execution. Also in complex social systems, cause and effect are often difficult to isolate due to the interconnected nature of our environments. Even in controlled experiments on volition, such as Libet-type paradigms, so-called self-initiated actions may be subtly influenced by external cues (including the experimenter). This again suggests the relevance of circular causality in understanding volitional behavior.\nThis paper has examined the causal dynamics of volition within the brain–mind system, highlighting the inherent difficulty of identifying linear cause–effect relationships in the action-perception cycle. When our self-initiated actions are based on conscious decisions, we may feel we are acting out of free will, but it is not clear how free we really are from external influences. Our decisions may be based on a number of circumstances, knowledge, memories, attitudes, preferences, feelings, impressions, etc.,–all of which may have come to us from the environment or other individuals.\nWhile the concept of freedom is much debated in philosophy it may be confusing when approaching volition scientifically. Hence, it can be argued that\nAs we have seen from the examples above, cause and effect are difficult to separate in complex networks of interconnected entities at different levels of organization. In such complex, feedback-driven systems, clear causal chains are futile to look for. Neural activity may precede, follow, or occur simultaneously with mental events, and both are shaped by internal and external factors: genetic, physiological, environmental, and social. This interconnected web of influences renders human decisions highly unpredictable based on neural signals, with outcomes potentially shaped by both upward and downward causation.\nUltimately, the complexity of brain organization and the embeddedness of the individual in a dynamic social and physical environment challenge simplistic interpretations of volition. The study of free (conscious) will must account for the continuous interplay between neural systems, subjective experience, and environmental context, highlighting the need for integrative approaches that go beyond reductionist models. Our own neurocomputational modeling supports the view that decisions may result from interactions within and without the brain-mind system, with an intentional process that gradually becomes more and more conscious (see\nContrary to the dominant view that consciousness lacks causal power and that conscious will is merely an illusion, I argue that the concept of circular causality provides a compelling framework for understanding the experience of agency. To date, neurophysiological experiments, such as those by Libet and others, have not offered conclusive evidence against the existence of conscious will (\nNotably, even conclusions drawn from one of the leading theories of consciousness, Integrated Information Theory (IIT), suggest that consciousness may indeed possess causal powers (", "content_for_embedding": "A central challenge in the study of\nFree will, commonly defined as the capacity to make choices not strictly determined by prior causes, remains a core issue in\nWe conceptualize the brain as a complex system, with activity reflected in complex neurodynamics, including oscillations and fluctuations observable in EEG recordings. These dynamics result from interactions among multiple organizational levels, from ion channels and neurons to networks, each governed by distinct time scales and regulatory mechanisms. Also input from the environment affect the brain dynamics in ways hard to control.\nComplex systems typically exhibit non-linear interdependencies, thresholds, and feedback loops, which contribute to unexpected or non-intuitive behaviors. These properties demand a multiscale perspective to properly capture causal interactions (\nHermann Haken’s concept of\nHaken applied this approach to the human brain-mind system (\nIn addressing volition, we aim to clarify the intricate causal relationships within brain dynamics, as determined by network physiology and connectivity. Neural activity is typically assumed to cause mental phenomena (upward causation), while the reverse—mental states influencing neural processes (downward causation)—is often neglected, with some insightful exceptions (see\nOur neurocomputational models simulate various cortical regions, examining how their neurodynamics depend on structural factors such as connectivity and cell types, as well as intrinsic and external signals. We also explore how the complex neurodynamics may influence neuronal populations and relate to mental functions. This is particularly relevant for the\nThe action-perception cycle is at the heart of Freeman’s theory of cortical dynamics, describing an organism’s interaction with its environment through a continuous loop of action and perception in search of food or social partners. Volitional actions are typically preceded by intentions, while perceptions are shaped by attention. These processes may not be sequential but parallel, involving distinct neural hierarchies. Thus, intention and attention, as key aspects of consciousness, should be viewed as interdependent and equally vital for adaptive behavior (\nIn the sections that follow, we explore how circular causality manifests at three interrelated levels relevant to volition: (1) within cortical neural networks, (2) across different brain regions, and (3) in interactions between the individual and the external environment.\nMesoscopic brain dynamics emerge from a delicate balance between excitation and inhibition, often producing oscillatory or more complex activity patterns (\nIt is evident that intrinsic noise can induce various phase transitions within network dynamics, potentially influencing higher-order functions. For instance, an elevated noise level localized to a small subset of network nodes can trigger widespread spatio-temporal oscillations across model networks, thereby recruiting otherwise inactive nodes into global network activity (\nThe complex neurodynamics of cortical networks can also exhibit chaotic behavior, with high sensitivity to initial conditions. Even small variations in firing patterns can lead to drastically different network outputs. Both experimental research (\nOur simulations show how microscopic events at the level of single neurons, or neuronal populations, can shape mesoscopic dynamics, which in turn relate to macroscopic cognitive functions. The modeled cortical structure includes one excitatory layer flanked by two inhibitory layers, representing pyramidal cells and two types of inhibitory interneurons, respectively. Excitatory network nodes are recurrently connected, and are also exciting the inhibitory nodes, while the inhibitory nodes are not interconnected. A bidirectional excitatory-inhibitory balance is maintained through distinct feedback loops (see\nModel of the cortical network structures used in our study. The upper and lower layers are each composed of 25 inhibitory nodes, corresponding to inhibitory interneurons, and the middle layer is composed of 100 excitatory nodes, corresponding to populations of pyramidal cells. External inputs stimulate a subset of the excitatory nodes, and all the excitatory nodes may excite the inhibitory ones. The model simulates a 10 × 10 mm square of each cortical structure.\nNetwork dynamics depend mainly on synaptic weight strengths and an order parameter,\n\nTo investigate how different brain regions interact during DM and volition, we developed neurocomputational models that simulate various experimental scenarios (\nOur DM model (\nSchematic flow chart of the subsystems and information flow in the modelled decision making process, including feedback within and without the brain-mind system (Adopted from\nNumerous internal and external factors influence DM, including desires, risks, and environmental uncertainty. For example, a large expected reward might drive behavior despite high costs, while uncertainty can lead to increased risk-taking and exploratory choices. In contrast, predictable situations support long-term planning.\nTime is a key parameter here: in intertemporal DM, future rewards are often undervalued—a tendency that leads to prioritizing immediate over delayed benefits, although this generally leads to a net loss (\nEffective DM is adaptive, influenced by individual experiences, preferences, attitudes, and social context. While attitudes are generally relatively stable, they can evolve at a longer time-scale through learning and social interaction. Our models also consider these influences (see next section and\nDecisions to act are closely linked to the readiness potential (RP) observed in Libet-type experiments (\nWe hypothesize that decisions arise from competition between System I (Amygdala/OFC) and System II (LPFC) dynamic cell assemblies. The “winning” signal proceeds to the supplementary motor area (SMA), initiating motor output. Our IC model investigates how RP might originate from earlier activity in LPFC, linked to rational decision-making (\nThe contribution of ACC to both rational and emotional aspects of human behavior makes it a hub for cortico-cortical and cortico-limbic connections (\nOur neurocomputational models illustrate the complex, dynamic interactions between brain areas involved in volition, where circular causality plays a central role. A key challenge is identifying where intentions originate and how they relate to earlier and subsequent decisions and actions. While models cannot fully explain brain processes, they can offer valuable insights.\nVolitional DM involves selecting among alternatives through conscious control. This process may be underpinned by complex, possibly chaotic, neurodynamics that eventually stabilize into more ordered, oscillatory activity associated with decision resolution. Our model-generated EEG-like signals help explore such oscillatory shifts, such as the transition from beta to gamma rhythms in the prefrontal cortex.\nWe have also simulated the evolution of neural attractor states and shown how intention may emerge through hierarchical processing of external and internal stimuli, as well as goal retrieval. These attractor states are modulated through feedback loops at multiple levels (\nThe choice of action, deciding among several options, is shaped by a range of internal and external factors. Whether consciously or unconsciously, we are influenced by others, both past and present. While personal experience often drives our behavior, social learning also plays a key role (\nFor instance, to encourage environmentally responsible behavior, authorities may offer incentives like subsidies for installing solar panels, buying electric cars, or organize recycling systems. A municipality and bus company might jointly offer free monthly tickets to commuters who leave their cars at home and instead take the bus to work. Often, early adopters inspire others, neighbors or coworkers, to follow suit, and over time, such behaviors become community norms, even after the incentives are removed (\nPositive or negative behaviors observed in others tend to impact our own decisions. Observing others (including figures of authority) can alter trust and inspire behavioral shifts (\nThe link between learning and goal-directed behavior extends to\nInterpersonal trust is closely tied to societal influence. Observing trusted individuals or authorities can shift one’s behavior, depending on the perceived reliability and consistency of their actions. In essence, observing others forms a foundation for learning and trust-building. As individuals interpret action-outcome relationships, predictability fosters trust, shaping both emotional and rational evaluations. Trust and observational learning thus emerge as key social forces that shape behavior and attitudes.\nIn the context of volition, it is especially important to trace the neural pathways involved, from intention formation to decision-making and action execution. Also in complex social systems, cause and effect are often difficult to isolate due to the interconnected nature of our environments. Even in controlled experiments on volition, such as Libet-type paradigms, so-called self-initiated actions may be subtly influenced by external cues (including the experimenter). This again suggests the relevance of circular causality in understanding volitional behavior.\nThis paper has examined the causal dynamics of volition within the brain–mind system, highlighting the inherent difficulty of identifying linear cause–effect relationships in the action-perception cycle. When our self-initiated actions are based on conscious decisions, we may feel we are acting out of free will, but it is not clear how free we really are from external influences. Our decisions may be based on a number of circumstances, knowledge, memories, attitudes, preferences, feelings, impressions, etc.,–all of which may have come to us from the environment or other individuals.\nWhile the concept of freedom is much debated in philosophy it may be confusing when approaching volition scientifically. Hence, it can be argued that\nAs we have seen from the examples above, cause and effect are difficult to separate in complex networks of interconnected entities at different levels of organization. In such complex, feedback-driven systems, clear causal chains are futile to look for. Neural activity may precede, follow, or occur simultaneously with mental events, and both are shaped by internal and external factors: genetic, physiological, environmental, and social. This interconnected web of influences renders human decisions highly unpredictable based on neural signals, with outcomes potentially shaped by both upward and downward causation.\nUltimately, the complexity of brain organization and the embeddedness of the individual in a dynamic social and physical environment challenge simplistic interpretations of volition. The study of free (conscious) will must account for the continuous interplay between neural systems, subjective experience, and environmental context, highlighting the need for integrative approaches that go beyond reductionist models. Our own neurocomputational modeling supports the view that decisions may result from interactions within and without the brain-mind system, with an intentional process that gradually becomes more and more conscious (see\nContrary to the dominant view that consciousness lacks causal power and that conscious will is merely an illusion, I argue that the concept of circular causality provides a compelling framework for understanding the experience of agency. To date, neurophysiological experiments, such as those by Libet and others, have not offered conclusive evidence against the existence of conscious will (\nNotably, even conclusions drawn from one of the leading theories of consciousness, Integrated Information Theory (IIT), suggest that consciousness may indeed possess causal powers (", "topic": "Brain"}
{"pmid": "39711753", "pmcid": "12307367", "title": "Enhancing golf swing performance through M1-targeted transcranial direct current stimulation: a double-blind, randomized crossover study", "publication_year": "N/A", "abstract": "", "full_text": "As a sport enjoyed by more than 60 million people worldwide (\nCurrent golf training programs are conducted under the guidance of existing biomechanical researches (\nOne such approach is transcranial direct current stimulation (tDCS), a non-invasive neuromodulation technique, offering the possibility to regulate the neuroplasticity. Via mechanisms such as long-term potentiation (\nA recent systematic review and meta-analysis supports the application of anodal tDCS in sport. This comprehensive review included 19 articles covering different aspects of performance, such as strength, endurance, and visuomotor skills (\nMost existing tDCS studies chose the primary motor cortex (M1) as the stimulation area, for its role in governing voluntary force generation (\nTherefore, this exploratory study aimed to investigate whether M1-targeted tDCS can acutely enhance golf swing performance across swing types with different performance demands. Specifically, we examined: (1) whether tDCS can enhance long-driving distance capacity (e.g., clubhead speed, ball speed, carry distance); (2) whether it can improve distance and direction accuracy (e.g., face angle, directional deviation). By integrating brain stimulation with complicated motor skills, this study seeks to advance understanding of how tDCS may be applied in sports performance contexts. For elite golfers, this approach may provide them with a new way to prepare for competition. Although this study focused exclusively on elite athletes, the findings may inform future research exploring whether tDCS can reduce training volume and injury risk in broader athletic populations. Consequently, this work offers a neuroscience-based perspective on how people can improve complex motor skills while protecting their bodies, bringing together high-performance goals and long-term well-being.\nEight professional golfers (4 males, 4 females; age: 22.3 ± 2.0 years; height: 176.6 ± 10.9 cm; body mass: 74.4 ± 15.2 kg; training years: 8.3 ± 0.7 years), all right-handed and actively competing in national-level tournaments, volunteered to participate in this study. The decision to include only professional golfers was based on the need to reduce inter-individual variability in technique, swing mechanics, and performance consistency. Elite golfers demonstrate stable and repeatable motor patterns, which is particularly important for verifying the acute and immediate effects of tDCS.\nAn\nHealth questionnaire was signed before the study to ensure all participants did not have any head trauma, neurological disorders, metallic implants, muscular dysfunction, prior brain stimulation exposure or any contraindications. After receiving a full explanation of the procedures, potential risks, and their right to withdraw, written informed consent was obtained from each participant. Ethical approval was granted by the institutional review board of Shanghai University of Sport, and all procedures met the international safety guidelines for non-invasive brain stimulation.\nA double-blind, randomized, counterbalanced crossover design (\nSummary of the study design.\nTo minimize practice-related improvements, all participants completed a standardized warm-up and 10 familiarization swings before data collection in each session. Only performance data from the main testing block were collected. Because each participant was exposed to the same procedural structure across both conditions, any learning effects were equally distributed across the experiment. Moreover, participants were young and highly trained for long time, allowing the minimal bias caused from learning effects and fatigue. The consistent format of testing, along with the within-subject crossover design and the stable competitive level, ensured that observed performance changes were attributable to stimulation condition rather than task familiarity or fatigue accumulation.\nThe intervention session was conducted in a separate room using a DC-Stimulator Plus device (NeuroConn, GmbH, Germany) and managed by the researcher not involved in data collection to ensure the blindness. The room was monitored to maintain a stable temperature and humidity (23 ± 1°C, 50 ± 5% relative humidity).\nAccording to the international 10–20 EEG system, the anodal electrode (5 cm × 7 cm, 5 mm sponge thickness) was placed over the left M1 (EEG location: C3), while the cathodal electrode (5 cm × 7 cm, 5 mm sponge thickness) was positioned over the contralateral prefrontal cortex (EEG location: Fp2). Electrode placement was checked by anatomical landmarks to ensure the standardization across participants and simulated using SIMNIBS software (version: 4.1.0) to see the possible activation of stimulation (\nSoftware simulation of brain activation pattern following the targeted stimulation protocol.\nThis montage was selected based on prior research showing that anodal stimulation of the dominant hemisphere's M1 enhances excitability of corticospinal pathways, especially in right-handed individuals. Cathodal stimulation, which typically reduces cortical excitability, is less aligned with the performance enhancement objectives of this investigation. Placement of the return electrode over contralateral prefrontal cortex minimizes the likelihood of inhibitory current flow through adjacent motor areas, thus focusing stimulation on the motor cortex and avoiding bilateral motor effects (\nFor the A-tDCS session, a 2 mA current (density: 0.057 mA/cm\nTo assess the integrity of blinding, participants were asked after each session whether they believed they had received active or sham stimulation, and to report any sensations experienced (e.g., tingling, warmth, itching, or fatigue). No significant differences were reported in perceived discomfort or sensation intensity between the A-tDCS and S-tDCS sessions. The most common sensation across both conditions was mild tingling at the electrode site within the first 30 s of stimulation onset, which dissipated quickly. These responses suggested that the sham protocol was effective in maintaining participant blinding.\nAll swing tests were carried out in a standard indoor golf room. A thick gray curtain was placed in the swing direction, with a red circle drawn at the center as a target. Performance metrics were recorded using a Doppler radar-based launch monitor (Trackman 4, Denmark), which captures the full swing and ball flight in 3D by measuring instantaneous clubhead and ball speeds, as well as computing derived metrics such as carry distance, face angle, and side deviation based on the launch angle, spin axis, ball velocity, and impact dynamics. Prior studies have examined the validity and reliability of the monitor in measuring swing performance metrics, with intraclass correlation coefficients greater than 0.87 (\nTrackman's carry distance is computed from measured launch data using a proprietary ball flight model that accounts for launch angle, spin rate, and atmospheric conditions. While the raw measurements (e.g., clubhead speed, ball speed) have high temporal fidelity (sampling rate = 40,000 samples per second), the derived values are estimates and may carry a larger margin of error, particularly in indoor settings. To minimize variability, all trials were conducted indoors under standardized lighting and temperature, with no wind, and consistent ball type and tee height across sessions (\nPrior to each testing session, the Trackman was calibrated using the manufacturer's auto-level and alignment tools to ensure accurate horizontal and vertical tracking. Each session began with a short calibration warm-up (3–5 swings) to confirm radar-tracking consistency.\nEach participant completed three swing tasks before and after each intervention session, whether A-tDCS or S-tDCS:\nDriver swing test: maximal driving distance,\nIron 7 (34° loft): a balanced task requiring both long-driving distance capacity and accuracy,\n100-yard wedge shot: optimal precision control.\nEach task consisted of 10 swing attempts, with a 60-second rest between each attempt. A certified strength and conditioning coach led the participants to warm up before the test, including a 5-minute dynamic stretch and 10 practice swings. During the test, a certified golf coach provided standardized oral instructions, such as “keep head steady”, to minimize technique variability and ensure the performance maximization.\nThe following swing performance variables were collected: (1) Long-driving distance capacity: clubhead speed, ball speed, and carry distance; (2) Accuracy-related: face angle, side distance. For wedge shot accuracy, the accuracy-related variables were calculated using side distance, carry distance, and target distance to get the horizontal, lateral, and radial error.\nFor data cleaning, the following procedure was applied: (1) trials with obvious signal dropouts, (2) miss-hits, or (3) data inconsistencies exceeding 3 SD from individual means were excluded. Fewer than 5% of trials were removed under these criteria. For consistency, performance variables were averaged from 3 valid swings selected from 5 consistent attempts (out of 10 total), excluding each participant's longest and shortest swings to avoid outliers and enhance within-subject reliability.\nAll data were processed using Excel and SPSS (Version 29.0, IBM). Outliers were checked using 3 median absolute deviation, and no data was excluded. To examine the effects of stimulation (A-tDCS vs. S-tDCS) and time (Pre vs. Post), we performed two-way repeated-measures ANOVAs for each dependent variable. Each ANOVA model included two within-subject factors: Condition (2 levels) and Time (2 levels). This approach allowed assessment of main effect as well as Condition × Time interactions.\nWhen a significant interaction was observed, we conducted\nAssumption of normality were assessed via Shapiro–Wilk tests and visual inspection of Q-Q plots. Shapiro–Wilk tests revealed all variables were normally distributed (W ≥ 0.823,\nThis randomized, double-blind, crossover trial evaluated the acute effects of A-tDCS vs. S-tDCS on golf swing performance across three swing tasks: driver, iron, and wedge. Descriptive statistics for all variables are presented in\nDescriptive summary of all variables.\nMean ± SD. A-tDCS, active transcranial direct current stimulation group; S-tDCS, sham transcranial direct current stimulation group.\nRepeated-measures ANOVA (\nResults of repeat-measures-ANOVAs for driver task.\nSignificant main effect (\nSignificant interaction effect (\nComparison of golf swing performance metrics under anodal (A) and sham (S) tDCS conditions in the driver task.\nA similar pattern was observed for carry distance, which also showed a significant Time × Condition interaction [F\nFace angle also showed a significant interaction effect [F\nOther variables, including ball speed and side distance, did not show statistically significant interactions (\nThe ANOVA results for iron task are presented in\nResults of repeat-measures-ANOVAs for iron task.\nSignificant main effect (\nSignificant interaction effect (\nPost hoc comparisons revealed a significant reduction in clubhead speed (\nComparison of golf swing performance metrics under anodal (A) and sham (S) tDCS conditions in the iron task.\nBall speed (\nCarry distance (\nIn terms of accuracy, side distance (\nNo significant Time × Condition interactions were observed for any of the accuracy variables in the wedge shot task (\nResults of repeat-measures-ANOVAs for wedge shot task.\nThis study selected the golf swing as a model to examine whether tDCS targeting the M1 could acutely enhance golf swing performance across different task demands. Three representative swing tasks were selected as the model for their distinct neural patterns, including maximal distance demanded driver swing, moderate distance- and precision-required iron swing, and optimal accuracy-depended wedge shot (\nUsing a double-blind, crossover design and 2 × 2 repeated-measures ANOVA, the results revealed that M1-targeted anodal A-tDCS elicited acute and significant improvements in long-driving distance capacity: ball speed and carry distance in iron tasks and carry distance in driver task. The robust improvements were accompanied by large effect sizes (Hedge's\nFurthermore, ball speed and clubhead speed significantly declined under the sham condition in the iron task, while remaining stable or improving under A-tDCS. These opposing trends served as complementary findings, strengthening the conclusion that A-tDCS enhances force generation and may also counteract performance fatigue during repetitive tasks (\nInterestingly, A-tDCS also led to a significant reduction in side deviation in the iron task, suggesting potential accuracy gains. However, traditional knowledge about M1 is its association with motor execution instead of fine motor control or visuospatial processing (\nIn contrast, M1-targeted tDCS was limited for tasks that heavily rely on cerebellar-parietal networks, as evidenced by the lack of significant stimulation-specific effects in the wedge task, where high spatial accuracy is dominated over force production. Identical improvements in both conditions confirm practice effects dominated over neuromodulation in this precision task. This finding aligns with the traditional dichotomy that matching stimulation targets to task-specific neural demands is important to address distinct issues (\nThe decision to target M1 in this study was based on its well-established role in modulating corticospinal excitability and voluntary motor output, particularly in power-dominant tasks (\nThe results found in this study provide several practical implications for applying tDCS in golf training. For professional golfers, the task-specific gains in long-driving distance capacity make M1-tDCS a possible tool for warm-ups before competitions, and a potential supplementary approach to maximize long-driving distance capacity with minimal physical loads. For recreational players, tDCS may offer a safer path for practicing and performance improvement, thus reducing the cumulative mechanical stress from repetitions. This is crucial to maintain their health condition and ensure the lifelong engagement in physical activities, especially under the background that golf globally attracts more than 60 million population (\nNevertheless, several limitations must be acknowledged. Firstly, though a satisfactory trend and significance was observed, the sample size is small and the participants is professional golfers, which restricts the generalizability of this study. However, this homogeneous sample was chosen to minimize interindividual variability, allow for accurate detection of acute stimulation effects, and provide the empirical evidence for future insightful research. Secondly, only acute effects were investigated in this study. It is still unknown whether long-term A-tDCS could induce better enhancements or it may face ceiling effects, and whether regular application of A-tDCS could reduce the training volume. Thirdly, the exclusive stimulation of M1 limits the interpretation of the neural mechanisms involved in accuracy improvements, especially in the absence of neuroimaging or neurophysiological data, such as EEG or TMS. Lastly, the potential placebo effect, although minimized through a double-blind, sham-controlled design, cannot be entirely ruled out. Further research may explore the chronic effects of repeated tDCS, as well as dual-site or high-definition tDCS, on larger sample sizes and broader golf populations, such as novice and master players. Moreover, biomechanical and neuromuscular measurements could be incorporated in future studies to explore the potential mechanisms underlying the observed performance enhancements in this study.\nIn conclusion, this study provides evidence that M1-targeted anodal tDCS can acutely enhance performance in golf swing tasks requiring explosive motor output, with some potential benefit to directional accuracy in tasks that integrate both power and precision. However, limited improvements were found in tasks highly dependent on precision control, which highlights the importance of intervention strategies in neural-based performance studies. These findings suggest that tDCS may serve as a valuable tool in sport training, especially for those demanding maximal and rapid force generation. By bridging the gap between motor performance requirement and injury prevention, tDCS may contribute to safer, more efficient training paradigms in both elite and broader athletic populations.", "content_for_embedding": "As a sport enjoyed by more than 60 million people worldwide (\nCurrent golf training programs are conducted under the guidance of existing biomechanical researches (\nOne such approach is transcranial direct current stimulation (tDCS), a non-invasive neuromodulation technique, offering the possibility to regulate the neuroplasticity. Via mechanisms such as long-term potentiation (\nA recent systematic review and meta-analysis supports the application of anodal tDCS in sport. This comprehensive review included 19 articles covering different aspects of performance, such as strength, endurance, and visuomotor skills (\nMost existing tDCS studies chose the primary motor cortex (M1) as the stimulation area, for its role in governing voluntary force generation (\nTherefore, this exploratory study aimed to investigate whether M1-targeted tDCS can acutely enhance golf swing performance across swing types with different performance demands. Specifically, we examined: (1) whether tDCS can enhance long-driving distance capacity (e.g., clubhead speed, ball speed, carry distance); (2) whether it can improve distance and direction accuracy (e.g., face angle, directional deviation). By integrating brain stimulation with complicated motor skills, this study seeks to advance understanding of how tDCS may be applied in sports performance contexts. For elite golfers, this approach may provide them with a new way to prepare for competition. Although this study focused exclusively on elite athletes, the findings may inform future research exploring whether tDCS can reduce training volume and injury risk in broader athletic populations. Consequently, this work offers a neuroscience-based perspective on how people can improve complex motor skills while protecting their bodies, bringing together high-performance goals and long-term well-being.\nEight professional golfers (4 males, 4 females; age: 22.3 ± 2.0 years; height: 176.6 ± 10.9 cm; body mass: 74.4 ± 15.2 kg; training years: 8.3 ± 0.7 years), all right-handed and actively competing in national-level tournaments, volunteered to participate in this study. The decision to include only professional golfers was based on the need to reduce inter-individual variability in technique, swing mechanics, and performance consistency. Elite golfers demonstrate stable and repeatable motor patterns, which is particularly important for verifying the acute and immediate effects of tDCS.\nAn\nHealth questionnaire was signed before the study to ensure all participants did not have any head trauma, neurological disorders, metallic implants, muscular dysfunction, prior brain stimulation exposure or any contraindications. After receiving a full explanation of the procedures, potential risks, and their right to withdraw, written informed consent was obtained from each participant. Ethical approval was granted by the institutional review board of Shanghai University of Sport, and all procedures met the international safety guidelines for non-invasive brain stimulation.\nA double-blind, randomized, counterbalanced crossover design (\nSummary of the study design.\nTo minimize practice-related improvements, all participants completed a standardized warm-up and 10 familiarization swings before data collection in each session. Only performance data from the main testing block were collected. Because each participant was exposed to the same procedural structure across both conditions, any learning effects were equally distributed across the experiment. Moreover, participants were young and highly trained for long time, allowing the minimal bias caused from learning effects and fatigue. The consistent format of testing, along with the within-subject crossover design and the stable competitive level, ensured that observed performance changes were attributable to stimulation condition rather than task familiarity or fatigue accumulation.\nThe intervention session was conducted in a separate room using a DC-Stimulator Plus device (NeuroConn, GmbH, Germany) and managed by the researcher not involved in data collection to ensure the blindness. The room was monitored to maintain a stable temperature and humidity (23 ± 1°C, 50 ± 5% relative humidity).\nAccording to the international 10–20 EEG system, the anodal electrode (5 cm × 7 cm, 5 mm sponge thickness) was placed over the left M1 (EEG location: C3), while the cathodal electrode (5 cm × 7 cm, 5 mm sponge thickness) was positioned over the contralateral prefrontal cortex (EEG location: Fp2). Electrode placement was checked by anatomical landmarks to ensure the standardization across participants and simulated using SIMNIBS software (version: 4.1.0) to see the possible activation of stimulation (\nSoftware simulation of brain activation pattern following the targeted stimulation protocol.\nThis montage was selected based on prior research showing that anodal stimulation of the dominant hemisphere's M1 enhances excitability of corticospinal pathways, especially in right-handed individuals. Cathodal stimulation, which typically reduces cortical excitability, is less aligned with the performance enhancement objectives of this investigation. Placement of the return electrode over contralateral prefrontal cortex minimizes the likelihood of inhibitory current flow through adjacent motor areas, thus focusing stimulation on the motor cortex and avoiding bilateral motor effects (\nFor the A-tDCS session, a 2 mA current (density: 0.057 mA/cm\nTo assess the integrity of blinding, participants were asked after each session whether they believed they had received active or sham stimulation, and to report any sensations experienced (e.g., tingling, warmth, itching, or fatigue). No significant differences were reported in perceived discomfort or sensation intensity between the A-tDCS and S-tDCS sessions. The most common sensation across both conditions was mild tingling at the electrode site within the first 30 s of stimulation onset, which dissipated quickly. These responses suggested that the sham protocol was effective in maintaining participant blinding.\nAll swing tests were carried out in a standard indoor golf room. A thick gray curtain was placed in the swing direction, with a red circle drawn at the center as a target. Performance metrics were recorded using a Doppler radar-based launch monitor (Trackman 4, Denmark), which captures the full swing and ball flight in 3D by measuring instantaneous clubhead and ball speeds, as well as computing derived metrics such as carry distance, face angle, and side deviation based on the launch angle, spin axis, ball velocity, and impact dynamics. Prior studies have examined the validity and reliability of the monitor in measuring swing performance metrics, with intraclass correlation coefficients greater than 0.87 (\nTrackman's carry distance is computed from measured launch data using a proprietary ball flight model that accounts for launch angle, spin rate, and atmospheric conditions. While the raw measurements (e.g., clubhead speed, ball speed) have high temporal fidelity (sampling rate = 40,000 samples per second), the derived values are estimates and may carry a larger margin of error, particularly in indoor settings. To minimize variability, all trials were conducted indoors under standardized lighting and temperature, with no wind, and consistent ball type and tee height across sessions (\nPrior to each testing session, the Trackman was calibrated using the manufacturer's auto-level and alignment tools to ensure accurate horizontal and vertical tracking. Each session began with a short calibration warm-up (3–5 swings) to confirm radar-tracking consistency.\nEach participant completed three swing tasks before and after each intervention session, whether A-tDCS or S-tDCS:\nDriver swing test: maximal driving distance,\nIron 7 (34° loft): a balanced task requiring both long-driving distance capacity and accuracy,\n100-yard wedge shot: optimal precision control.\nEach task consisted of 10 swing attempts, with a 60-second rest between each attempt. A certified strength and conditioning coach led the participants to warm up before the test, including a 5-minute dynamic stretch and 10 practice swings. During the test, a certified golf coach provided standardized oral instructions, such as “keep head steady”, to minimize technique variability and ensure the performance maximization.\nThe following swing performance variables were collected: (1) Long-driving distance capacity: clubhead speed, ball speed, and carry distance; (2) Accuracy-related: face angle, side distance. For wedge shot accuracy, the accuracy-related variables were calculated using side distance, carry distance, and target distance to get the horizontal, lateral, and radial error.\nFor data cleaning, the following procedure was applied: (1) trials with obvious signal dropouts, (2) miss-hits, or (3) data inconsistencies exceeding 3 SD from individual means were excluded. Fewer than 5% of trials were removed under these criteria. For consistency, performance variables were averaged from 3 valid swings selected from 5 consistent attempts (out of 10 total), excluding each participant's longest and shortest swings to avoid outliers and enhance within-subject reliability.\nAll data were processed using Excel and SPSS (Version 29.0, IBM). Outliers were checked using 3 median absolute deviation, and no data was excluded. To examine the effects of stimulation (A-tDCS vs. S-tDCS) and time (Pre vs. Post), we performed two-way repeated-measures ANOVAs for each dependent variable. Each ANOVA model included two within-subject factors: Condition (2 levels) and Time (2 levels). This approach allowed assessment of main effect as well as Condition × Time interactions.\nWhen a significant interaction was observed, we conducted\nAssumption of normality were assessed via Shapiro–Wilk tests and visual inspection of Q-Q plots. Shapiro–Wilk tests revealed all variables were normally distributed (W ≥ 0.823,\nThis randomized, double-blind, crossover trial evaluated the acute effects of A-tDCS vs. S-tDCS on golf swing performance across three swing tasks: driver, iron, and wedge. Descriptive statistics for all variables are presented in\nDescriptive summary of all variables.\nMean ± SD. A-tDCS, active transcranial direct current stimulation group; S-tDCS, sham transcranial direct current stimulation group.\nRepeated-measures ANOVA (\nResults of repeat-measures-ANOVAs for driver task.\nSignificant main effect (\nSignificant interaction effect (\nComparison of golf swing performance metrics under anodal (A) and sham (S) tDCS conditions in the driver task.\nA similar pattern was observed for carry distance, which also showed a significant Time × Condition interaction [F\nFace angle also showed a significant interaction effect [F\nOther variables, including ball speed and side distance, did not show statistically significant interactions (\nThe ANOVA results for iron task are presented in\nResults of repeat-measures-ANOVAs for iron task.\nSignificant main effect (\nSignificant interaction effect (\nPost hoc comparisons revealed a significant reduction in clubhead speed (\nComparison of golf swing performance metrics under anodal (A) and sham (S) tDCS conditions in the iron task.\nBall speed (\nCarry distance (\nIn terms of accuracy, side distance (\nNo significant Time × Condition interactions were observed for any of the accuracy variables in the wedge shot task (\nResults of repeat-measures-ANOVAs for wedge shot task.\nThis study selected the golf swing as a model to examine whether tDCS targeting the M1 could acutely enhance golf swing performance across different task demands. Three representative swing tasks were selected as the model for their distinct neural patterns, including maximal distance demanded driver swing, moderate distance- and precision-required iron swing, and optimal accuracy-depended wedge shot (\nUsing a double-blind, crossover design and 2 × 2 repeated-measures ANOVA, the results revealed that M1-targeted anodal A-tDCS elicited acute and significant improvements in long-driving distance capacity: ball speed and carry distance in iron tasks and carry distance in driver task. The robust improvements were accompanied by large effect sizes (Hedge's\nFurthermore, ball speed and clubhead speed significantly declined under the sham condition in the iron task, while remaining stable or improving under A-tDCS. These opposing trends served as complementary findings, strengthening the conclusion that A-tDCS enhances force generation and may also counteract performance fatigue during repetitive tasks (\nInterestingly, A-tDCS also led to a significant reduction in side deviation in the iron task, suggesting potential accuracy gains. However, traditional knowledge about M1 is its association with motor execution instead of fine motor control or visuospatial processing (\nIn contrast, M1-targeted tDCS was limited for tasks that heavily rely on cerebellar-parietal networks, as evidenced by the lack of significant stimulation-specific effects in the wedge task, where high spatial accuracy is dominated over force production. Identical improvements in both conditions confirm practice effects dominated over neuromodulation in this precision task. This finding aligns with the traditional dichotomy that matching stimulation targets to task-specific neural demands is important to address distinct issues (\nThe decision to target M1 in this study was based on its well-established role in modulating corticospinal excitability and voluntary motor output, particularly in power-dominant tasks (\nThe results found in this study provide several practical implications for applying tDCS in golf training. For professional golfers, the task-specific gains in long-driving distance capacity make M1-tDCS a possible tool for warm-ups before competitions, and a potential supplementary approach to maximize long-driving distance capacity with minimal physical loads. For recreational players, tDCS may offer a safer path for practicing and performance improvement, thus reducing the cumulative mechanical stress from repetitions. This is crucial to maintain their health condition and ensure the lifelong engagement in physical activities, especially under the background that golf globally attracts more than 60 million population (\nNevertheless, several limitations must be acknowledged. Firstly, though a satisfactory trend and significance was observed, the sample size is small and the participants is professional golfers, which restricts the generalizability of this study. However, this homogeneous sample was chosen to minimize interindividual variability, allow for accurate detection of acute stimulation effects, and provide the empirical evidence for future insightful research. Secondly, only acute effects were investigated in this study. It is still unknown whether long-term A-tDCS could induce better enhancements or it may face ceiling effects, and whether regular application of A-tDCS could reduce the training volume. Thirdly, the exclusive stimulation of M1 limits the interpretation of the neural mechanisms involved in accuracy improvements, especially in the absence of neuroimaging or neurophysiological data, such as EEG or TMS. Lastly, the potential placebo effect, although minimized through a double-blind, sham-controlled design, cannot be entirely ruled out. Further research may explore the chronic effects of repeated tDCS, as well as dual-site or high-definition tDCS, on larger sample sizes and broader golf populations, such as novice and master players. Moreover, biomechanical and neuromuscular measurements could be incorporated in future studies to explore the potential mechanisms underlying the observed performance enhancements in this study.\nIn conclusion, this study provides evidence that M1-targeted anodal tDCS can acutely enhance performance in golf swing tasks requiring explosive motor output, with some potential benefit to directional accuracy in tasks that integrate both power and precision. However, limited improvements were found in tasks highly dependent on precision control, which highlights the importance of intervention strategies in neural-based performance studies. These findings suggest that tDCS may serve as a valuable tool in sport training, especially for those demanding maximal and rapid force generation. By bridging the gap between motor performance requirement and injury prevention, tDCS may contribute to safer, more efficient training paradigms in both elite and broader athletic populations.", "topic": "Brain"}
{"pmid": "39658556", "pmcid": "12306509", "title": "The Effect of Overcoming the Digital Divide on Middle Frontal Gyrus Atrophy in Aging Adults: Large-Scale Retrospective Magnetic Resonance Imaging Cohort Study", "publication_year": "N/A", "abstract": "", "full_text": "Although mobile devices have become some of the most indispensable technologies in modern society, there is still a significant portion of the population that has never used them. This gap between individuals who are adept at using digital information and communication technologies (ICTs) and those who are not is referred to as the digital divide (DD) [\nThe theory of neural plasticity suggests that the structure of the human brain can undergo long-term changes in response to environmental stimuli and situational triggers [\nThe theory of frontal lobe control and the extended theory of internet addiction suggest that degenerated prefrontal cortical regions could lead to the dysfunction of cognitive control and inhibition, which is the potential physiological basis for problematic online behaviors [\nIn sum, the long-term implications of addressing the DD among the older adult population, whether positive or negative, are yet to be substantiated.\nTo ascertain the long-term implications of addressing the DD among the older population, this study, using a large-sample neuroimaging cohort, will investigate the following: (1) the differences in brain structure and cognitive performance between individuals who overcome the DD and those who do not, (2) whether the identified brain regions that classify the 2 groups will also predict their cognitive performance, and (3) the longitudinal alterations of the aging rate in cognitive function and brain structure caused by the DD.\nA relationship may exist between the longitudinal changes in cognitive function and brain structure observed in aging populations and the existence of a DD.\nThe study population was derived from the Beijing Aging Brain Rejuvenation Initiative, a longitudinal neuroimaging initiative investigating aging-related cognitive trajectories. Participants were not prospectively recruited for this specific analysis but constituted a subset of individuals meeting predefined inclusion criteria. Our samples were community-based. They were all recruited voluntarily. The majority of them live in different communities in Beijing. Established at Beijing Normal University in 2008, the Beijing Aging Brain Rejuvenation Initiative has conducted cohort studies based on the registry of a large community population in the greater metropolitan area of Beijing. All of the participants were 50 years or above at the time of baseline enrollment, capable of living independently, without nervous system diseases or psychiatric disorders, with no metal implants or any other contraindications for undergoing magnetic resonance imaging (MRI) within the body, with 6 or more years of formal education. A total of 3380 participants with MRI data were recruited; participants with a Mini-Mental State Examination score below 24 and without the data to quantify the DD were excluded. As a result, only 1400 participants were recruited in this study. Of 1400 participants, 1280 were included in this study based on propensity score matching (PSM). All of the participants who were registered were revisited every 2-3 years. Throughout the follow-up period, factors like bodily metal implants, severe ailments, loss to follow-up, or participant refusal led to only 689 individuals undergoing a subsequent MRI scan (see Figure S1 in\nAccording to the cross-sectional data, we divided the participants into the DD group and overcoming DD (ODD) group based on the quantification of the DD (see the Measurements section). Power calculations were conducted a priori to ensure adequate statistical sensitivity. For the primary comparison between the digital engagement groups (DD vs ODD), we estimated required sample sizes using a 2-sample\nThis calculation guided our longitudinal tracking cohort design (n=350 per group) and cross-sectional neuroimaging subsample (n=700 per group). The effect size threshold (\nThe quantification indicator of the DD involves an item about the frequency of using ICTs from the Leisure Activity Scale: “How often do you use a computer and mobile devices?” We classified individuals with scores of 0 (never), 1 (≥once per year), and 2 (≥once per month) into the DD group and individuals with scores of 4 (≥once per week) and 5 (everyday) into the ODD group [\nIn the longitudinal data analysis, participants for whom the use of ICTs could be tracked over time were categorized into either the DD group or the ODD group. We excluded participants who transitioned their status of using ICTs because crossing the DD is a relatively stable state that would not change in the short term. If a transition occurred, it may be due to uncontrollable external factors, which are not the focus of this study.\nAs described in our previous study, all participants underwent a battery of neuropsychological tests at baseline recruitment [\nWe conducted a confirmatory factor analysis analysis for the performance of each cognitive domain (Figure S2 in\nMRI data were acquired using a SIEMENS PRISMA 3T scanner at the Imaging Center for Brain Research at Beijing Normal University during the baseline recruitment and at follow-up several years later. Participants were in a supine position with their heads snugly fixed by straps and foam pads to minimize head movement. The T1-weighted structural images were acquired using 3D magnetization-prepared rapid gradient echo sequences (192 sagittal slices, repetition time=2530 ms, echo time=2.27 ms, slice thickness=1 mm, flip angle=7°, and field of view=256 mm × 256 mm).\nThe MATLAB2021b [\nHigh-resolution T1 structural image data were processed using the Cat12 toolbox [\nThe intergroup variations between the DD and ODD groups were assessed through an independent samples\nThe mixed linear model (MLM) was used to examine the influence of the DD variable on the rate of cognitive aging at an individual level. Initially, we established the null model and unconditional growth model. The null model was used to determine the hierarchical structure of the longitudinal data for different cognitive functions, which was suitable for MLM analysis. The unconditional growth model was used to identify significant aging patterns in various cognitive functions over time. After selecting these 2 models, we constructed the full model that encompassed level 1 (described the individual cognitive level aging patterns) and level 2 (investigated the influence of the DD variable on the aging patterns of multiple cognitive abilities in individuals).\nLevel 1:\nLevel 2:\nThe Statistical Package for Social Science (version 25.0; IBM Corp) was used for descriptive statistics and difference analysis. The Mplus (version 8.3) software was used to build an MLM for each domain of cognition. An α of .05 was applied to indicate statistical significance.\nWe used a 2-sample\nA scheme of multivoxel pattern analysis based on support vector machines (SVM) was used to constrain the brain regions representing the group differences of the DD and predicting the individual’s cognitive performance. The whole statistical framework is illustrated in\nBased on this, we calculated the annual decline rate of gray matter volume (GMV) for each participant at the voxel level and region level (Anatomical Automatic Labeling template). The maps of decline rate between the DD group and ODD group were analyzed using 2-sample\nDPABI software was used for the analysis. An independent samples\nThe study was conducted in accordance with the institutional review board at the Imaging Center for Brain Research at Beijing Normal University (ICBIR_A_0041_002_02) and was approved in March 2015, with waived requirements for additional registration given its observational design and use of anonymized archival data. The protocol was approved by the ethics committee of the State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University. We used STROBE (Strengthening the Reporting of Observational Studies in Epidemiology) as our reporting framework. Written informed consent and sociodemographic information were obtained from the participants before initiating the neuropsychological tests. All participants were reimbursed with daily necessities valued at 20 RMB (approximately US $10) and provided with a free screening report covering multiple domains of cognition as a token of appreciation.\nDue to the demographic information being controlled, an independent sample\nCompared with the DD group, the ODD group showed significantly greater GMV in various brain regions in both hemispheres, mainly located in 4 clusters (\nTo find out the regions that could effectively represent the features of overcoming the DD, we constructed a statistical framework (\nBased on the aforementioned framework, we also developed a statistical model to analyze the cognitive representation of these regions (\nSignificant interindividual variations were observed for all cognitive abilities in the null model, suggesting the viability of constructing the subsequent MLM (Table S5 in\nThe groups did not differ significantly at baseline in terms of demographic variables, as they were matched. Fewer participants were followed up longitudinally. Thus, we did the independent\nTo investigate the influence of the DD on structural aging in older adults, the annual aging rate of GMV was calculated for each voxel (see the Statistical Analysis section). We generated brain maps depicting the GMV decline rate for each individual and identified brain regions showing significant group differences in decline rates. The findings revealed that the rate of GMV decline in the middle frontal gyrus (MFG) was notably lower in the ODD group than in the DD group (BA=46; X=−38, Y=53, Z=11; cluster size=719, peak\nThrough the utilization of the searchlight technique in cross-sectional data and integrating it with a cross-validation classification prediction model, this study more precisely constrains the distinct brain regions between the DD and ODD groups. This research not only pinpoints the primary brain regions that exhibit the greatest accuracy in discerning differences between the 2 groups but also delineates how the structural features extracted from these brain regions represent individual cognitive performance. Furthermore, our findings suggest a long-term impact of the DD on brain structure. Specifically, the MFG may exhibit a faster rate of aging related to the episodic memory alternations attributable to the DD.\nThrough traditional intergroup comparisons, this study observed that the GMV of the ODD group exhibited significant advantages over the DD group in several brain regions, including the fusiform gyrus, hippocampus, parahippocampal gyrus, temporal pole, superior temporal sulcus, and orbitofrontal region. Following the regional screening constraints within the statistical framework of this study, the brain regions capable of more accurately distinguishing between the DD group and ODD group were limited to the fusiform gyrus, hippocampus, parahippocampal gyrus, and a segment of the superior temporal sulcus, while the excluded regions were predominantly concentrated in the VMOF region. The VMOF area is considered a component of the reward circuit [\nThe Introduction section of this study mentioned a theory suggesting that older adults may experience reduced self-control due to aging of the frontal lobe, which could impede their cognitive benefits from ICTs. However, based on the above results, we do not support this view because the VMOF brain region related to self-control behavior in the frontal lobe shows a low correlation with ICT usage [\nThe GMV features of each voxel in the aforementioned brain regions are considered effective in distinguishing structural variances between the DD and ODD groups. The rationale for the discriminative capability of these voxels may stem from cognitive-behavioral distinctions linked to their features and the conditions of the DD and ODD groups. Findings from this research indicate that more than 80% of the voxels in the delineating brain regions can predict executive function, with processing speed following closely. These results align with behavioral outcomes, highlighting the predictive capacity of gray matter structural traits in differing cognitive functions between the 2 groups. Notably, brain regions exhibiting relatively strong predictive efficacy encompass the temporal pole, parahippocampal gyrus, and hippocampus. These brain regions are all associated with a range of neurodegenerative diseases and may explain the cognitive impairments found in these diseases.\nThe temporal pole plays a significant role in the initial pathological progression observed in Alzheimer disease [\nThe absence of the hippocampus in predicting individuals’ memory performance might appear puzzling at first. However, in this study, this result is reasonable because the computational framework of this study distinguishes the DD and ODD groups based on the structural characteristics of each voxel obtained, and memory is not included in characterizing the cognitive performance differences between the 2 groups. As a result, the prediction of individual cognitive abilities based on brain structure features that differentiate these groups without involving memory aligns with the behavioral findings. Additionally, we adopted the searchlight technique, which is particularly effective for group differentiation [\nInitially, we might not instinctively correlate the MFG with memory-related functions. Upon conducting in-depth studies, it has been revealed that the MFG predominantly contributes to attentional functions [\nNevertheless, the corrected data within this study demonstrated that the MFG was linked to the decline in episodic memory, albeit its effect size might confer limited explanatory power (\nOur study can be further expanded to an alternative perspective, as mobile-device–based cognitive training has emerged as a crucial strategy to address the challenges that dementia poses for cognitive health [\nThe future development of cognitive decline prevention strategies through mobile devices presents a multifaceted opportunity for advancing personalized health care. For example, neuroplasticity-based cognitive training is an effective way to enhance cognitive function and prevent dementia in healthy older adults. With the spread of the internet and mobile devices, technological advances foster the swift development of computerized cognitive training (CCT), making it possible for older adults to access adaptive multidomain cognitive training in the community or even in their homes. CCT is a convenient and sustainable combination of existing cognitive training and ICTs. Nevertheless, our study revealed that when it comes to leveraging mobile devices to enhance the cognitive abilities of older adults, extensive cognitive engagement on their part might not be necessary. Instead, they merely passively absorb information and, to a certain extent, cooperate through finger movements, which can nonetheless facilitate cognitive improvement. This finding holds significant implications for the future design of more diverse forms of CCT.\nHowever, our study has certain limitations. The conclusion that the causal relationship between overcoming the DD and brain structural preservation remains inadequately established is appropriately considered. Although the longitudinal data provide valuable insights into the correlation between ICT use and slower decline in MFG volume, the study design cannot definitively demonstrate that ICT use is the causal factor. Alternative explanations, such as preexisting cognitive differences that might predispose certain individuals to adopt technology, require more thorough consideration. Therefore, we suggest that future research further explores this correlation.\nBased on the above findings and limitations, we recommend the following for future work. First, more rigorous experimental designs can be used to better understand the causal relationship between ICT use and cognitive improvement. Second, it is suggested to conduct further research to identify and control for potential confounding factors such as preexisting cognitive differences. Finally, the exploration of more diverse methods of cognitive enhancement through mobile devices can be expanded to develop more personalized and effective CCT programs.\nIn summary, while our study provides some evidence of the potential of mobile devices in enhancing the cognitive abilities of older adults, further research is needed to strengthen these conclusions and explore more effective strategies for preventing cognitive decline.\nAlthough this study provides critical insights into the DD’s health impacts, several limitations warrant consideration. First, our operationalization of digital engagement through a single self-reported frequency question (“How often do you use a computer and mobile devices?”) inherently simplifies the multidimensional nature of digital access. Although validated by national surveillance patterns and our prior cohort findings [\nThis study provides preliminary evidence that older adults who engage with digital technologies exhibit associations with preserved GMV in Alzheimer disease–vulnerable regions. Our cross-sectional analyses suggest structural differences in the fusiform gyrus, hippocampus, and parahippocampal gyrus between digitally engaged (ODD) and nonengaged (DD) groups, with these regions demonstrating predictive value for executive function and processing speed. Longitudinal observations indicate a correlation between technology use and slower GMV decline in the MFG, though the observational design precludes causal attribution. Although these findings highlight potential neurostructural correlates of digital engagement in aging, alternative explanations—such as preexisting cognitive advantages predisposing individuals to technology adoption—require systematic evaluation. Future intervention studies are needed to clarify whether targeted digital training can modulate GMV trajectories and whether such changes translate to clinically meaningful cognitive preservation.", "content_for_embedding": "Although mobile devices have become some of the most indispensable technologies in modern society, there is still a significant portion of the population that has never used them. This gap between individuals who are adept at using digital information and communication technologies (ICTs) and those who are not is referred to as the digital divide (DD) [\nThe theory of neural plasticity suggests that the structure of the human brain can undergo long-term changes in response to environmental stimuli and situational triggers [\nThe theory of frontal lobe control and the extended theory of internet addiction suggest that degenerated prefrontal cortical regions could lead to the dysfunction of cognitive control and inhibition, which is the potential physiological basis for problematic online behaviors [\nIn sum, the long-term implications of addressing the DD among the older adult population, whether positive or negative, are yet to be substantiated.\nTo ascertain the long-term implications of addressing the DD among the older population, this study, using a large-sample neuroimaging cohort, will investigate the following: (1) the differences in brain structure and cognitive performance between individuals who overcome the DD and those who do not, (2) whether the identified brain regions that classify the 2 groups will also predict their cognitive performance, and (3) the longitudinal alterations of the aging rate in cognitive function and brain structure caused by the DD.\nA relationship may exist between the longitudinal changes in cognitive function and brain structure observed in aging populations and the existence of a DD.\nThe study population was derived from the Beijing Aging Brain Rejuvenation Initiative, a longitudinal neuroimaging initiative investigating aging-related cognitive trajectories. Participants were not prospectively recruited for this specific analysis but constituted a subset of individuals meeting predefined inclusion criteria. Our samples were community-based. They were all recruited voluntarily. The majority of them live in different communities in Beijing. Established at Beijing Normal University in 2008, the Beijing Aging Brain Rejuvenation Initiative has conducted cohort studies based on the registry of a large community population in the greater metropolitan area of Beijing. All of the participants were 50 years or above at the time of baseline enrollment, capable of living independently, without nervous system diseases or psychiatric disorders, with no metal implants or any other contraindications for undergoing magnetic resonance imaging (MRI) within the body, with 6 or more years of formal education. A total of 3380 participants with MRI data were recruited; participants with a Mini-Mental State Examination score below 24 and without the data to quantify the DD were excluded. As a result, only 1400 participants were recruited in this study. Of 1400 participants, 1280 were included in this study based on propensity score matching (PSM). All of the participants who were registered were revisited every 2-3 years. Throughout the follow-up period, factors like bodily metal implants, severe ailments, loss to follow-up, or participant refusal led to only 689 individuals undergoing a subsequent MRI scan (see Figure S1 in\nAccording to the cross-sectional data, we divided the participants into the DD group and overcoming DD (ODD) group based on the quantification of the DD (see the Measurements section). Power calculations were conducted a priori to ensure adequate statistical sensitivity. For the primary comparison between the digital engagement groups (DD vs ODD), we estimated required sample sizes using a 2-sample\nThis calculation guided our longitudinal tracking cohort design (n=350 per group) and cross-sectional neuroimaging subsample (n=700 per group). The effect size threshold (\nThe quantification indicator of the DD involves an item about the frequency of using ICTs from the Leisure Activity Scale: “How often do you use a computer and mobile devices?” We classified individuals with scores of 0 (never), 1 (≥once per year), and 2 (≥once per month) into the DD group and individuals with scores of 4 (≥once per week) and 5 (everyday) into the ODD group [\nIn the longitudinal data analysis, participants for whom the use of ICTs could be tracked over time were categorized into either the DD group or the ODD group. We excluded participants who transitioned their status of using ICTs because crossing the DD is a relatively stable state that would not change in the short term. If a transition occurred, it may be due to uncontrollable external factors, which are not the focus of this study.\nAs described in our previous study, all participants underwent a battery of neuropsychological tests at baseline recruitment [\nWe conducted a confirmatory factor analysis analysis for the performance of each cognitive domain (Figure S2 in\nMRI data were acquired using a SIEMENS PRISMA 3T scanner at the Imaging Center for Brain Research at Beijing Normal University during the baseline recruitment and at follow-up several years later. Participants were in a supine position with their heads snugly fixed by straps and foam pads to minimize head movement. The T1-weighted structural images were acquired using 3D magnetization-prepared rapid gradient echo sequences (192 sagittal slices, repetition time=2530 ms, echo time=2.27 ms, slice thickness=1 mm, flip angle=7°, and field of view=256 mm × 256 mm).\nThe MATLAB2021b [\nHigh-resolution T1 structural image data were processed using the Cat12 toolbox [\nThe intergroup variations between the DD and ODD groups were assessed through an independent samples\nThe mixed linear model (MLM) was used to examine the influence of the DD variable on the rate of cognitive aging at an individual level. Initially, we established the null model and unconditional growth model. The null model was used to determine the hierarchical structure of the longitudinal data for different cognitive functions, which was suitable for MLM analysis. The unconditional growth model was used to identify significant aging patterns in various cognitive functions over time. After selecting these 2 models, we constructed the full model that encompassed level 1 (described the individual cognitive level aging patterns) and level 2 (investigated the influence of the DD variable on the aging patterns of multiple cognitive abilities in individuals).\nLevel 1:\nLevel 2:\nThe Statistical Package for Social Science (version 25.0; IBM Corp) was used for descriptive statistics and difference analysis. The Mplus (version 8.3) software was used to build an MLM for each domain of cognition. An α of .05 was applied to indicate statistical significance.\nWe used a 2-sample\nA scheme of multivoxel pattern analysis based on support vector machines (SVM) was used to constrain the brain regions representing the group differences of the DD and predicting the individual’s cognitive performance. The whole statistical framework is illustrated in\nBased on this, we calculated the annual decline rate of gray matter volume (GMV) for each participant at the voxel level and region level (Anatomical Automatic Labeling template). The maps of decline rate between the DD group and ODD group were analyzed using 2-sample\nDPABI software was used for the analysis. An independent samples\nThe study was conducted in accordance with the institutional review board at the Imaging Center for Brain Research at Beijing Normal University (ICBIR_A_0041_002_02) and was approved in March 2015, with waived requirements for additional registration given its observational design and use of anonymized archival data. The protocol was approved by the ethics committee of the State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University. We used STROBE (Strengthening the Reporting of Observational Studies in Epidemiology) as our reporting framework. Written informed consent and sociodemographic information were obtained from the participants before initiating the neuropsychological tests. All participants were reimbursed with daily necessities valued at 20 RMB (approximately US $10) and provided with a free screening report covering multiple domains of cognition as a token of appreciation.\nDue to the demographic information being controlled, an independent sample\nCompared with the DD group, the ODD group showed significantly greater GMV in various brain regions in both hemispheres, mainly located in 4 clusters (\nTo find out the regions that could effectively represent the features of overcoming the DD, we constructed a statistical framework (\nBased on the aforementioned framework, we also developed a statistical model to analyze the cognitive representation of these regions (\nSignificant interindividual variations were observed for all cognitive abilities in the null model, suggesting the viability of constructing the subsequent MLM (Table S5 in\nThe groups did not differ significantly at baseline in terms of demographic variables, as they were matched. Fewer participants were followed up longitudinally. Thus, we did the independent\nTo investigate the influence of the DD on structural aging in older adults, the annual aging rate of GMV was calculated for each voxel (see the Statistical Analysis section). We generated brain maps depicting the GMV decline rate for each individual and identified brain regions showing significant group differences in decline rates. The findings revealed that the rate of GMV decline in the middle frontal gyrus (MFG) was notably lower in the ODD group than in the DD group (BA=46; X=−38, Y=53, Z=11; cluster size=719, peak\nThrough the utilization of the searchlight technique in cross-sectional data and integrating it with a cross-validation classification prediction model, this study more precisely constrains the distinct brain regions between the DD and ODD groups. This research not only pinpoints the primary brain regions that exhibit the greatest accuracy in discerning differences between the 2 groups but also delineates how the structural features extracted from these brain regions represent individual cognitive performance. Furthermore, our findings suggest a long-term impact of the DD on brain structure. Specifically, the MFG may exhibit a faster rate of aging related to the episodic memory alternations attributable to the DD.\nThrough traditional intergroup comparisons, this study observed that the GMV of the ODD group exhibited significant advantages over the DD group in several brain regions, including the fusiform gyrus, hippocampus, parahippocampal gyrus, temporal pole, superior temporal sulcus, and orbitofrontal region. Following the regional screening constraints within the statistical framework of this study, the brain regions capable of more accurately distinguishing between the DD group and ODD group were limited to the fusiform gyrus, hippocampus, parahippocampal gyrus, and a segment of the superior temporal sulcus, while the excluded regions were predominantly concentrated in the VMOF region. The VMOF area is considered a component of the reward circuit [\nThe Introduction section of this study mentioned a theory suggesting that older adults may experience reduced self-control due to aging of the frontal lobe, which could impede their cognitive benefits from ICTs. However, based on the above results, we do not support this view because the VMOF brain region related to self-control behavior in the frontal lobe shows a low correlation with ICT usage [\nThe GMV features of each voxel in the aforementioned brain regions are considered effective in distinguishing structural variances between the DD and ODD groups. The rationale for the discriminative capability of these voxels may stem from cognitive-behavioral distinctions linked to their features and the conditions of the DD and ODD groups. Findings from this research indicate that more than 80% of the voxels in the delineating brain regions can predict executive function, with processing speed following closely. These results align with behavioral outcomes, highlighting the predictive capacity of gray matter structural traits in differing cognitive functions between the 2 groups. Notably, brain regions exhibiting relatively strong predictive efficacy encompass the temporal pole, parahippocampal gyrus, and hippocampus. These brain regions are all associated with a range of neurodegenerative diseases and may explain the cognitive impairments found in these diseases.\nThe temporal pole plays a significant role in the initial pathological progression observed in Alzheimer disease [\nThe absence of the hippocampus in predicting individuals’ memory performance might appear puzzling at first. However, in this study, this result is reasonable because the computational framework of this study distinguishes the DD and ODD groups based on the structural characteristics of each voxel obtained, and memory is not included in characterizing the cognitive performance differences between the 2 groups. As a result, the prediction of individual cognitive abilities based on brain structure features that differentiate these groups without involving memory aligns with the behavioral findings. Additionally, we adopted the searchlight technique, which is particularly effective for group differentiation [\nInitially, we might not instinctively correlate the MFG with memory-related functions. Upon conducting in-depth studies, it has been revealed that the MFG predominantly contributes to attentional functions [\nNevertheless, the corrected data within this study demonstrated that the MFG was linked to the decline in episodic memory, albeit its effect size might confer limited explanatory power (\nOur study can be further expanded to an alternative perspective, as mobile-device–based cognitive training has emerged as a crucial strategy to address the challenges that dementia poses for cognitive health [\nThe future development of cognitive decline prevention strategies through mobile devices presents a multifaceted opportunity for advancing personalized health care. For example, neuroplasticity-based cognitive training is an effective way to enhance cognitive function and prevent dementia in healthy older adults. With the spread of the internet and mobile devices, technological advances foster the swift development of computerized cognitive training (CCT), making it possible for older adults to access adaptive multidomain cognitive training in the community or even in their homes. CCT is a convenient and sustainable combination of existing cognitive training and ICTs. Nevertheless, our study revealed that when it comes to leveraging mobile devices to enhance the cognitive abilities of older adults, extensive cognitive engagement on their part might not be necessary. Instead, they merely passively absorb information and, to a certain extent, cooperate through finger movements, which can nonetheless facilitate cognitive improvement. This finding holds significant implications for the future design of more diverse forms of CCT.\nHowever, our study has certain limitations. The conclusion that the causal relationship between overcoming the DD and brain structural preservation remains inadequately established is appropriately considered. Although the longitudinal data provide valuable insights into the correlation between ICT use and slower decline in MFG volume, the study design cannot definitively demonstrate that ICT use is the causal factor. Alternative explanations, such as preexisting cognitive differences that might predispose certain individuals to adopt technology, require more thorough consideration. Therefore, we suggest that future research further explores this correlation.\nBased on the above findings and limitations, we recommend the following for future work. First, more rigorous experimental designs can be used to better understand the causal relationship between ICT use and cognitive improvement. Second, it is suggested to conduct further research to identify and control for potential confounding factors such as preexisting cognitive differences. Finally, the exploration of more diverse methods of cognitive enhancement through mobile devices can be expanded to develop more personalized and effective CCT programs.\nIn summary, while our study provides some evidence of the potential of mobile devices in enhancing the cognitive abilities of older adults, further research is needed to strengthen these conclusions and explore more effective strategies for preventing cognitive decline.\nAlthough this study provides critical insights into the DD’s health impacts, several limitations warrant consideration. First, our operationalization of digital engagement through a single self-reported frequency question (“How often do you use a computer and mobile devices?”) inherently simplifies the multidimensional nature of digital access. Although validated by national surveillance patterns and our prior cohort findings [\nThis study provides preliminary evidence that older adults who engage with digital technologies exhibit associations with preserved GMV in Alzheimer disease–vulnerable regions. Our cross-sectional analyses suggest structural differences in the fusiform gyrus, hippocampus, and parahippocampal gyrus between digitally engaged (ODD) and nonengaged (DD) groups, with these regions demonstrating predictive value for executive function and processing speed. Longitudinal observations indicate a correlation between technology use and slower GMV decline in the MFG, though the observational design precludes causal attribution. Although these findings highlight potential neurostructural correlates of digital engagement in aging, alternative explanations—such as preexisting cognitive advantages predisposing individuals to technology adoption—require systematic evaluation. Future intervention studies are needed to clarify whether targeted digital training can modulate GMV trajectories and whether such changes translate to clinically meaningful cognitive preservation.", "topic": "Brain"}
{"pmid": "39601433", "pmcid": "12309442", "title": "Third‐order self‐embedded vocal motifs in wild orangutans, and the selective evolution of recursion", "publication_year": "N/A", "abstract": "Recursion, the neuro‐computational operation of nesting a signal or pattern within itself, lies at the structural basis of language. Classically considered absent in the vocal repertoires of nonhuman animals, whether recursion evolved step‐by‐step or saltationally in humans is among the most fervent debates in cognitive science since Chomsky's seminal work on syntax in the 1950s. The recent discovery of self‐embedded vocal motifs in wild (nonhuman) great apes—Bornean male orangutans’ long calls—lends initial but important support to the notion that recursion, or at least temporal recursion, is not uniquely human among hominids and that its evolution was based on shared ancestry. Building on these findings, we test four necessary predictions for a gradual evolutionary scenario in wild Sumatran female orangutans’ alarm calls, the longest known combinations of consonant‐like and vowel‐like calls among great apes (excepting humans). From the data, we propose third‐order self‐embedded isochrony: three hierarchical levels of nested isochronous combinatoric units, with each level exhibiting unique variation dynamics and information content relative to context. Our findings confirm that recursive operations underpin great ape call combinatorics, operations that likely evolved gradually in the human lineage as vocal sequences became longer and more intricate.", "full_text": "Language defines being human, but its evolution defies scientific explanation. Features once believed to be unique to language have been found to be shared with animal communication,\nTo assess the recursive capacities of (nonhuman) animals, artificial grammar experiments using humanmade tokens and rules have shown that other species can learn to perceive and process recursive structures after receiving dedicated training\nRecently, the first evidence for recursive operations has, however, emerged in great ape call combinations, suggesting that these operations may have been hiding in plain sight. Lameira and colleagues\nRecursive self‐embedded isochrony in Bornean male orangutan loud calls, as originally described by Lameira et al.\nTempo differences across levels for alarm calls given in response to natural (light pink) and non‐natural (pink) predators. From left to right: tempo of combinations, bouts, and series of alarm calls. * Denotes\nThe findings by Lameira et al.\nIf recursive operations were indeed available in the vocal toolkit of ancestral hominids and were evolutionarily relevant for the emergence of language, then four obligatory key predictions would need to be fulfilled. First, recursive operations among nonhuman hominids should not be restricted to one species or one location. Only then would recursive operations be sufficiently resilient to stochastic demographic variations, ecologic vagaries, and/or extinction events,\nSecond, recursive operations should not be restricted to one sex, given that ultimately any individual ought to be able of deploying recursion.\nThird, recursive operations should not be restricted to one vocal behavior. Notably, to support the view that the use of recursive operations in ancient hominids would have been pertinent for language evolution, they ought to underlie different types of consonant‐like and vowel‐like calls, and combinations thereof, given that these eventually became the two elemental building blocks of the primary medium of language.\nFourth, recursive operations should not be restricted to one context. They ought to occur across, be affected by, and help predict, different external objects or events. Recursive operations across contexts would be imperative for functional (i.e., informative) use, given that only then could recursive operations begin to offer fitness benefits, setting selective forces in motion. Environmental responsiveness and context‐sensitivity would, thus, be a requirement if recursive operations were to ultimately operate on elements encoding semantic information (e.g., words).\nNote that these conditions, while essential for a gradual evolutionary pathway, inherently counter possible saltationist arguments. A single mutational event capable of engendering a major transformation in an ancestral hominid would be likely confined to a single species, location, vocal behavior, or context (had individuals hypothetically remained reproductively unaffected after such an event). Such a limitation would constrain mutation persistence, spread, and fixation within and across populations.\nGiven that initial proof for the presence of recursive operations exists for Bornean male orangutans’ long calls used for sexual advertisement,\nWe tested seven wild adult female orangutans, habituated to human presence, who represented all adult female local residents of the Ketambe forest (3°41′N, 97°39′E) in Aceh, Sumatra, Indonesia, between 2010 and 2011, comprising about 450 hectares of lowland and hill dipterocarp rainforest. Subjects were presented with four predator models, consisting of a human experimenter walking on all fours on the forest ground draped over a sheet with one of four different types of print: tiger patterned (orangutans’ natural predator in Sumatra\nSpectrograms were visually inspected in Raven Pro\nTo evaluate the rhythmic structure of orangutan alarm calls, we considered three levels of analysis: (1) combination level: calls separated by 0.2 s or less were considered to represent a combination; (2) bout level: combinations separated by more than 0.2 and less than 2 s were considered to represent a bout; (3) series level: bouts separated by more than 2 and less than 20 s were considered to represent a series. When a silent gap between two calls was over 20 s, calls were considered to belong to different series. From 14 recordings, we obtained 48 different series.\nFor each level of analysis, we calculated the inter‐onset interval duration (aka: IOI, hereafter\nTo investigate the occurrence of rhythmic categories, we divided the ratio distribution into on‐integer and off‐integer ratio ranges, centering the on‐integer ratio range around 1:1 (or 0.50—representing isochrony), 1:2 (or 0.33), 1:3 (or 0.25), 2:1 (or 0.66). Following previous studies,\nThe range of a 1:3 integer ratio was considered when\nWe used nine generalized linear mixed models (GLMMs) using the\nTo examine the potential influence of context on each level's tempo, we used an LMM per level to assess differences in\nFor the combination level, we ran a model using as a response variable the\nWe created two subsets of\nFor the bout level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nFor the series level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nThe average\nRecursive self‐embedded isochrony in Sumatran female alarm calls. Findings represent a case of third‐order self‐embedded isochrony underpinned by applying a recursive operation twice. Dark blue blocks represent call units of eight different possible call types. Light‐blue blocks containing dark squares represent bouts. Light gray blocks containing light‐blue blocks represent series. Orange lines between blocks represent silent intervals. Red lines represent inter‐onset\n\nThe visual inspection of the density distribution suggested the presence of a 1:1 ratio (isochrony) for the combination level (Figure\nWe found that the temporal organization of alarm calls within a combination was isochronous (1:1 off vs. 1:1 on; estimate = −0.345, SE = 0.108, z‐ratio = −3.186,\nThe temporal organization of consecutive bouts of alarm calls was overall isochronous (1:1 off vs. 1:1 on; estimate = −0.816, SE = 0.227, z‐ratio = −3.599,\nThe density distribution showed a bimodal distribution (Figure\nWe found that consecutive bouts of alarm calls were isochronous (1:1 on vs. 1:1 off; estimate = 0.429, SE = 0.156, z‐ratio = 2.753,\nOur results demonstrate the presence of recursive self‐embedded vocal motifs organized across three hierarchical strata in the alarm calls of female Sumatran orangutans. These results expand on previous findings in the long calls of male Bornean orangutans, in direct agreement with the predictions defined at the start of the study for the gradual evolution of recursion. Together, these two converging lines of evidence show that recursive vocal operations were likely present in ancient hominids across taxa and geography, likely deployed by males and females, and dynamically used to encode information about external objects and events in various call types. These conditions would have been necessary and sufficient to allow natural selection processes to target recursive vocal combinatorics based on their proximate functions, information content, and communication benefits.\nOur findings provide, to our knowledge, the first empirical support to the view that ever more powerful recursive capacities could have been selected for and evolved incrementally. In one context, individuals invoked a recursive\nOur results revealed differences in temporal organization across levels in natural versus non‐natural predator models. Notably, the predator model type affected a combination isochrony and bout tempo. At the bout level, this result is in line with other instances in the animal world where alarm calls\nBoth instances of predator information encoding based on on/off‐isochrony and variation in isochrony tempo support the view that this recursive system has evolved for, and facilitates, effective communication, which ought to be expected to be at a selective premium among dispersed social organizations, as is characteristic for wild orangutans.\nOur findings reveal for the first time, to our knowledge, the presence of an isochronous temporal organization at a large timescale, a regular tempo with >10 s between each sound onset. Previous work on primate vocal rhythms has focused primarily on temporal intervals with durations of <5 seconds.\nOur study was purely observational, but we can offer two possible proximate reasons. Using recurring silence gaps that are at least 10 s long during an alarm call response is likely to allow individuals to systematically monitor and gather information from their environment, being it about the presence or movement of the predator they have just encountered or possible nearby conspecifics. Using isochrony at this scale would then allow individuals to adjust their ensuing antipredator and general behaviors accordingly, something difficult to achieve with recurring gaps shorter by one or two orders of magnitude (i.e., at first‐ and second‐order isochrony). Second, third‐order isochrony with >10 s gaps could also represent a byproduct of optimal resting times between series of alarm calls.\nThere are multiple examples of patterns in music with intervals equally as lengthy as orangutans’ alarm call series, including Bolero's ostinatos, “cycles” in African and Indian music, isorhythm in medieval music, rondo in classical music, and passacaglia and chaconne in Baroque music. Because rhythm tempos progress in these instances over cycles of >10 s, they can only emerge and be detected as extended compositions that surpass the length of typical ordinary speech exchanges and typical laboratory stimuli. This could begin to explain why this temporal scale of organization has not, and cannot often be, investigated. Our results invite a “zoom out” of the empirical window for the study of acoustic communication and vocal rhythms in long sequences across taxa, including humans (e.g., speeches). Nonetheless, the parallel with music is limited. Long‐cycle musical motifs unfold over several bars and (therein) beats, thus, the ratios between these levels must be necessarily even integers. This differs from our observation in wild orangutans, where the three layers of isochrony were related by the odd ratio 2:9:67.\nEncoding information in the temporal structure across hierarchical layers supports the view that, once present, recursive assembly operations could provide proximate benefits, multiplying the amount of information that a message may encode and diversifying how the same message can be encoded. However, most signal systems in nature are, and have been, also shaped by similar benefits and selective forces, and yet, the emergence of different hierarchical layers has not ensued in most cases. Why recursive operations have only been detected in a hominid remains, thus, unclear. A possible answer could rest on the size and scale of the hominid brain. Human\nClassically, primate vocal output is seen as a sonic cast of its vocal anatomy.\nC.D.G. and A.R.L.: Conceptualization. C.D.G.: Methodology. C.D.G.: Investigation. A.R.L. and M.G.: Supervision. C.D.G. and A.R.L.: Writing—original draft. C.D.G., A.R.L., and M.G.: Writing—review and editing. C.D.G. and A.R.L.: Visualization. All authors contributed to the article and approved the submitted version.\nThe authors declare no competing interests.\nThe peer review history for this article is available at:", "content_for_embedding": "Language defines being human, but its evolution defies scientific explanation. Features once believed to be unique to language have been found to be shared with animal communication,\nTo assess the recursive capacities of (nonhuman) animals, artificial grammar experiments using humanmade tokens and rules have shown that other species can learn to perceive and process recursive structures after receiving dedicated training\nRecently, the first evidence for recursive operations has, however, emerged in great ape call combinations, suggesting that these operations may have been hiding in plain sight. Lameira and colleagues\nRecursive self‐embedded isochrony in Bornean male orangutan loud calls, as originally described by Lameira et al.\nTempo differences across levels for alarm calls given in response to natural (light pink) and non‐natural (pink) predators. From left to right: tempo of combinations, bouts, and series of alarm calls. * Denotes\nThe findings by Lameira et al.\nIf recursive operations were indeed available in the vocal toolkit of ancestral hominids and were evolutionarily relevant for the emergence of language, then four obligatory key predictions would need to be fulfilled. First, recursive operations among nonhuman hominids should not be restricted to one species or one location. Only then would recursive operations be sufficiently resilient to stochastic demographic variations, ecologic vagaries, and/or extinction events,\nSecond, recursive operations should not be restricted to one sex, given that ultimately any individual ought to be able of deploying recursion.\nThird, recursive operations should not be restricted to one vocal behavior. Notably, to support the view that the use of recursive operations in ancient hominids would have been pertinent for language evolution, they ought to underlie different types of consonant‐like and vowel‐like calls, and combinations thereof, given that these eventually became the two elemental building blocks of the primary medium of language.\nFourth, recursive operations should not be restricted to one context. They ought to occur across, be affected by, and help predict, different external objects or events. Recursive operations across contexts would be imperative for functional (i.e., informative) use, given that only then could recursive operations begin to offer fitness benefits, setting selective forces in motion. Environmental responsiveness and context‐sensitivity would, thus, be a requirement if recursive operations were to ultimately operate on elements encoding semantic information (e.g., words).\nNote that these conditions, while essential for a gradual evolutionary pathway, inherently counter possible saltationist arguments. A single mutational event capable of engendering a major transformation in an ancestral hominid would be likely confined to a single species, location, vocal behavior, or context (had individuals hypothetically remained reproductively unaffected after such an event). Such a limitation would constrain mutation persistence, spread, and fixation within and across populations.\nGiven that initial proof for the presence of recursive operations exists for Bornean male orangutans’ long calls used for sexual advertisement,\nWe tested seven wild adult female orangutans, habituated to human presence, who represented all adult female local residents of the Ketambe forest (3°41′N, 97°39′E) in Aceh, Sumatra, Indonesia, between 2010 and 2011, comprising about 450 hectares of lowland and hill dipterocarp rainforest. Subjects were presented with four predator models, consisting of a human experimenter walking on all fours on the forest ground draped over a sheet with one of four different types of print: tiger patterned (orangutans’ natural predator in Sumatra\nSpectrograms were visually inspected in Raven Pro\nTo evaluate the rhythmic structure of orangutan alarm calls, we considered three levels of analysis: (1) combination level: calls separated by 0.2 s or less were considered to represent a combination; (2) bout level: combinations separated by more than 0.2 and less than 2 s were considered to represent a bout; (3) series level: bouts separated by more than 2 and less than 20 s were considered to represent a series. When a silent gap between two calls was over 20 s, calls were considered to belong to different series. From 14 recordings, we obtained 48 different series.\nFor each level of analysis, we calculated the inter‐onset interval duration (aka: IOI, hereafter\nTo investigate the occurrence of rhythmic categories, we divided the ratio distribution into on‐integer and off‐integer ratio ranges, centering the on‐integer ratio range around 1:1 (or 0.50—representing isochrony), 1:2 (or 0.33), 1:3 (or 0.25), 2:1 (or 0.66). Following previous studies,\nThe range of a 1:3 integer ratio was considered when\nWe used nine generalized linear mixed models (GLMMs) using the\nTo examine the potential influence of context on each level's tempo, we used an LMM per level to assess differences in\nFor the combination level, we ran a model using as a response variable the\nWe created two subsets of\nFor the bout level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nFor the series level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nThe average\nRecursive self‐embedded isochrony in Sumatran female alarm calls. Findings represent a case of third‐order self‐embedded isochrony underpinned by applying a recursive operation twice. Dark blue blocks represent call units of eight different possible call types. Light‐blue blocks containing dark squares represent bouts. Light gray blocks containing light‐blue blocks represent series. Orange lines between blocks represent silent intervals. Red lines represent inter‐onset\n\nThe visual inspection of the density distribution suggested the presence of a 1:1 ratio (isochrony) for the combination level (Figure\nWe found that the temporal organization of alarm calls within a combination was isochronous (1:1 off vs. 1:1 on; estimate = −0.345, SE = 0.108, z‐ratio = −3.186,\nThe temporal organization of consecutive bouts of alarm calls was overall isochronous (1:1 off vs. 1:1 on; estimate = −0.816, SE = 0.227, z‐ratio = −3.599,\nThe density distribution showed a bimodal distribution (Figure\nWe found that consecutive bouts of alarm calls were isochronous (1:1 on vs. 1:1 off; estimate = 0.429, SE = 0.156, z‐ratio = 2.753,\nOur results demonstrate the presence of recursive self‐embedded vocal motifs organized across three hierarchical strata in the alarm calls of female Sumatran orangutans. These results expand on previous findings in the long calls of male Bornean orangutans, in direct agreement with the predictions defined at the start of the study for the gradual evolution of recursion. Together, these two converging lines of evidence show that recursive vocal operations were likely present in ancient hominids across taxa and geography, likely deployed by males and females, and dynamically used to encode information about external objects and events in various call types. These conditions would have been necessary and sufficient to allow natural selection processes to target recursive vocal combinatorics based on their proximate functions, information content, and communication benefits.\nOur findings provide, to our knowledge, the first empirical support to the view that ever more powerful recursive capacities could have been selected for and evolved incrementally. In one context, individuals invoked a recursive\nOur results revealed differences in temporal organization across levels in natural versus non‐natural predator models. Notably, the predator model type affected a combination isochrony and bout tempo. At the bout level, this result is in line with other instances in the animal world where alarm calls\nBoth instances of predator information encoding based on on/off‐isochrony and variation in isochrony tempo support the view that this recursive system has evolved for, and facilitates, effective communication, which ought to be expected to be at a selective premium among dispersed social organizations, as is characteristic for wild orangutans.\nOur findings reveal for the first time, to our knowledge, the presence of an isochronous temporal organization at a large timescale, a regular tempo with >10 s between each sound onset. Previous work on primate vocal rhythms has focused primarily on temporal intervals with durations of <5 seconds.\nOur study was purely observational, but we can offer two possible proximate reasons. Using recurring silence gaps that are at least 10 s long during an alarm call response is likely to allow individuals to systematically monitor and gather information from their environment, being it about the presence or movement of the predator they have just encountered or possible nearby conspecifics. Using isochrony at this scale would then allow individuals to adjust their ensuing antipredator and general behaviors accordingly, something difficult to achieve with recurring gaps shorter by one or two orders of magnitude (i.e., at first‐ and second‐order isochrony). Second, third‐order isochrony with >10 s gaps could also represent a byproduct of optimal resting times between series of alarm calls.\nThere are multiple examples of patterns in music with intervals equally as lengthy as orangutans’ alarm call series, including Bolero's ostinatos, “cycles” in African and Indian music, isorhythm in medieval music, rondo in classical music, and passacaglia and chaconne in Baroque music. Because rhythm tempos progress in these instances over cycles of >10 s, they can only emerge and be detected as extended compositions that surpass the length of typical ordinary speech exchanges and typical laboratory stimuli. This could begin to explain why this temporal scale of organization has not, and cannot often be, investigated. Our results invite a “zoom out” of the empirical window for the study of acoustic communication and vocal rhythms in long sequences across taxa, including humans (e.g., speeches). Nonetheless, the parallel with music is limited. Long‐cycle musical motifs unfold over several bars and (therein) beats, thus, the ratios between these levels must be necessarily even integers. This differs from our observation in wild orangutans, where the three layers of isochrony were related by the odd ratio 2:9:67.\nEncoding information in the temporal structure across hierarchical layers supports the view that, once present, recursive assembly operations could provide proximate benefits, multiplying the amount of information that a message may encode and diversifying how the same message can be encoded. However, most signal systems in nature are, and have been, also shaped by similar benefits and selective forces, and yet, the emergence of different hierarchical layers has not ensued in most cases. Why recursive operations have only been detected in a hominid remains, thus, unclear. A possible answer could rest on the size and scale of the hominid brain. Human\nClassically, primate vocal output is seen as a sonic cast of its vocal anatomy.\nC.D.G. and A.R.L.: Conceptualization. C.D.G.: Methodology. C.D.G.: Investigation. A.R.L. and M.G.: Supervision. C.D.G. and A.R.L.: Writing—original draft. C.D.G., A.R.L., and M.G.: Writing—review and editing. C.D.G. and A.R.L.: Visualization. All authors contributed to the article and approved the submitted version.\nThe authors declare no competing interests.\nThe peer review history for this article is available at:", "topic": "Brain"}
{"pmid": "39540383", "pmcid": "12308364", "title": "The Slit–Robo signalling pathway in nervous system development: a comparative perspective from vertebrates and invertebrates", "publication_year": "2025", "abstract": "During nervous system development, growing axons find their targets with the help of guidance cues. These cues, which can be secreted molecules provided by neighbouring cells or transmembrane proteins mediating cell–cell contacts with the growing axons, act as either chemoattractants or chemorepellents. Over the last decades, several axon guidance molecules have been identified. One of the classical guidance cues is the Slit protein. Slit is a secreted protein, initially identified in a genetic screen in the fruit fly", "full_text": "A key event in our comprehension of axonal guidance and cell migration during development came with the discovery of Slit proteins. Slit was first detected in the fly embryonic midline glial cells of the ventral nerve cord (VNC), from where it is released to the extracellular space and distributed along axon tracts. Slit is a member of the protein family containing EGF-like repeats [\nThe discovery of Robo came out of a\nSubsequent research revealed Slit as a ligand for the Robo receptor in both vertebrates and\nSlit is an extracellular matrix-secreted glycoprotein [\nAll Slit proteins share a common structural framework (\nSchematic representation of Slit and Robo structure. (A) Mammals encode three Slit and\nThe specific functions of Slit fragments were investigated\nSlit fragments interactions. Information about the specific functions of Slit fragments has only been recently investigated\nRecently, a Slit metalloprotease able to generate Slit-N and Slit-C was identified. This protein is a member of the BMP1/Tolloid family of Astacin-like metalloproteases and was named Tolkin (Tok; also known as Tolloid-related, Tlr, or Piranha). In\nRobo is a single-pass transmembrane receptor belonging to a subfamily of the immunoglobulin (Ig) superfamily proteins, with an apparent molecular weight of approximately 180 kDa for\nThe function of the Robo cytoplasmic domain in signalling has been studied in some detail. It is responsible for mediating the repulsive response, as demonstrated using a chimeric receptor consisting of the Robo ectodomain and Frazzled (Fra, a receptor involved in growth cone attraction that responds to the protein Netrin) cytoplasmic domain. In these experiments performed in\nRegarding the ectodomain, the requirement for the repulsive function of Robo1 Ig1 domain has been highlighted recently [\nIn the\nExpression patterns of Slit and Robo are dynamic during development and have been investigated in detail in some systems. Here we will give an overview that will be complemented in the next sections with more in-depth analysis for specific examples. In invertebrates such as the fly Slit is secreted from ventral midline cells during embryo development (\nMechanisms of Slit–Robo-mediated axon guidance in the mouse and fly central nervous system. (A) The fly VNC is the region in which Slit expression and function is best characterized. Slit is secreted by the midline glia and axons will respond to Slit according to the presence of Robo in the membrane of the growth cone. (B) In the mouse spinal cord, pre-crossing commissural axons do not express Robo1/2 (signal OFF) on their surface. However, as they reach the midline, they express Robo1/2 (signal ON) and respond to Slit secreted from the floor plate (FP) glia to exit this structure. P: posterior, A: anterior. (C) In the\nSlit distribution in the vertebrate brain is more complex than in the spinal cord, implying more diversified roles in axon guidance and other developmental processes in this region. For instance, in the mouse visual system, only Slit1 and Slit2 are expressed, whereas in zebrafish, Slit2 and Slit3 are both required and cooperate in guiding retinal ganglion cell axons [\nRobo1 and Robo2 proteins in the spinal cord are predominantly expressed in post-crossing axons [\nThis section provides a general overview of the regulatory mechanisms governing Slit and Robo expression, function and signalling output. Some of these mechanisms are described in more detail, along with specific examples, in the following sections.\nDuring nervous system development, the expression patterns of the Slit and Robo genes are closely regulated. The spatiotemporal expression of Slit and Robo is controlled by transcriptional regulation, which is essential for axonal pathfinding and neuronal migration. This fine regulation ensures that neurons migrate to the correct places and axons are guided to the proper targets. In pontine neurons (PN) along the anteroposterior (AP) axis during mouse hindbrain development, Robo2 is a direct target of Hoxa2, a homeodomain transcription factor of the Hox gene family [\nAnother layer of regulation is the post-transcriptional control. For instance, in the chicken spinal cord, the microRNA miR-92 suppresses Robo1 translation specifically in precrossing commissural axons. miR-92 binds to the 3′ UTR of\nRegarding post-translational regulation, during vertebrate commissural axon midline crossing, a ubiquitin-specific protease 33 (USP33) maintains the stability of Robo1 after its interaction with Slit. USP33 promotes Robo1 deubiquitination, which may prevent degradation of the Robo1 receptor and/or enable its recycling following Slit stimulation [\nSlit interacts not only with Robo receptors but also with other receptors, transmembrane proteins and extracellular matrix components, which act as co-receptors or help with its proper extracellular localization. For instance, Slit-C fragment (and probably full-length Slit) can bind to Dystroglycan, a transmembrane glycoprotein, through its laminin-G domain (\nSlit-Robo signalling. (A) The interaction with heparan sulfate proteoglycans (HSPGs) enhances Slit–Robo signalling in both vertebrates and\nIn vertebrates, cell-surface heparan sulfate proteoglycans (HSPGs) play a critical role in enhancing Slit–Robo signalling by binding to the D2 domain of Slit2 [\nIn addition to functioning as a ligand–receptor pair, Slit and Robo influence axon development through interactions with additional receptors and ligands, respectively (\nNotably, the mammalian Robo3 does not bind to Slit [\nUpon activation, Slit–Robo signalling initiates a series of intracellular events that can result in cytoskeletal reorganization and other cellular responses (\nIt has been shown that Robo1 receptors must be internalized via endocytosis to ensure proper intracellular signalling. Specifically, Chance & Bashaw [\nMoreover, Slit–Robo regulates neurogenesis in mammalian neural progenitor cells by interacting with the Notch pathway and transcriptionally activating the Notch effector Hes1. However, the activation of Hes1 by Robo1/2’s is independent of Notch signalling and is mediated by the Robo cytoplasmic domains [\nSmall GTPases are GTP-binding proteins that modify the cytoskeleton in order to control cell movement, and they play a central role in transducing signals downstream of activated Robo receptors. The small GTP-binding proteins act as molecular switches, cycling between active GTP-bound and inactive GDP-bound states to regulate various cellular processes, including cytoskeletal dynamics. Robo receptors recruit GTPases and their regulatory proteins, including GEFs (guanine nucleotide exchange factors), Dock/Nck (mammal Nck) and srGAPs (Slit–Robo GTPase activating proteins, in vertebrates) to modulate their activity and downstream signalling pathways. Thus, recruiting RhoGEF proteins downstream of the cytoplasmic Robo domain is required for Slit–Robo signalling [\nThe Slit–Robo signalling regulates actin dynamics by modulating the activity of small GTPases and other cytoskeletal regulators proteins. Wong\nUpon activation, the Robo ectodomain is cleaved and the rest of the protein undergoes internalization. This process seems to be required to activate repulsive signalling by the recruitment of Son of Sevenless (Sos) [\nAnother downstream effector of Slit–Robo signalling is Enabled (Ena). The Ena proline-rich protein family that promotes actin polymerization and cell motility. Ena interacts with the Robo receptor to enhance axonal repulsion by stopping the formation and extension of filopodia when responding to Slit [\nSlit–Robo signalling interacts with N-cadherin to regulate cell adhesion and neurite growth. This has been studied in chick retinal cells, where Slit binding to Robo receptors triggers Abl to interact with the intracellular domain (CC3) of Robo, facilitating a connection between the Robo and N-cadherin via the protein Cables a substrate of Abl and CDK5. Upon forming this protein complex, β-catenin is phosphorylated by Abl, leading to its dissociation from N-cadherin. As a result, cell attachment mediated by N-cadherin is compromised and neurites are not formed in cell culture experiments. Furthermore, phosphorylated β-catenin moves into the nucleus, to modify gene expression [\nThe VNC of\nSlit is the main repulsive guidance cue secreted by the midline glia, in\nNevertheless, Comm is not the only way that Slit and Robo availability can be modulated during the crossing of the midline in\nBesides its function in midline crossing, the Slit–Robo pathway is also involved in the positioning of longitudinal axonal tracks in the medio-lateral axis [\nIn vertebrates, axon guidance molecules have been studied in the commissural neurons of the spinal cord for decades (\nSlit–Robo classically regulates axon midline crossing by a repulsion mechanism, although the specific mechanisms used by the distinct Robo receptors diverge. Slits in the spinal cord are secreted by floor plate cells, which are located ventrally [\nIndeed, loss of Robo3 caused commissural axons to re-route and turn into the longitudinal axis without crossing the midline [\nRobo3 has multiple isoforms. Two of these isoforms [\nSlit–Robo signalling is also involved in guidance of motor axons. The spinal cord motor columns house the neurons that innervate the muscles, the motor neurons. These neurons extend their axons from the central nervous system (the spinal cord) to the periphery (muscle) and they exit through specific locations of the spinal cord. Robo2 and Slits mediate the exit of spinal accessory motor neurons through the lateral exit point possibly through a short-range attractive mechanism [\nMechanistically, the fine regulation of attractive cues and the repulsive activity of Slit–Robo signalling is required to ensure axons to be attracted to an intermediate target, such as the floor plate at the midline, and once they have reached it to continue their journey away from this structure. A reported mechanism for regulating the timing and location of repulsive activity involves the expression of Robo receptors in axonal growth cones by alternative splicing, discovering a previously unknown layer of regulation [\nAlternatively, insertion of Robo1 into the cell surface is modulated by RabGDI (Rab Guanine Nucleotide Dissociation Inhibitor) in chick embryos, but the mechanism is different from that of Comm. RabGDI is expressed specifically during midline crossing and promotes Robo1-containing vesicles to be shuttled to the growth cone membrane by Calsyntenin-1 [\nThe repulsive role of the Slit–Robo signalling has also been described in axons other than commissural axons. In zebrafish, axons that descend ipsilaterally from the ventral diencephalon are repelled from the midline by Robo2 [\nThere is a well-documented relationship between the Slit–Robo and Netrin-DCC signalling pathways during the development of the nervous system. Netrin is a secreted guidance cue which has been involved in attraction and repulsion of axons depending on the context [\nIn mammals, Netrin1-DCC and Slit–Robo signalling pathways also have opposing roles in spinal cord midline crossing. Although, by contrast to the fly, it was originally thought that the action of both pathways was sequential in their common goal to steer axons towards the floor plate (Netrin-DCC) and subsequently avoid re-entry into this intermediate target (Slit–Robo) [\nThe Slit–Robo pathway plays roles in several regions of the\nStrikingly, sex specific regulation of Robo has been implicated in the formation of sexually dimorphic neuronal circuits [\nThe retina of\nSimilar to the fly brain, in vertebrates Slit distribution in the developing brain is more complex than in the spinal cord (\nSlit–Robo mediated repulsion also modulates the pathfinding of axons that project from retinal ganglion cells (RGCs) in the eye to the brain in mammals. Normally, in mice, a relatively large proportion of RGC axons cross the midline to form the optic chiasm, whereas a relatively low proportion of axons remain ipsilateral [\nSo far there is little data supporting a function of Slit–Robo signalling in early neurogenesis of the fly CNS. One study has shown that Slit regulates asymmetric division of a population of ganglion mother cell (GMC1) derived from neuroblast NB4.2 in the VNC of the\nBy contrast, the function of Slit–Robo has been addressed in several studies using the mouse as a model. During early cortical development, neuroepithelial progenitors in the ventricular zone (apical side) divide symmetrically to increase their number. Later on, excitatory projection neurons are generated by asymmetrical divisions either from apical radial glia cells (direct neurogenesis) or from basal intermediate progenitor cells (indirect neurogenesis) [\nAnother aspect of neurogenesis is the process of newly born neurons detaching from the apical side soon after cell division. During retinogenesis, the newborn neurons also detach from the apical (outer) neuroepithelium. In zebrafish, the apical retraction of RGCs depends on Slit1b and Robo3 [\nBesides axonal guidance, Slit–Robo is also involved more widely in the architecture of brain neuropils by defining boundaries that limit the localization of different cellular populations. This function has been described in the optic lobe of\nThe fly optic lobe shares some similarities with the mammalian cortex, since neuronal migration has been shown to play a role in the assembly of neuronal circuits [\nSlit–Robo signalling regulates neuronal migration through the modulation of focal adhesion kinase (FAK) activity and cytoskeletal dynamics in vertebrates. Slit2 influences Cdc42 activity and lamellipodia formation on\nIn vertebrates Slit–Robo has also a role in neuronal migration. Slit was first discovered to influence directional migration of neuroblasts to the olfactory bulb [\nSame as the typical commissural neurons in the spinal cord, inferior olive neurons of the brainstem extend axons that cross the midline in route to their target in the contralateral side of the cerebellum. However, their somas do not cross this structure, despite their dorso-ventral migration towards the vicinity of the midline. Migrating inferior olive neurons express Robo3, and in its absence they still form the inferior olive nucleus in its proper location, albeit more slowly and with abnormal cellular shape and polarization [\nIn the spinal cord, dI1 neurons migrate slightly in the ventral direction from the dorsal area after their axons cross the midline. Robo2 is enriched in the ipsilateral subpopulation of dI1 neurons. In its absence, the cell bodies of ipsilateral neurons (but not the commissural population) are distributed in regions that are too ventral and/or medial. This effect of Robo2 seems to be a partial result of Slit2 action in the surrounding regions of dI1 neuron migration, since the phenotype in\nMotor neuron cell bodies are contained in the ventral horn of the spinal cord and brainstem. Most motor neurons are born close to their final residence (in the ventral progenitor area), therefore the notion that they do not migrate much. In the spinal cord and brainstem, the repulsive action of Slit–Robo signalling is crucial for motor neurons to reach their proper location, as in Robo1/2 or Slits1/2/3 mutant mice these cell bodies are misplaced in the floor plate, as opposed to be adjacent to it in control animals. This effect seems to be counteracted by Netrin-DCC signalling, suggesting a balance of attractive and repulsive forces to keep the neuronal somas in place [\nApart from the active migration that motor neurons need to undergo, it is crucial that these neurons do not migrate outside of the central nervous system into the periphery. Several studies have shown that guidance molecules prevent exit of motor neurons to the periphery by regulating the function of boundary cap cells [\nRobo1 influences the entrance and the migration of interneurons throughout the cerebral cortex at the stages when these neurons are\nA system that uses guidepost cells to physically narrow and permit the path of thalamic axons\nIn addition to tangential migration of inhibitory neurons, Robo1 and Robo4 play a role in the migration of excitatory pyramidal neurons to the cortex [\nWe have described so far that Slit–Robo signalling plays crucial roles during multiple aspects of nervous system development, extending beyond its classical role on axon guidance. Neurons form synapses with other neurons or different cell types to maintain the flow of information within a circuit. Unsurprisingly, Slit–Robo signalling has also been involved in synapse formation in vertebrates, although not too much is known about the mechanisms of this process. Morphological changes also occur, as neurons need to arborize and stratify on specific sites. In the zebrafish optic tectum, Slit1a inhibits arborization and premature maturation of axon terminals of retinal ganglion cells, and also reduces presynaptic sites via a Robo2-dependent and independent mechanism [\nSince the discovery of Slit and Robo proteins in late 1980s and 1990s, we have obtained a substantial amount of data that have allowed us to learn the way axons select their path in detail. In the future, we will probably understand better how complex interactions between this pathway and others involved in axonal pathfinding work together to steer the growth cone and establish the normal wiring patterns.\nAfter the initial discovery of Slit in flies, and later confirmation of an orthologue gene in vertebrates, we have unravelled several molecular details about the way this pathway works in both systems. Interestingly, there are big similarities, not only in the way the signal is transduced but also in the expression patterns. Notably, both models are still actively contributing towards a better understanding of the pathway. One pending task is that some aspects have been assessed only in\nA challenge for the future is to identify pharmacological modulators of this pathway, for which screenings using invertebrate models could provide an excellent platform for later validation in mammalian models. Drugs that modulate this pathway could aid in the treatment of neurodevelopmental disorders and cancer, as recent literature increasingly supports the role of Slit and Robo in these diseases.", "content_for_embedding": "A key event in our comprehension of axonal guidance and cell migration during development came with the discovery of Slit proteins. Slit was first detected in the fly embryonic midline glial cells of the ventral nerve cord (VNC), from where it is released to the extracellular space and distributed along axon tracts. Slit is a member of the protein family containing EGF-like repeats [\nThe discovery of Robo came out of a\nSubsequent research revealed Slit as a ligand for the Robo receptor in both vertebrates and\nSlit is an extracellular matrix-secreted glycoprotein [\nAll Slit proteins share a common structural framework (\nSchematic representation of Slit and Robo structure. (A) Mammals encode three Slit and\nThe specific functions of Slit fragments were investigated\nSlit fragments interactions. Information about the specific functions of Slit fragments has only been recently investigated\nRecently, a Slit metalloprotease able to generate Slit-N and Slit-C was identified. This protein is a member of the BMP1/Tolloid family of Astacin-like metalloproteases and was named Tolkin (Tok; also known as Tolloid-related, Tlr, or Piranha). In\nRobo is a single-pass transmembrane receptor belonging to a subfamily of the immunoglobulin (Ig) superfamily proteins, with an apparent molecular weight of approximately 180 kDa for\nThe function of the Robo cytoplasmic domain in signalling has been studied in some detail. It is responsible for mediating the repulsive response, as demonstrated using a chimeric receptor consisting of the Robo ectodomain and Frazzled (Fra, a receptor involved in growth cone attraction that responds to the protein Netrin) cytoplasmic domain. In these experiments performed in\nRegarding the ectodomain, the requirement for the repulsive function of Robo1 Ig1 domain has been highlighted recently [\nIn the\nExpression patterns of Slit and Robo are dynamic during development and have been investigated in detail in some systems. Here we will give an overview that will be complemented in the next sections with more in-depth analysis for specific examples. In invertebrates such as the fly Slit is secreted from ventral midline cells during embryo development (\nMechanisms of Slit–Robo-mediated axon guidance in the mouse and fly central nervous system. (A) The fly VNC is the region in which Slit expression and function is best characterized. Slit is secreted by the midline glia and axons will respond to Slit according to the presence of Robo in the membrane of the growth cone. (B) In the mouse spinal cord, pre-crossing commissural axons do not express Robo1/2 (signal OFF) on their surface. However, as they reach the midline, they express Robo1/2 (signal ON) and respond to Slit secreted from the floor plate (FP) glia to exit this structure. P: posterior, A: anterior. (C) In the\nSlit distribution in the vertebrate brain is more complex than in the spinal cord, implying more diversified roles in axon guidance and other developmental processes in this region. For instance, in the mouse visual system, only Slit1 and Slit2 are expressed, whereas in zebrafish, Slit2 and Slit3 are both required and cooperate in guiding retinal ganglion cell axons [\nRobo1 and Robo2 proteins in the spinal cord are predominantly expressed in post-crossing axons [\nThis section provides a general overview of the regulatory mechanisms governing Slit and Robo expression, function and signalling output. Some of these mechanisms are described in more detail, along with specific examples, in the following sections.\nDuring nervous system development, the expression patterns of the Slit and Robo genes are closely regulated. The spatiotemporal expression of Slit and Robo is controlled by transcriptional regulation, which is essential for axonal pathfinding and neuronal migration. This fine regulation ensures that neurons migrate to the correct places and axons are guided to the proper targets. In pontine neurons (PN) along the anteroposterior (AP) axis during mouse hindbrain development, Robo2 is a direct target of Hoxa2, a homeodomain transcription factor of the Hox gene family [\nAnother layer of regulation is the post-transcriptional control. For instance, in the chicken spinal cord, the microRNA miR-92 suppresses Robo1 translation specifically in precrossing commissural axons. miR-92 binds to the 3′ UTR of\nRegarding post-translational regulation, during vertebrate commissural axon midline crossing, a ubiquitin-specific protease 33 (USP33) maintains the stability of Robo1 after its interaction with Slit. USP33 promotes Robo1 deubiquitination, which may prevent degradation of the Robo1 receptor and/or enable its recycling following Slit stimulation [\nSlit interacts not only with Robo receptors but also with other receptors, transmembrane proteins and extracellular matrix components, which act as co-receptors or help with its proper extracellular localization. For instance, Slit-C fragment (and probably full-length Slit) can bind to Dystroglycan, a transmembrane glycoprotein, through its laminin-G domain (\nSlit-Robo signalling. (A) The interaction with heparan sulfate proteoglycans (HSPGs) enhances Slit–Robo signalling in both vertebrates and\nIn vertebrates, cell-surface heparan sulfate proteoglycans (HSPGs) play a critical role in enhancing Slit–Robo signalling by binding to the D2 domain of Slit2 [\nIn addition to functioning as a ligand–receptor pair, Slit and Robo influence axon development through interactions with additional receptors and ligands, respectively (\nNotably, the mammalian Robo3 does not bind to Slit [\nUpon activation, Slit–Robo signalling initiates a series of intracellular events that can result in cytoskeletal reorganization and other cellular responses (\nIt has been shown that Robo1 receptors must be internalized via endocytosis to ensure proper intracellular signalling. Specifically, Chance & Bashaw [\nMoreover, Slit–Robo regulates neurogenesis in mammalian neural progenitor cells by interacting with the Notch pathway and transcriptionally activating the Notch effector Hes1. However, the activation of Hes1 by Robo1/2’s is independent of Notch signalling and is mediated by the Robo cytoplasmic domains [\nSmall GTPases are GTP-binding proteins that modify the cytoskeleton in order to control cell movement, and they play a central role in transducing signals downstream of activated Robo receptors. The small GTP-binding proteins act as molecular switches, cycling between active GTP-bound and inactive GDP-bound states to regulate various cellular processes, including cytoskeletal dynamics. Robo receptors recruit GTPases and their regulatory proteins, including GEFs (guanine nucleotide exchange factors), Dock/Nck (mammal Nck) and srGAPs (Slit–Robo GTPase activating proteins, in vertebrates) to modulate their activity and downstream signalling pathways. Thus, recruiting RhoGEF proteins downstream of the cytoplasmic Robo domain is required for Slit–Robo signalling [\nThe Slit–Robo signalling regulates actin dynamics by modulating the activity of small GTPases and other cytoskeletal regulators proteins. Wong\nUpon activation, the Robo ectodomain is cleaved and the rest of the protein undergoes internalization. This process seems to be required to activate repulsive signalling by the recruitment of Son of Sevenless (Sos) [\nAnother downstream effector of Slit–Robo signalling is Enabled (Ena). The Ena proline-rich protein family that promotes actin polymerization and cell motility. Ena interacts with the Robo receptor to enhance axonal repulsion by stopping the formation and extension of filopodia when responding to Slit [\nSlit–Robo signalling interacts with N-cadherin to regulate cell adhesion and neurite growth. This has been studied in chick retinal cells, where Slit binding to Robo receptors triggers Abl to interact with the intracellular domain (CC3) of Robo, facilitating a connection between the Robo and N-cadherin via the protein Cables a substrate of Abl and CDK5. Upon forming this protein complex, β-catenin is phosphorylated by Abl, leading to its dissociation from N-cadherin. As a result, cell attachment mediated by N-cadherin is compromised and neurites are not formed in cell culture experiments. Furthermore, phosphorylated β-catenin moves into the nucleus, to modify gene expression [\nThe VNC of\nSlit is the main repulsive guidance cue secreted by the midline glia, in\nNevertheless, Comm is not the only way that Slit and Robo availability can be modulated during the crossing of the midline in\nBesides its function in midline crossing, the Slit–Robo pathway is also involved in the positioning of longitudinal axonal tracks in the medio-lateral axis [\nIn vertebrates, axon guidance molecules have been studied in the commissural neurons of the spinal cord for decades (\nSlit–Robo classically regulates axon midline crossing by a repulsion mechanism, although the specific mechanisms used by the distinct Robo receptors diverge. Slits in the spinal cord are secreted by floor plate cells, which are located ventrally [\nIndeed, loss of Robo3 caused commissural axons to re-route and turn into the longitudinal axis without crossing the midline [\nRobo3 has multiple isoforms. Two of these isoforms [\nSlit–Robo signalling is also involved in guidance of motor axons. The spinal cord motor columns house the neurons that innervate the muscles, the motor neurons. These neurons extend their axons from the central nervous system (the spinal cord) to the periphery (muscle) and they exit through specific locations of the spinal cord. Robo2 and Slits mediate the exit of spinal accessory motor neurons through the lateral exit point possibly through a short-range attractive mechanism [\nMechanistically, the fine regulation of attractive cues and the repulsive activity of Slit–Robo signalling is required to ensure axons to be attracted to an intermediate target, such as the floor plate at the midline, and once they have reached it to continue their journey away from this structure. A reported mechanism for regulating the timing and location of repulsive activity involves the expression of Robo receptors in axonal growth cones by alternative splicing, discovering a previously unknown layer of regulation [\nAlternatively, insertion of Robo1 into the cell surface is modulated by RabGDI (Rab Guanine Nucleotide Dissociation Inhibitor) in chick embryos, but the mechanism is different from that of Comm. RabGDI is expressed specifically during midline crossing and promotes Robo1-containing vesicles to be shuttled to the growth cone membrane by Calsyntenin-1 [\nThe repulsive role of the Slit–Robo signalling has also been described in axons other than commissural axons. In zebrafish, axons that descend ipsilaterally from the ventral diencephalon are repelled from the midline by Robo2 [\nThere is a well-documented relationship between the Slit–Robo and Netrin-DCC signalling pathways during the development of the nervous system. Netrin is a secreted guidance cue which has been involved in attraction and repulsion of axons depending on the context [\nIn mammals, Netrin1-DCC and Slit–Robo signalling pathways also have opposing roles in spinal cord midline crossing. Although, by contrast to the fly, it was originally thought that the action of both pathways was sequential in their common goal to steer axons towards the floor plate (Netrin-DCC) and subsequently avoid re-entry into this intermediate target (Slit–Robo) [\nThe Slit–Robo pathway plays roles in several regions of the\nStrikingly, sex specific regulation of Robo has been implicated in the formation of sexually dimorphic neuronal circuits [\nThe retina of\nSimilar to the fly brain, in vertebrates Slit distribution in the developing brain is more complex than in the spinal cord (\nSlit–Robo mediated repulsion also modulates the pathfinding of axons that project from retinal ganglion cells (RGCs) in the eye to the brain in mammals. Normally, in mice, a relatively large proportion of RGC axons cross the midline to form the optic chiasm, whereas a relatively low proportion of axons remain ipsilateral [\nSo far there is little data supporting a function of Slit–Robo signalling in early neurogenesis of the fly CNS. One study has shown that Slit regulates asymmetric division of a population of ganglion mother cell (GMC1) derived from neuroblast NB4.2 in the VNC of the\nBy contrast, the function of Slit–Robo has been addressed in several studies using the mouse as a model. During early cortical development, neuroepithelial progenitors in the ventricular zone (apical side) divide symmetrically to increase their number. Later on, excitatory projection neurons are generated by asymmetrical divisions either from apical radial glia cells (direct neurogenesis) or from basal intermediate progenitor cells (indirect neurogenesis) [\nAnother aspect of neurogenesis is the process of newly born neurons detaching from the apical side soon after cell division. During retinogenesis, the newborn neurons also detach from the apical (outer) neuroepithelium. In zebrafish, the apical retraction of RGCs depends on Slit1b and Robo3 [\nBesides axonal guidance, Slit–Robo is also involved more widely in the architecture of brain neuropils by defining boundaries that limit the localization of different cellular populations. This function has been described in the optic lobe of\nThe fly optic lobe shares some similarities with the mammalian cortex, since neuronal migration has been shown to play a role in the assembly of neuronal circuits [\nSlit–Robo signalling regulates neuronal migration through the modulation of focal adhesion kinase (FAK) activity and cytoskeletal dynamics in vertebrates. Slit2 influences Cdc42 activity and lamellipodia formation on\nIn vertebrates Slit–Robo has also a role in neuronal migration. Slit was first discovered to influence directional migration of neuroblasts to the olfactory bulb [\nSame as the typical commissural neurons in the spinal cord, inferior olive neurons of the brainstem extend axons that cross the midline in route to their target in the contralateral side of the cerebellum. However, their somas do not cross this structure, despite their dorso-ventral migration towards the vicinity of the midline. Migrating inferior olive neurons express Robo3, and in its absence they still form the inferior olive nucleus in its proper location, albeit more slowly and with abnormal cellular shape and polarization [\nIn the spinal cord, dI1 neurons migrate slightly in the ventral direction from the dorsal area after their axons cross the midline. Robo2 is enriched in the ipsilateral subpopulation of dI1 neurons. In its absence, the cell bodies of ipsilateral neurons (but not the commissural population) are distributed in regions that are too ventral and/or medial. This effect of Robo2 seems to be a partial result of Slit2 action in the surrounding regions of dI1 neuron migration, since the phenotype in\nMotor neuron cell bodies are contained in the ventral horn of the spinal cord and brainstem. Most motor neurons are born close to their final residence (in the ventral progenitor area), therefore the notion that they do not migrate much. In the spinal cord and brainstem, the repulsive action of Slit–Robo signalling is crucial for motor neurons to reach their proper location, as in Robo1/2 or Slits1/2/3 mutant mice these cell bodies are misplaced in the floor plate, as opposed to be adjacent to it in control animals. This effect seems to be counteracted by Netrin-DCC signalling, suggesting a balance of attractive and repulsive forces to keep the neuronal somas in place [\nApart from the active migration that motor neurons need to undergo, it is crucial that these neurons do not migrate outside of the central nervous system into the periphery. Several studies have shown that guidance molecules prevent exit of motor neurons to the periphery by regulating the function of boundary cap cells [\nRobo1 influences the entrance and the migration of interneurons throughout the cerebral cortex at the stages when these neurons are\nA system that uses guidepost cells to physically narrow and permit the path of thalamic axons\nIn addition to tangential migration of inhibitory neurons, Robo1 and Robo4 play a role in the migration of excitatory pyramidal neurons to the cortex [\nWe have described so far that Slit–Robo signalling plays crucial roles during multiple aspects of nervous system development, extending beyond its classical role on axon guidance. Neurons form synapses with other neurons or different cell types to maintain the flow of information within a circuit. Unsurprisingly, Slit–Robo signalling has also been involved in synapse formation in vertebrates, although not too much is known about the mechanisms of this process. Morphological changes also occur, as neurons need to arborize and stratify on specific sites. In the zebrafish optic tectum, Slit1a inhibits arborization and premature maturation of axon terminals of retinal ganglion cells, and also reduces presynaptic sites via a Robo2-dependent and independent mechanism [\nSince the discovery of Slit and Robo proteins in late 1980s and 1990s, we have obtained a substantial amount of data that have allowed us to learn the way axons select their path in detail. In the future, we will probably understand better how complex interactions between this pathway and others involved in axonal pathfinding work together to steer the growth cone and establish the normal wiring patterns.\nAfter the initial discovery of Slit in flies, and later confirmation of an orthologue gene in vertebrates, we have unravelled several molecular details about the way this pathway works in both systems. Interestingly, there are big similarities, not only in the way the signal is transduced but also in the expression patterns. Notably, both models are still actively contributing towards a better understanding of the pathway. One pending task is that some aspects have been assessed only in\nA challenge for the future is to identify pharmacological modulators of this pathway, for which screenings using invertebrate models could provide an excellent platform for later validation in mammalian models. Drugs that modulate this pathway could aid in the treatment of neurodevelopmental disorders and cancer, as recent literature increasingly supports the role of Slit and Robo in these diseases.", "topic": "Brain"}
{"pmid": "39483872", "pmcid": "12310195", "title": "New Donor Selection Criteria Result in Optimal Outcomes of Kidneys from Uncontrolled Donation After the Circulatory Determination of Death", "publication_year": "N/A", "abstract": "", "full_text": "Kidney transplantation is the most effective treatment for patients with kidney failure and is associated with better survival and quality of life compared with chronic dialysis.\nDonation after the circulatory determination of death (DCDD) emerged in the 1990s as a promising alternative to address the disparity between organ supply and demand.\nSurprisingly, the limited implementation of uDCDD programs persists despite recommendations from the European Resuscitation Guidelines to consider uDCDD when advanced cardiopulmonary resuscitation (aCPR) is deemed unsuccessful.\nuDCDD represents a vast and underused source of kidneys for transplantation. Although the literature describing the outcomes of kidneys obtained from uDCDD donors is limited, available studies report satisfactory long-term graft survival.\nThe inclusion criteria for uDCDD donors have remained unchanged during the past 20 y, reflecting the limited research conducted on this type of donation.\nApplying stricter and more refined inclusion criteria may improve short-term outcomes compared with those traditionally reported. This study aims to share our experience with uDCDD kidney transplantation based on a protocol that incorporates new and easy-to-apply criteria for donor selection, potentially addressing some of the challenges and maximizing the benefits of this underused donor pool.\nData were prospectively obtained from all kidney recipients who received a graft from a local uDCDD donor between the start of our program (December 2013) and April 2024. Follow-up continued for all recipients until May 2024. The control group comprised all kidney transplant recipients during the same period who received organs from local DNDD donors meeting standard criteria as defined by the United Network for Organ Sharing—donors younger than 50 y or donors younger than 60 y with a maximum of one of the following: history of arterial hypertension, serum creatinine >130 mmol/L, or death due to cerebrovascular events.\nThe study protocol received institutional review board approval, adhering to Spanish legislation.\nThe donor selection criteria for our uDCDD program are described in Table\nCriteria for the selection of potential uDCDD kidney donors\nA-NRP, abdominal normothermic regional perfusion; E-CPR, extracorporeal-assisted cardiopulmonary resuscitation; uDCDD, uncontrolled donation after the circulatory determination.\nThe uDCDD protocol has been detailed previously.\nIf donor eligibility criteria were met, cardiac compression and mechanical ventilation would continue beyond futility to preserve donation opportunities during the transfer of the potential donor to the hospital. All potential uDCDD donors were transferred to the hospital with mechanical cardiac compression using the LUCAS 2 device.\nTo minimize WIT, the potential uDCDD donor was directly transferred to the intensive care unit (ICU), where death was declared and legal permission was obtained to proceed with postmortem preservation measures. The declaration of death was performed by the ICU physician on duty, who was independent of the donation and transplantation team. In Spain, the diagnosis of death based on circulatory criteria only requires a physician independent of the transplant team and the coordination team. This physician declared death after confirming that aCPR had been exhausted and that there was no indication for alternative therapeutic interventions, as well as after observing a “no-touch period” of 5 min of complete absence of cardiopulmonary activity, defined by the absence of respiratory movements and electrical activity on the electrocardiogram. Postmortem interventions entailed the cannulation of femoral vessels and the initiation of abdominal normothermic regional perfusion (A-NRP). A double-lumen cannula (ThruPort Systems, Edwards) was inserted into the femoral artery, and through it, an aortic occlusion balloon was placed, avoiding the need to cannulate the contralateral groin and minimizing WIT.\nRelatives were approached to discuss donation opportunities and assess whether organ donation was consistent with the person’s wishes and values. On consent and confirmation of medical eligibility, the donor was transferred to the operation theater for kidney recovery. All recovered kidneys underwent machine perfusion (LifePort; Organ Recovery System, Diegem, Belgium) for at least 2 h. Kidneys with a vascular resistance index (RI) >0.3 mm Hg/mL/min were discarded.\nRecipients of uDCDD kidney grafts received induction with thymoglobulin and immunosuppressive treatment based on the delayed introduction of tacrolimus, associated with mycophenolate mofetil and steroids. Standard criteria DNDD kidney recipients received standard immunosuppression (tacrolimus, mycophenolate mofetil, and steroids). According to the treating team, induction with thymoglobulin or basiliximab was associated with patients at high risk of acute rejection or DGF.\nThe no-flow period was defined as the time from CA to the start of aCPR. WIT extended from CA to the initiation of in situ preservation with A-NRP.\nPNF was defined as the failure of a graft to ever function. DGF was considered by the need for dialysis during the first week after transplantation. Death-censored graft survival was calculated from the date of transplantation to the date of irreversible graft failure, defined by return to long-term dialysis or retransplantation. In the event of death with a functioning graft, the follow-up period was censored at the date of death. The glomerular filtration rate (GFR) was estimated by using the Chronic Kidney Disease Epidemiology Collaboration equation.\nA descriptive analysis was performed, presenting categorical variables as absolute numbers and percentages and continuous variables as measures of central tendency and dispersion. Comparisons between uDCDD and standard criteria DNDD kidney recipients were conducted using the Student’s\nBetween December 2013 and April 2024, the EMS reported 109 potential uDCDD donors to the donor coordination team. In 69 cases, the uDCDD pathway was not activated for the eligibility criteria were not met in most cases or logistical problems such as being with a cDCDD or DNDD donor at the same time (see Figure\nStudy profile.\nBaseline donor and recipient data, as well as posttransplant outcomes, are summarized in Table\nBaseline features and outcomes of recipients of kidneys from uDCDD vs standard criteria DNDD donors\naCPR, advanced cardiopulmonary resuscitation; A-NRP, abdominal normothermic regional perfusion; CIT, cold ischemic time; cPRA, calculated panel-reactive antibody; DGF, delayed graft function; DNDD, donation after the neurological determination of death; ICU, intensive care unit; IQR, interquartile range; PNF, primary nonfunction; T0, time of cardiac arrest; uDCDD, uncontrolled donation after the circulatory determination of death.\nNo statistically significant differences were observed in the clinical or demographic characteristics of donors and recipients between the 2 groups. However, recipients of uDCDD donors had a significantly shorter median cold ischemic time (CIT) compared with recipients of DNDD kidneys (15 h [IQR, 9–21] versus 19 h [IQR, 15.5–22];\nThe incidence of PNF was low in both groups, with no cases reported in the standard DNDD cohort and only 1 case (2.3%) in the uDCDD group (\nSequential serum creatinine levels (A) and glomerular filtration rate (B) in kidney recipients from both groups (in black uDCDD; in gray DNDD). The bar represents the median and the error bars the interquartile range for each time point. A general linear model for a repeated measures test was used. At all-time points\nKaplan-Meier graphs. A, Death-censored kidney graft survival. B, No death-censored kidney graft survival. DNDD, donation after the neurological determination of death; uDCDD, uncontrolled donation after the circulatory determination of death.\nRecipients of uDCDD kidneys had a significantly higher probability of developing DGF in univariate analysis (odds ratio, 3.2; 95% confidence interval [CI], 1.4-7.2;\nOur experience with the transplantation of highly selected uDCDD kidneys confirms the feasibility of these programs and demonstrates that posttransplant outcomes can be comparable with those achieved with standard criteria DNDD donors. Published literature on uDCDD kidney transplants has often reported suboptimal outcomes,\nTo justify the value of uDCDD programs, most researches have compared the outcomes of recipients of kidneys from uDCDD donors with those from DNDD donors meeting expanded criteria.\nOne major limitation of uDCDD outcomes is the lack of evolution in donor acceptance criteria during the past 2 decades. Some centers in Spain continue to use unrestricted donor selection criteria established >20 y ago,\nOur protocol was designed to minimize these risks by excluding donors who did not meet at least 1 of 3 ideal criteria: donor age younger than 50 y, no-flow period of <10 min, or WIT of <120 min. These criteria, informed by previous studies, aimed to optimize organ utilization and posttransplant outcomes. Viglietti et al\nPNF remains the most concerning complication of uDCDD kidney transplantation, with reported incidences ranging from 1.8% to 20%.\nSeveral factors such as advance donor age, prolonged WIT, extended no-flow periods, or in situ cooling as a preservation method have been found to be associated with high rates of PNF in uDCDD.\nDespite a higher DGF rate in our uDCDD group (46%) compared with the DNDD group, this was lower than what was previously documented.\nOur 5-y graft survival rate of 92% surpasses previously reported rates\nWe also analyzed data on kidney function in the long term, which were again appropriate in recipients of kidneys from uDCDD donors, with no differences in serum creatinine or eGFR values at 1 and 5 y compared with recipients of ideal DNDD kidneys. The description of analytical parameters in our series is of great importance since most studies published on uDCDD kidney transplantation have only reported survival data, with no information on graft function.\nOne of the main discouraging issues in uDCDD is the high kidney discard rate, >30% in large Spanish studies.\nWe would like to emphasize that the favorable results obtained in our center with the transplantation of uDCDD kidneys were not related to a restrictive selection of kidney recipients. In fact, there were no differences between both recipient groups in donor age, recipient age, rate of sensitized patients, or the percentage of second recipient transplant.\nThe keystone of successful uDCDD programs is optimizing all logistical requirements with the clear goal of reducing warm ischemic injury. Although we cannot interfere with donor age or in the duration of no-flow periods that all EMS try to reduce as much as possible to increase the probability of return of spontaneous circulation and save lives, we are able to reduce the duration of intrahospital procedures to minimize the period until A-NRP. In this regard, the obvious feature that distinguishes our uDCDD program from others is that both the diagnosis of death and the start of A-NRP were performed in the ICU. The centralization of both phases of the process in the same unit helped us to avoid in-hospital transfers that turned out to be difficult in multimonitored patients subject to mechanical cardiac compression and mechanical ventilation. This approach reduces WIT and largely contributes to improving graft quality.\nIn our study, all kidneys were perfused on pulsatile hypothermic machine perfusion (HMP). Pulsatile HMP has shown to be effective in reducing the incidence of DGF, both in DNDD and DCDD, but has not been associated with a decreased risk of PNF.\nThere has been a debate concerning the coexistence of E-CPR and uDCDD programs. In our hospital, E-CPR is available, and to activate the uDCDD procedure, it is mandatory to check that there is no indication to activate the E-CPR pathway. The inclusion criteria for both programs are different.\nWe are aware of the ethical-legal barriers to setting up DCDD programs,\nOur study has several limitations. Our kidney survival data have included our learning curve in uDCDD, and the study period is >10 y, although the number of cases with a prolonged follow-up is still extremely low. We are also aware of the small sample size and that our study was not randomized. Finally, although we used a control group with similar characteristics, recipients were selected to receive a DNDD or a uDCDD graft based on an assessment of donor-recipient compatibility and other data by the treating team, so we cannot exclude a certain selection bias.\nIn summary, our results support that obtaining renal grafts from uDCDD donors is not only feasible but also provides results that are as good as those obtained from standard DNDD donors. Strict donor selection criteria, improved in-hospital logistics to reduce WIT, and the systematic use of A-NRP are key factors for success in terms of kidney utilization and posttransplant outcomes.\nThe authors wish to thank all members of the donor coordination units and kidney transplant teams of centers that provided data on kidney recipients transplanted with uDCDD grafts obtained at our center.", "content_for_embedding": "Kidney transplantation is the most effective treatment for patients with kidney failure and is associated with better survival and quality of life compared with chronic dialysis.\nDonation after the circulatory determination of death (DCDD) emerged in the 1990s as a promising alternative to address the disparity between organ supply and demand.\nSurprisingly, the limited implementation of uDCDD programs persists despite recommendations from the European Resuscitation Guidelines to consider uDCDD when advanced cardiopulmonary resuscitation (aCPR) is deemed unsuccessful.\nuDCDD represents a vast and underused source of kidneys for transplantation. Although the literature describing the outcomes of kidneys obtained from uDCDD donors is limited, available studies report satisfactory long-term graft survival.\nThe inclusion criteria for uDCDD donors have remained unchanged during the past 20 y, reflecting the limited research conducted on this type of donation.\nApplying stricter and more refined inclusion criteria may improve short-term outcomes compared with those traditionally reported. This study aims to share our experience with uDCDD kidney transplantation based on a protocol that incorporates new and easy-to-apply criteria for donor selection, potentially addressing some of the challenges and maximizing the benefits of this underused donor pool.\nData were prospectively obtained from all kidney recipients who received a graft from a local uDCDD donor between the start of our program (December 2013) and April 2024. Follow-up continued for all recipients until May 2024. The control group comprised all kidney transplant recipients during the same period who received organs from local DNDD donors meeting standard criteria as defined by the United Network for Organ Sharing—donors younger than 50 y or donors younger than 60 y with a maximum of one of the following: history of arterial hypertension, serum creatinine >130 mmol/L, or death due to cerebrovascular events.\nThe study protocol received institutional review board approval, adhering to Spanish legislation.\nThe donor selection criteria for our uDCDD program are described in Table\nCriteria for the selection of potential uDCDD kidney donors\nA-NRP, abdominal normothermic regional perfusion; E-CPR, extracorporeal-assisted cardiopulmonary resuscitation; uDCDD, uncontrolled donation after the circulatory determination.\nThe uDCDD protocol has been detailed previously.\nIf donor eligibility criteria were met, cardiac compression and mechanical ventilation would continue beyond futility to preserve donation opportunities during the transfer of the potential donor to the hospital. All potential uDCDD donors were transferred to the hospital with mechanical cardiac compression using the LUCAS 2 device.\nTo minimize WIT, the potential uDCDD donor was directly transferred to the intensive care unit (ICU), where death was declared and legal permission was obtained to proceed with postmortem preservation measures. The declaration of death was performed by the ICU physician on duty, who was independent of the donation and transplantation team. In Spain, the diagnosis of death based on circulatory criteria only requires a physician independent of the transplant team and the coordination team. This physician declared death after confirming that aCPR had been exhausted and that there was no indication for alternative therapeutic interventions, as well as after observing a “no-touch period” of 5 min of complete absence of cardiopulmonary activity, defined by the absence of respiratory movements and electrical activity on the electrocardiogram. Postmortem interventions entailed the cannulation of femoral vessels and the initiation of abdominal normothermic regional perfusion (A-NRP). A double-lumen cannula (ThruPort Systems, Edwards) was inserted into the femoral artery, and through it, an aortic occlusion balloon was placed, avoiding the need to cannulate the contralateral groin and minimizing WIT.\nRelatives were approached to discuss donation opportunities and assess whether organ donation was consistent with the person’s wishes and values. On consent and confirmation of medical eligibility, the donor was transferred to the operation theater for kidney recovery. All recovered kidneys underwent machine perfusion (LifePort; Organ Recovery System, Diegem, Belgium) for at least 2 h. Kidneys with a vascular resistance index (RI) >0.3 mm Hg/mL/min were discarded.\nRecipients of uDCDD kidney grafts received induction with thymoglobulin and immunosuppressive treatment based on the delayed introduction of tacrolimus, associated with mycophenolate mofetil and steroids. Standard criteria DNDD kidney recipients received standard immunosuppression (tacrolimus, mycophenolate mofetil, and steroids). According to the treating team, induction with thymoglobulin or basiliximab was associated with patients at high risk of acute rejection or DGF.\nThe no-flow period was defined as the time from CA to the start of aCPR. WIT extended from CA to the initiation of in situ preservation with A-NRP.\nPNF was defined as the failure of a graft to ever function. DGF was considered by the need for dialysis during the first week after transplantation. Death-censored graft survival was calculated from the date of transplantation to the date of irreversible graft failure, defined by return to long-term dialysis or retransplantation. In the event of death with a functioning graft, the follow-up period was censored at the date of death. The glomerular filtration rate (GFR) was estimated by using the Chronic Kidney Disease Epidemiology Collaboration equation.\nA descriptive analysis was performed, presenting categorical variables as absolute numbers and percentages and continuous variables as measures of central tendency and dispersion. Comparisons between uDCDD and standard criteria DNDD kidney recipients were conducted using the Student’s\nBetween December 2013 and April 2024, the EMS reported 109 potential uDCDD donors to the donor coordination team. In 69 cases, the uDCDD pathway was not activated for the eligibility criteria were not met in most cases or logistical problems such as being with a cDCDD or DNDD donor at the same time (see Figure\nStudy profile.\nBaseline donor and recipient data, as well as posttransplant outcomes, are summarized in Table\nBaseline features and outcomes of recipients of kidneys from uDCDD vs standard criteria DNDD donors\naCPR, advanced cardiopulmonary resuscitation; A-NRP, abdominal normothermic regional perfusion; CIT, cold ischemic time; cPRA, calculated panel-reactive antibody; DGF, delayed graft function; DNDD, donation after the neurological determination of death; ICU, intensive care unit; IQR, interquartile range; PNF, primary nonfunction; T0, time of cardiac arrest; uDCDD, uncontrolled donation after the circulatory determination of death.\nNo statistically significant differences were observed in the clinical or demographic characteristics of donors and recipients between the 2 groups. However, recipients of uDCDD donors had a significantly shorter median cold ischemic time (CIT) compared with recipients of DNDD kidneys (15 h [IQR, 9–21] versus 19 h [IQR, 15.5–22];\nThe incidence of PNF was low in both groups, with no cases reported in the standard DNDD cohort and only 1 case (2.3%) in the uDCDD group (\nSequential serum creatinine levels (A) and glomerular filtration rate (B) in kidney recipients from both groups (in black uDCDD; in gray DNDD). The bar represents the median and the error bars the interquartile range for each time point. A general linear model for a repeated measures test was used. At all-time points\nKaplan-Meier graphs. A, Death-censored kidney graft survival. B, No death-censored kidney graft survival. DNDD, donation after the neurological determination of death; uDCDD, uncontrolled donation after the circulatory determination of death.\nRecipients of uDCDD kidneys had a significantly higher probability of developing DGF in univariate analysis (odds ratio, 3.2; 95% confidence interval [CI], 1.4-7.2;\nOur experience with the transplantation of highly selected uDCDD kidneys confirms the feasibility of these programs and demonstrates that posttransplant outcomes can be comparable with those achieved with standard criteria DNDD donors. Published literature on uDCDD kidney transplants has often reported suboptimal outcomes,\nTo justify the value of uDCDD programs, most researches have compared the outcomes of recipients of kidneys from uDCDD donors with those from DNDD donors meeting expanded criteria.\nOne major limitation of uDCDD outcomes is the lack of evolution in donor acceptance criteria during the past 2 decades. Some centers in Spain continue to use unrestricted donor selection criteria established >20 y ago,\nOur protocol was designed to minimize these risks by excluding donors who did not meet at least 1 of 3 ideal criteria: donor age younger than 50 y, no-flow period of <10 min, or WIT of <120 min. These criteria, informed by previous studies, aimed to optimize organ utilization and posttransplant outcomes. Viglietti et al\nPNF remains the most concerning complication of uDCDD kidney transplantation, with reported incidences ranging from 1.8% to 20%.\nSeveral factors such as advance donor age, prolonged WIT, extended no-flow periods, or in situ cooling as a preservation method have been found to be associated with high rates of PNF in uDCDD.\nDespite a higher DGF rate in our uDCDD group (46%) compared with the DNDD group, this was lower than what was previously documented.\nOur 5-y graft survival rate of 92% surpasses previously reported rates\nWe also analyzed data on kidney function in the long term, which were again appropriate in recipients of kidneys from uDCDD donors, with no differences in serum creatinine or eGFR values at 1 and 5 y compared with recipients of ideal DNDD kidneys. The description of analytical parameters in our series is of great importance since most studies published on uDCDD kidney transplantation have only reported survival data, with no information on graft function.\nOne of the main discouraging issues in uDCDD is the high kidney discard rate, >30% in large Spanish studies.\nWe would like to emphasize that the favorable results obtained in our center with the transplantation of uDCDD kidneys were not related to a restrictive selection of kidney recipients. In fact, there were no differences between both recipient groups in donor age, recipient age, rate of sensitized patients, or the percentage of second recipient transplant.\nThe keystone of successful uDCDD programs is optimizing all logistical requirements with the clear goal of reducing warm ischemic injury. Although we cannot interfere with donor age or in the duration of no-flow periods that all EMS try to reduce as much as possible to increase the probability of return of spontaneous circulation and save lives, we are able to reduce the duration of intrahospital procedures to minimize the period until A-NRP. In this regard, the obvious feature that distinguishes our uDCDD program from others is that both the diagnosis of death and the start of A-NRP were performed in the ICU. The centralization of both phases of the process in the same unit helped us to avoid in-hospital transfers that turned out to be difficult in multimonitored patients subject to mechanical cardiac compression and mechanical ventilation. This approach reduces WIT and largely contributes to improving graft quality.\nIn our study, all kidneys were perfused on pulsatile hypothermic machine perfusion (HMP). Pulsatile HMP has shown to be effective in reducing the incidence of DGF, both in DNDD and DCDD, but has not been associated with a decreased risk of PNF.\nThere has been a debate concerning the coexistence of E-CPR and uDCDD programs. In our hospital, E-CPR is available, and to activate the uDCDD procedure, it is mandatory to check that there is no indication to activate the E-CPR pathway. The inclusion criteria for both programs are different.\nWe are aware of the ethical-legal barriers to setting up DCDD programs,\nOur study has several limitations. Our kidney survival data have included our learning curve in uDCDD, and the study period is >10 y, although the number of cases with a prolonged follow-up is still extremely low. We are also aware of the small sample size and that our study was not randomized. Finally, although we used a control group with similar characteristics, recipients were selected to receive a DNDD or a uDCDD graft based on an assessment of donor-recipient compatibility and other data by the treating team, so we cannot exclude a certain selection bias.\nIn summary, our results support that obtaining renal grafts from uDCDD donors is not only feasible but also provides results that are as good as those obtained from standard DNDD donors. Strict donor selection criteria, improved in-hospital logistics to reduce WIT, and the systematic use of A-NRP are key factors for success in terms of kidney utilization and posttransplant outcomes.\nThe authors wish to thank all members of the donor coordination units and kidney transplant teams of centers that provided data on kidney recipients transplanted with uDCDD grafts obtained at our center.", "topic": "Brain"}
{"pmid": "39366088", "pmcid": "12301052", "title": "Hybrid Deep–Geometric Approach for Efficient Consistency Assessment of Stereo Images", "publication_year": "N/A", "abstract": "", "full_text": "Stereoscopic imaging enables depth perception by capturing a scene from two slightly different viewpoints, mimicking human binocular vision. It has been widely adopted across domains—from medicine and robotics to autonomous vehicles and entertainment—to facilitate reasoning about 3D structure [\nGeometric consistency in this context means that the relative camera geometry (orientation, focal alignment, etc.) is consistent with the stereo image pair, so that corresponding points in the scene lie on the same epipolar line in both images. Traditional solutions to enforce or evaluate this consistency have relied on explicit camera calibration procedures (e.g., using known calibration patterns or multiple images to estimate intrinsic/extrinsic parameters). While effective, such methods are labor-intensive and not always feasible in the field, where the calibration may drift over time or where only a single stereoscopic image pair is available. There is a clear need for automatic methods to assess stereo image geometric alignment from the image data alone, without requiring special calibration targets or prior camera parameters [\nRecently, Kowalczyk [\nTo address these limitations, we propose a fully deterministic and self-contained method for evaluating geometric consistency directly from a single stereo pair. The method estimates the fundamental matrix using robust feature matching and outlier rejection, and derives consistency indicators based on the deviation of epipolar line parameters. This enables accurate, calibration-free evaluation with minimal computational cost, making it particularly suitable for embedded or real-time systems. As illustrated in\nAt the same time, the past five years have seen rapid advances in deep learning for stereo vision. Tasks such as stereo matching and depth estimation have greatly benefited from convolutional neural networks (CNNs) and, more recently, Transformer-based architectures [\nThe main contributions of this work can be summarized:\nWe propose a hybrid geometric consistency assessment method (HGC-Net) that fuses classical epipolar geometry analysis with deep learning techniques.\nThe core metric, a scalar score\nWe enhance robustness and accuracy by integrating deep feature matchers (e.g., LoFTR), Transformer-based disparity estimators (e.g., STTR), and attention-based diagnostic modules.\nOur method outperforms baseline techniques in detecting both global miscalibration and local geometric anomalies, achieving correlation with simulated distortion levels.\nTo the best of our knowledge, this is the first method to combine no-reference geometric evaluation with learned components in a unified, interpretable framework.\nA longstanding approach to ensure stereo image consistency is rigorous camera calibration. Traditional calibration techniques (e.g., using the method of Zhang) estimate intrinsic and extrinsic parameters of each camera by observing a known target (checkerboard, etc.), after which images can be rectified to align epipolar lines [\nAnother category of methods assess stereo consistency via the disparity map (depth map) obtained from the stereo pair [\nMore broadly, the field of stereoscopic image quality assessment addresses how to predict the perceptual quality of a stereo pair without a reference image. Recent NR-SIQA methods use machine learning to handle various distortion types, including geometric misalignment. For example, Li et al. [\nIn the last five years, researchers have begun applying deep learning to estimate geometric alignment and calibration parameters from images. One notable example is the work of Poursaeed et al., who introduced deep networks to estimate the fundamental matrix F directly from raw image pairs [\nFor a given stereo pair (left image and right image), if the cameras have a consistent geometric relationship, all corresponding points lie on pairs of conjugate epipolar lines—i.e., the projection of a 3D point will appear at some position in the left image and at a position in the right image constrained to a specific line (the epipolar line) determined by the camera poses. In an ideal rectified stereo setup, these epipolar lines are horizontal scanlines of the images. Geometric inconsistencies (from miscalibration) manifest as tilted or vertically shifted epipolar lines between the two images. The core idea of the base method is to quantitatively measure such deviations.\nThe algorithm begins by finding a sparse set of feature correspondences between the left and right images. Keypoints are detected in both images, and feature descriptors are extracted (for example, using ORB or SURF features). The descriptors from the two images are then matched to identify candidate corresponding points.\nTo increase reliability, standard outlier rejection techniques are applied:\nLowe’s ratio test—to ensure unique best matches;\nsymmetry check—keeping only matches that are mutual;\nrobust estimation via RANSAC—to filter any remaining incorrect matches.\nBy the end of this process, we obtain a set of\nUsing these correspondences, the fundamental matrix\nFor each matched point\nIf the stereo geometry is perfect, these lines should be horizontal and aligned between views: ideally,\nNon-zero slope (\nA difference in intercept (\nThe method introduces a geometric consistency score\nSlope error\nThe average absolute slope of all epipolar lines in both images, defined as\nOffset error\nThe average vertical mismatch between epipolar lines, computed as\nEach component is scaled by a tunable elasticity coefficient\nThis yields a value\nThe coefficient\nThe process is computationally lightweight. Feature detection and matching (especially with ORB) is efficient even on high-resolution (1080p) images. The normalized eight-point algorithm [\nImportantly, the method is deterministic: given the same input and a fixed RANSAC seed, it produces the same\nThe base method was validated on the Middlebury stereo dataset and synthetic scenes with injected misalignments. It outperformed disparity-based and reprojection-based baselines by 5.7 and 4.1 percentage points, respectively. The score\nIn summary, the method provides\nA scalar score\nNo need for ground truth depth or camera calibration;\nHigh sensitivity to tilt and vertical shift misalignments;\nDeterministic and fast execution suitable for real-time use.\nThe limitation of the classical approach lies primarily in its dependence on detected feature correspondences. In very texture-poor scenes or extreme distortions, obtaining enough correct matches can be challenging. If the feature matching yields few inliers or a degenerate configuration, the fundamental matrix might be estimated inaccurately, and thus,\nTo address the aforementioned limitations and further improve the system, we integrate deep learning components into the classical pipeline at keypoints. Our hybrid approach preserves the interpretability and theoretical soundness of the epipolar geometry method while exploiting the power of learned representations to handle difficult scenarios. We focus on several aspects:\nInstead of relying solely on traditional feature detectors/descriptors (e.g., ORB, SURF), we utilize modern deep feature matching networks to obtain more robust correspondences. In particular, we incorporate LoFTR (Local Feature Transformer) as a drop-in replacement for ORB matching. LoFTR is a Transformer-based matcher that forgoes explicit keypoint detection and uses a CNN+Transformer backbone to densely match image patches. This results in a much denser and more reliable set of correspondences, covering low-texture or repetitive regions typically missed by ORB or SIFT.\nIn experiments on Middlebury and KITTI [\nWe also experimented with replacing or augmenting the normalized eight-point algorithm with a small CNN trained to regress the fundamental matrix\nThis method aims to handle difficult scenes with few explicit matches by inferring global alignment patterns. However, we observed that direct regression lacks interpretability and generalization, unless trained on a large variety of misalignment examples. A compromise we found useful was to use the regressed\nAs an auxiliary signal, we use stereo depth estimation networks such as STTR (Stereo Transformer) to validate stereo consistency. STTR does not assume calibration but relies on correct epipolar geometry to produce dense disparity maps. If the stereo pair is geometrically well-aligned, STTR outputs a clean map with high confidence. Misalignments result in degraded maps with invalid regions or high uncertainty.\nWe quantify this by analyzing STTR’s attention maps and per-pixel confidence. A simple metric—the mean confidence—correlates with the\nTo gain interpretability, we introduce an attention-based diagnostic module. After computing all epipolar lines, we generate an “epipolar deviation map” by assigning each row or pixel a value based on\nAs an experimental idea, we treated stereo generation as a conditional prediction task. Using an autoregressive model (e.g., PixelCNN or Transformer decoder), we attempt to generate the right image row by row from the left image. When the geometry is correct, the disparity is mostly constant per row, enabling accurate prediction. Misalignment introduces errors that accumulate in the predicted image. We observed that such a model’s loss increases with geometric distortion, showing potential as an alternative consistency check. Due to computational demands, we did not include it in the final pipeline.\nIn summary, our hybrid system enhances the classical pipeline with deep learning modules at three levels:\nInput enhancement: Deep feature matchers improve the quality of correspondences for\nAuxiliary validation: Networks like STTR provide an independent check on stereo geometry quality.\nInterpretability: Attention-based diagnostics localize misalignment sources.\nDespite these additions, the primary output remains the interpretable scalar score\nImplementation details. All deep components of HGC-Net were implemented using the PyTorch 2.2 framework and executed on an NVIDIA RTX-series GPU. We utilized publicly available pre-trained models for the LoFTR feature matcher [\nIn terms of runtime, the complete\nFor consistency classification, we set a high sensitivity threshold—specifically, we flag a pair as geometrically inconsistent if\nWe first evaluate the core geometric consistency metric\nThe disparity-based baseline similarly classified all these as consistent, as did the calibration-check baseline (since ground truth calibration is known for Middlebury). This sanity check confirms that when images are truly consistent, none of the methods raises a false alarm.\nTo evaluate robustness to geometric misalignment, we generated 6 synthetic variants (3 tilt, 3 shift) of 10 randomly selected Middlebury image pairs, totaling 60 test cases. Next, we assessed performance on synthetically distorted versions of Middlebury images. We introduced three levels of misalignment:\nMild: 0.5° tilt or 2 px vertical shift.\nModerate: 1° tilt or 5 px shift.\nSevere: 2° tilt or 10 px shift.\nThese represent scenarios from barely noticeable to obviously miscalibrated.\nSeveral observations can be made. Our\nAt\nThe calibration baseline (which “knows” the true camera parameters) performed slightly better than the disparity method at mild levels but still missed many\nWe note that for vertical shifts, our method is extremely sensitive—even a 2 px shift in 1080p images yields a noticeable drop in\nOverall, our method achieved 100% detection for all moderate and severe misalignments, and still a high rate for mild ones, whereas baselines only reached 100% at severe levels. The false positive rate was zero for all methods on perfectly aligned images, as previously noted.\nWe also computed correlation between distortion magnitude and the geometric consistency score\nWe next evaluated the effect of integrating deep learning into the method, focusing on difficult image pairs (e.g., low texture) and larger misalignments where classical matching might fail.\nFor instance, in the Middlebury Adirondack image with an added\nIn extremely low-texture scenes—such as the Blanket background from Middlebury, or synthetic scenes depicting a mostly uniform wall—ORB failed to find enough keypoints. LoFTR, however, still yielded dozens of valid matches using learned descriptors. In a simulated case with minimal texture, ORB could not estimate\nThe regression-based deep network from\nOne advantage of the learned approach is its runtime: estimating\nThe STTR stereo network provided useful qualitative insights. For well-aligned stereo pairs, its disparity maps exhibited over 95% high-confidence pixels. For misaligned pairs with a\nFor example, in one pair with\nThese results confirm the decision threshold of\nFinally, our prototype attention module produced anomaly heatmaps highlighting regions with large epipolar errors. In a road scene with added rotation, the attention map focused on the skyline region, consistent with global tilt. In vertically shifted pairs, the highlighted regions aligned with horizontal edges across the image. These results, while qualitative, support the idea of explainability: operators could be shown the most affected image areas (e.g., “features in the upper image region appear misaligned”), aiding in calibration diagnostics.\nThe results demonstrate that our hybrid geometric consistency assessment method effectively combines the strengths of classical geometry and deep learning. We discuss here several important aspects: improvements achieved, limitations, and implications for various use cases.\nAdvantages of the Hybrid Approach.\nBy integrating deep learning, we overcame key limitations of the classical method, namely its reliance on good feature points and its behavior in low-texture scenarios. Deep feature matchers (like LoFTR) provided resilience against even feature-poor images, ensuring that the fundamental matrix could be estimated accurately where a purely feature-based approach might fail. This greatly extends the utility of the method to environments like snowy landscapes, blank walls, or night scenes.\nMoreover, the deep networks added robustness to noise: learned features are often less sensitive to image noise or moderate illumination changes than handcrafted features, meaning our\nOne of the most significant outcomes is that our method can serve as a universal no-reference stereo QA tool. It does not require any knowledge of the scene or cameras, yet it yields a single interpretable metric (\nComparison to Prior Work.\nNo prior method, to our knowledge, addresses exactly the same problem in the same way. Related works, such as stereo rectification, disparity confidence estimation, and stereo IQA, share certain goals but differ fundamentally in scope and output. Our metric\nLimitations.\nOur approach assumes a static scene with sufficient overlap and minimal distortion. It may produce unreliable results in cases with dynamic content (e.g., moving objects), extremely wide baselines, or significant radial distortion unless pre-rectified. Additionally, deep components may need domain adaptation for non-standard imagery.\nThreshold Selection.\nThe metric\nPerfect calibration:\nMinor errors:\nClear misalignment:\nFuture Improvements.\nFuture directions include integrating temporal data for drift detection, applying the method to multi-camera systems, combining with other sensors (e.g., LiDAR), and embedding auto-rectification functionality.\nImplications.\nIn VR/AR, automated consistency scoring could improve user comfort. In robotics and autonomous vehicles,\nWe introduced a hybrid geometric consistency assessment pipeline, HGC-Net, which combines classical epipolar geometry with deep learning components to produce an interpretable scalar score\nBeyond detection accuracy, the method provides stable and monotonic responses to geometric deviations, supporting use in continuous calibration monitoring. Its real-time performance (12.5 fps on 1080p data) makes it practical for embedded systems in robotics, AR/VR, and autonomous platforms.\nLooking ahead, future work will focus on expanding evaluation to in-the-wild stereo datasets with real calibration faults, improving performance in extremely low-texture environments, and integrating the anomaly localization module into interactive diagnostic tools.", "content_for_embedding": "Stereoscopic imaging enables depth perception by capturing a scene from two slightly different viewpoints, mimicking human binocular vision. It has been widely adopted across domains—from medicine and robotics to autonomous vehicles and entertainment—to facilitate reasoning about 3D structure [\nGeometric consistency in this context means that the relative camera geometry (orientation, focal alignment, etc.) is consistent with the stereo image pair, so that corresponding points in the scene lie on the same epipolar line in both images. Traditional solutions to enforce or evaluate this consistency have relied on explicit camera calibration procedures (e.g., using known calibration patterns or multiple images to estimate intrinsic/extrinsic parameters). While effective, such methods are labor-intensive and not always feasible in the field, where the calibration may drift over time or where only a single stereoscopic image pair is available. There is a clear need for automatic methods to assess stereo image geometric alignment from the image data alone, without requiring special calibration targets or prior camera parameters [\nRecently, Kowalczyk [\nTo address these limitations, we propose a fully deterministic and self-contained method for evaluating geometric consistency directly from a single stereo pair. The method estimates the fundamental matrix using robust feature matching and outlier rejection, and derives consistency indicators based on the deviation of epipolar line parameters. This enables accurate, calibration-free evaluation with minimal computational cost, making it particularly suitable for embedded or real-time systems. As illustrated in\nAt the same time, the past five years have seen rapid advances in deep learning for stereo vision. Tasks such as stereo matching and depth estimation have greatly benefited from convolutional neural networks (CNNs) and, more recently, Transformer-based architectures [\nThe main contributions of this work can be summarized:\nWe propose a hybrid geometric consistency assessment method (HGC-Net) that fuses classical epipolar geometry analysis with deep learning techniques.\nThe core metric, a scalar score\nWe enhance robustness and accuracy by integrating deep feature matchers (e.g., LoFTR), Transformer-based disparity estimators (e.g., STTR), and attention-based diagnostic modules.\nOur method outperforms baseline techniques in detecting both global miscalibration and local geometric anomalies, achieving correlation with simulated distortion levels.\nTo the best of our knowledge, this is the first method to combine no-reference geometric evaluation with learned components in a unified, interpretable framework.\nA longstanding approach to ensure stereo image consistency is rigorous camera calibration. Traditional calibration techniques (e.g., using the method of Zhang) estimate intrinsic and extrinsic parameters of each camera by observing a known target (checkerboard, etc.), after which images can be rectified to align epipolar lines [\nAnother category of methods assess stereo consistency via the disparity map (depth map) obtained from the stereo pair [\nMore broadly, the field of stereoscopic image quality assessment addresses how to predict the perceptual quality of a stereo pair without a reference image. Recent NR-SIQA methods use machine learning to handle various distortion types, including geometric misalignment. For example, Li et al. [\nIn the last five years, researchers have begun applying deep learning to estimate geometric alignment and calibration parameters from images. One notable example is the work of Poursaeed et al., who introduced deep networks to estimate the fundamental matrix F directly from raw image pairs [\nFor a given stereo pair (left image and right image), if the cameras have a consistent geometric relationship, all corresponding points lie on pairs of conjugate epipolar lines—i.e., the projection of a 3D point will appear at some position in the left image and at a position in the right image constrained to a specific line (the epipolar line) determined by the camera poses. In an ideal rectified stereo setup, these epipolar lines are horizontal scanlines of the images. Geometric inconsistencies (from miscalibration) manifest as tilted or vertically shifted epipolar lines between the two images. The core idea of the base method is to quantitatively measure such deviations.\nThe algorithm begins by finding a sparse set of feature correspondences between the left and right images. Keypoints are detected in both images, and feature descriptors are extracted (for example, using ORB or SURF features). The descriptors from the two images are then matched to identify candidate corresponding points.\nTo increase reliability, standard outlier rejection techniques are applied:\nLowe’s ratio test—to ensure unique best matches;\nsymmetry check—keeping only matches that are mutual;\nrobust estimation via RANSAC—to filter any remaining incorrect matches.\nBy the end of this process, we obtain a set of\nUsing these correspondences, the fundamental matrix\nFor each matched point\nIf the stereo geometry is perfect, these lines should be horizontal and aligned between views: ideally,\nNon-zero slope (\nA difference in intercept (\nThe method introduces a geometric consistency score\nSlope error\nThe average absolute slope of all epipolar lines in both images, defined as\nOffset error\nThe average vertical mismatch between epipolar lines, computed as\nEach component is scaled by a tunable elasticity coefficient\nThis yields a value\nThe coefficient\nThe process is computationally lightweight. Feature detection and matching (especially with ORB) is efficient even on high-resolution (1080p) images. The normalized eight-point algorithm [\nImportantly, the method is deterministic: given the same input and a fixed RANSAC seed, it produces the same\nThe base method was validated on the Middlebury stereo dataset and synthetic scenes with injected misalignments. It outperformed disparity-based and reprojection-based baselines by 5.7 and 4.1 percentage points, respectively. The score\nIn summary, the method provides\nA scalar score\nNo need for ground truth depth or camera calibration;\nHigh sensitivity to tilt and vertical shift misalignments;\nDeterministic and fast execution suitable for real-time use.\nThe limitation of the classical approach lies primarily in its dependence on detected feature correspondences. In very texture-poor scenes or extreme distortions, obtaining enough correct matches can be challenging. If the feature matching yields few inliers or a degenerate configuration, the fundamental matrix might be estimated inaccurately, and thus,\nTo address the aforementioned limitations and further improve the system, we integrate deep learning components into the classical pipeline at keypoints. Our hybrid approach preserves the interpretability and theoretical soundness of the epipolar geometry method while exploiting the power of learned representations to handle difficult scenarios. We focus on several aspects:\nInstead of relying solely on traditional feature detectors/descriptors (e.g., ORB, SURF), we utilize modern deep feature matching networks to obtain more robust correspondences. In particular, we incorporate LoFTR (Local Feature Transformer) as a drop-in replacement for ORB matching. LoFTR is a Transformer-based matcher that forgoes explicit keypoint detection and uses a CNN+Transformer backbone to densely match image patches. This results in a much denser and more reliable set of correspondences, covering low-texture or repetitive regions typically missed by ORB or SIFT.\nIn experiments on Middlebury and KITTI [\nWe also experimented with replacing or augmenting the normalized eight-point algorithm with a small CNN trained to regress the fundamental matrix\nThis method aims to handle difficult scenes with few explicit matches by inferring global alignment patterns. However, we observed that direct regression lacks interpretability and generalization, unless trained on a large variety of misalignment examples. A compromise we found useful was to use the regressed\nAs an auxiliary signal, we use stereo depth estimation networks such as STTR (Stereo Transformer) to validate stereo consistency. STTR does not assume calibration but relies on correct epipolar geometry to produce dense disparity maps. If the stereo pair is geometrically well-aligned, STTR outputs a clean map with high confidence. Misalignments result in degraded maps with invalid regions or high uncertainty.\nWe quantify this by analyzing STTR’s attention maps and per-pixel confidence. A simple metric—the mean confidence—correlates with the\nTo gain interpretability, we introduce an attention-based diagnostic module. After computing all epipolar lines, we generate an “epipolar deviation map” by assigning each row or pixel a value based on\nAs an experimental idea, we treated stereo generation as a conditional prediction task. Using an autoregressive model (e.g., PixelCNN or Transformer decoder), we attempt to generate the right image row by row from the left image. When the geometry is correct, the disparity is mostly constant per row, enabling accurate prediction. Misalignment introduces errors that accumulate in the predicted image. We observed that such a model’s loss increases with geometric distortion, showing potential as an alternative consistency check. Due to computational demands, we did not include it in the final pipeline.\nIn summary, our hybrid system enhances the classical pipeline with deep learning modules at three levels:\nInput enhancement: Deep feature matchers improve the quality of correspondences for\nAuxiliary validation: Networks like STTR provide an independent check on stereo geometry quality.\nInterpretability: Attention-based diagnostics localize misalignment sources.\nDespite these additions, the primary output remains the interpretable scalar score\nImplementation details. All deep components of HGC-Net were implemented using the PyTorch 2.2 framework and executed on an NVIDIA RTX-series GPU. We utilized publicly available pre-trained models for the LoFTR feature matcher [\nIn terms of runtime, the complete\nFor consistency classification, we set a high sensitivity threshold—specifically, we flag a pair as geometrically inconsistent if\nWe first evaluate the core geometric consistency metric\nThe disparity-based baseline similarly classified all these as consistent, as did the calibration-check baseline (since ground truth calibration is known for Middlebury). This sanity check confirms that when images are truly consistent, none of the methods raises a false alarm.\nTo evaluate robustness to geometric misalignment, we generated 6 synthetic variants (3 tilt, 3 shift) of 10 randomly selected Middlebury image pairs, totaling 60 test cases. Next, we assessed performance on synthetically distorted versions of Middlebury images. We introduced three levels of misalignment:\nMild: 0.5° tilt or 2 px vertical shift.\nModerate: 1° tilt or 5 px shift.\nSevere: 2° tilt or 10 px shift.\nThese represent scenarios from barely noticeable to obviously miscalibrated.\nSeveral observations can be made. Our\nAt\nThe calibration baseline (which “knows” the true camera parameters) performed slightly better than the disparity method at mild levels but still missed many\nWe note that for vertical shifts, our method is extremely sensitive—even a 2 px shift in 1080p images yields a noticeable drop in\nOverall, our method achieved 100% detection for all moderate and severe misalignments, and still a high rate for mild ones, whereas baselines only reached 100% at severe levels. The false positive rate was zero for all methods on perfectly aligned images, as previously noted.\nWe also computed correlation between distortion magnitude and the geometric consistency score\nWe next evaluated the effect of integrating deep learning into the method, focusing on difficult image pairs (e.g., low texture) and larger misalignments where classical matching might fail.\nFor instance, in the Middlebury Adirondack image with an added\nIn extremely low-texture scenes—such as the Blanket background from Middlebury, or synthetic scenes depicting a mostly uniform wall—ORB failed to find enough keypoints. LoFTR, however, still yielded dozens of valid matches using learned descriptors. In a simulated case with minimal texture, ORB could not estimate\nThe regression-based deep network from\nOne advantage of the learned approach is its runtime: estimating\nThe STTR stereo network provided useful qualitative insights. For well-aligned stereo pairs, its disparity maps exhibited over 95% high-confidence pixels. For misaligned pairs with a\nFor example, in one pair with\nThese results confirm the decision threshold of\nFinally, our prototype attention module produced anomaly heatmaps highlighting regions with large epipolar errors. In a road scene with added rotation, the attention map focused on the skyline region, consistent with global tilt. In vertically shifted pairs, the highlighted regions aligned with horizontal edges across the image. These results, while qualitative, support the idea of explainability: operators could be shown the most affected image areas (e.g., “features in the upper image region appear misaligned”), aiding in calibration diagnostics.\nThe results demonstrate that our hybrid geometric consistency assessment method effectively combines the strengths of classical geometry and deep learning. We discuss here several important aspects: improvements achieved, limitations, and implications for various use cases.\nAdvantages of the Hybrid Approach.\nBy integrating deep learning, we overcame key limitations of the classical method, namely its reliance on good feature points and its behavior in low-texture scenarios. Deep feature matchers (like LoFTR) provided resilience against even feature-poor images, ensuring that the fundamental matrix could be estimated accurately where a purely feature-based approach might fail. This greatly extends the utility of the method to environments like snowy landscapes, blank walls, or night scenes.\nMoreover, the deep networks added robustness to noise: learned features are often less sensitive to image noise or moderate illumination changes than handcrafted features, meaning our\nOne of the most significant outcomes is that our method can serve as a universal no-reference stereo QA tool. It does not require any knowledge of the scene or cameras, yet it yields a single interpretable metric (\nComparison to Prior Work.\nNo prior method, to our knowledge, addresses exactly the same problem in the same way. Related works, such as stereo rectification, disparity confidence estimation, and stereo IQA, share certain goals but differ fundamentally in scope and output. Our metric\nLimitations.\nOur approach assumes a static scene with sufficient overlap and minimal distortion. It may produce unreliable results in cases with dynamic content (e.g., moving objects), extremely wide baselines, or significant radial distortion unless pre-rectified. Additionally, deep components may need domain adaptation for non-standard imagery.\nThreshold Selection.\nThe metric\nPerfect calibration:\nMinor errors:\nClear misalignment:\nFuture Improvements.\nFuture directions include integrating temporal data for drift detection, applying the method to multi-camera systems, combining with other sensors (e.g., LiDAR), and embedding auto-rectification functionality.\nImplications.\nIn VR/AR, automated consistency scoring could improve user comfort. In robotics and autonomous vehicles,\nWe introduced a hybrid geometric consistency assessment pipeline, HGC-Net, which combines classical epipolar geometry with deep learning components to produce an interpretable scalar score\nBeyond detection accuracy, the method provides stable and monotonic responses to geometric deviations, supporting use in continuous calibration monitoring. Its real-time performance (12.5 fps on 1080p data) makes it practical for embedded systems in robotics, AR/VR, and autonomous platforms.\nLooking ahead, future work will focus on expanding evaluation to in-the-wild stereo datasets with real calibration faults, improving performance in extremely low-texture environments, and integrating the anomaly localization module into interactive diagnostic tools.", "topic": "Brain"}
{"pmid": "39311855", "pmcid": "12308923", "title": "The effects of Gegen Qinlian decoction and its main constituents on glucolipid metabolic disorders in diabetes: an overview of systematic reviews and meta-analysis", "publication_year": "N/A", "abstract": "Currently, an increasing number of individuals diagnosed with diabetes mellitus (DM) are opting for combined functional substances in edible plant products, like herbal prescription or traditional Chinese medicine (TCM), to enhance the efficacy of blood glucose and lipid profile control. Among various TCM-included treatments available, Gegen Qinlian decoction (GQD) is frequently utilized. Nevertheless, the efficacy and safety of GQD and its main constituents (berberine, puerarin, etc.) in managing glucolipid metabolic disorders (GLMD) in DM remain under considerable debate. This overview aims to provide a concise summary of the key findings from systematic review and meta-analysis (SRMAs) on GQD and its main constituents for GLMD in DM, while also assessing the methodological quality of these reviews. A comprehensive search of online databases was conducted with no language restrictions, covering the period from inception to February 20th, 2025. The methodological quality, risk of bias, and strength of evidence of those included SRMAs were accessed. Overlapping randomized controlled trials (RCTs) were excluded, and the results of included meta-analysis were summarized. After screening, 35 studies matching the inclusion criteria were identified. The current evidence indicated that GQD, berberine, and puerarin appeared to be effective in improving the anti-diabetes role. As for blood lipid profile, it is indicated that GQD and berberine intervention could effectively reduce total cholesterol (TC), total triglycerides (TG), low-density lipoprotein (LDL-c) and increase high-density lipoprotein (HDL-c). Consequently, further high-quality primary studies are imperative to establish a more conclusive understanding of GQD and its main constituents for GLMD in DM.", "full_text": "With the improvement of our lifestyle, the global prevalence of metabolic diseases, represented by diabetes mellitus (DM) and diabetic complications, poses a serious threat to human life and health [\nTraditional Chinese medicine (TCM), primarily recognized for its use of functional herbs, encompasses a diverse array of bioactive compounds and has been extensively employed in China for millennia to treat metabolic diseases such as DM and obesity [\nDespite the explosion of review literature on the use of GQD and its main constituents for GLMD in DM, the overall efficacy of this approach was evaluated across different populations and focused on different outcome measures. Furthermore, few of these review studies were prepared strictly following the standards and the conclusions were limited by the quality of included trials and high heterogeneity. The reliability of the results was greatly affected by the quality of included trials. To provide patients and clinicians with a current and comprehensive reference on the efficacy, safety, and tolerability of GQD and its primary constituents as a treatment for GLMD in DM, we undertook an extensive analysis of pertinent systematic reviews. The principal aim of this overview of SRMAs is to synthesize evidence from SRMAs and deliver a succinct summary of the effects of GQD and its main constituents on GLMD in DM.\nThis overview was conducted following the guidance of the Cochrane Handbook 6.0 and reported this analyses in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines [\nInclusion and exclusion criteria are listed in Table\nInclusion and exclusion criteria\nAccording to the COMET (Core Outcome Measures in Effectiveness Trials) initiative [\nA comprehensive search was conducted using the electronic databases PubMed, EMBASE, Cochrane Library, Ovid, China National Knowledge Infrastructure (CNKI), and Wanfang Database, from their respective inception dates until July 15th, 2024 (last update). Provided that an appropriate English translation could be obtained, we placed no restriction on language of the review. To identify relevant meta-analysis studies, we used key search terms and variations of text words related to GQD and its main constituents, diabetes and meta-analysis study design: (“Gegen Qinlian Decoction” OR “GQD” OR “\nWe conducted the study selection process involved with the collaborative efforts of the authors (C.X. and Y.C.). Two reviewers screened the titles and abstracts of each identified report to determine eligibility for inclusion. Reports that were not excluded by both reviewers were further assessed through a full-text review, adhering to the predetermined inclusion criteria. We included the most recent and/or largest meta-analysis study when multiple pooled analyses were available for the same outcome. Any disagreements were resolved through discussion or consultation with a third author (H.W.) to establish a consensus. The extracted information mainly included the following: (1) basic information: authors, publication date, sample size, the interventions of experimental and control group, clinical outcomes, and adverse events; (2) Key elements of methodological evaluation; (3) Key elements of reported quality evaluation.\nA Measurement Tool to Assess Systematic Reviews Version 2.0 (AMSTAR 2) was used for the methodological quality assessment [\nThe heatmaps showed the relationship between included SRMAs and the original RCTs were created using GraphPad Prism 10. The value of CCA is effective to quantify the degree of overlap between two or more studies and to help decide how to deal with overlap. A graph crosstab (citation matrix) for SRMAs was created to help calculate CCA, providing the percentage of major research overlaps and evaluating the degree of overlap between them [\nCCAs are expressed as percentages, where\nWhen overlap between reviews was noted, two overview authors (C.X. and Y.C.) discussed the overlap with consideration of each review question and comparisons explored, the date of the last search and key aspects of methodological quality. To optimize the SRMA selection criteria, the following priority hierarchy and exclusion rules are proposed. Priority criteria include: (1) Quality assessment: Prioritize SRMAs with high quality elevated by AMSTAR2 tool. (2) Comprehensiveness: Favor SRMAs covering ≥ 80% of relevant primary studies, then 50–79%, and exclude those with < 50%. (3) Timeliness: Prioritize studies published within the past 5 years, then 5–10 years, and exclude older studies unless they are seminal works. (4) Topic specificity: Prioritize SRMAs whose research questions fully align with the current umbrella analysis over partially aligned ones. Exclusion rules: (1) For SRMAs with > 50% overlap in included primary studies, retain the one with a higher AMSTAR score or broader coverage of primary studies. (2) Exclude SRMAs whose primary studies are entirely covered by another SRMA.\nAll statistical syntheses were performed using Stata software (version MP 14.1). The aim of this review is to present a detailed summary of treatment-effect-safety data for GQD and its main constituents interventions aimed at GLMD management. An updated synthesis was used for evidence mapping. Operating according to previous methods and guidance, we used a random effects meta-analysis model to reanalyse the effect estimates for each outcome. We also calculated 95% prediction intervals and assessed excess significance bias as part of our reanalysis. For dichotomous outcomes, we calculated the RR as the effect size and used the Mantel‐Haenszel random‐effects method for the analysis. For continuous outcomes, we used the MD when the results were presented using the same scale/instruments. The performance test was 0.05.\nSelective reporting of adverse events was a common source of bias in meta-analyses and could lead to an underestimation of the true risk of a drug or intervention. Investigators/journals tend to report “positive results” (e.g. significant efficacy) and ignore adverse effects (analysed in the discussion). Therefore, we tracked the data from the primary studies, extracted standardised data, performed statistical analyses of adverse effects type, severity, frequency, causal relationship with the study intervention, and reported AEs. Those clinical trial platforms such as Clinical Trials.gov were searched to check whether the original study declared a monitoring plan for adverse reactions at the time of registration. The reports of regulatory agencies (such as FDA and EMA drug review reports) and authoritative high-level large-sample RCT studies were also checked. When obtaining unpublished data, it is necessary to comply with data sharing agreements and privacy regulations, and contact the authors to obtain the results of the data. Newly published studies will be regularly documented and updated the results of meta-analysis.\nWe collected information from review reports for the following prespecified subgroups: (1) typical form GQD or modified GQD; (2) concomitant medications; (3) duration. Overviews optimize evidence synthesis efficiency by utilizing existing meta-analytical outputs, minimizing redundant data reprocessing while maintaining validity.\nAs of July 15th, 2024, the literature search yielded 608 results. After removing duplicates and excluding papers based on title and abstract screening, 34 records matching the inclusion criteria were identified. On February 20th, 2025, the search had been updated, and 1 record of berberine was included in this analysis (Fig.\nThe PRISMA flow diagram of literature search and study selection process\nThe 11 reviews included were published between 2017 and 2024. Four reviews were published in English, while the remaining seven were published in Chinese. This comprehensive overview encompassed a total of 68 original studies involving 16,518 participants. The number of RCTs included in each SRMA varied from 5 to 33 studies. It is worth noting that all SRMAs employed appropriate tools to assess the methodological quality and risk of bias in the included RCTs. Specifically, 90.9% of the SRMAs (10 out of 11) utilized the Cochrane Risk of Bias (ROB) tool, while the remaining 9.1% (1 out of 11) employed the five-point Oxford Quality Scale, also known as the Jadad score. For the certainty of evidence, and the reporting standards, no SRMAs reported GRADE score, STRICTA or recommended that future studies should apply the STRICTA statement. Details of characteristics of included SRMAs see Table\nBasic characteristics of the included SRMAs on GQD for glucolipid metabolic disorders in DM\nThe overview included a total of 11 SRMAs, and the number of all original studies included was 180, 68 after the removal of duplicate literature. According to the formula, CCA = (182–68)/(68 × 10–68) × 100% = 18.63%, which indicated a large overlap. This reflects the unnecessary duplication of SRMAs on GQD. Future SRMAs should only be conducted when the research objectives are significantly different. Figure\nPairwise CCA assessment of the included SRMAs on GQD\nAfter selection of eligible SRMAs on main constituents of GQD for GLMD in DM, berberine and puerarin were identified as the only two constituents of GQD reported for clinical therapy or dietary supplementation for patients with DM. Three overviews of SRMAs on the role of berberine has been published, thus keeping an updated and analysis based on those studies could be a better choice to compare the difference of efficacy and safety among those interventions. Details of characteristics of included SRMAs see Supplementary Table S4-5. In brief, 22 SRMAs on berberine and 1 on puerarin were included. For SRMAs on berberine, 12 reviews and 3 overviews of SRMAs were published in English, and 8 were published in Chinese. Three comprehensive overviews cover less than half of SRMAs (9/20), while the results and conclusions in those SRMAs were almost consistent. In the only SRMA on puerarin published in Chinese, all puerarin were used as the injection for diabetic kidney disease (DKD). The prevention and management of diabetic microvascular complications with puerarin, rather than control of GLMD, might be put more emphasis on.\nThe methodological quality of the studies on GQD was assessed using AMSTAR-2. Upon critical evaluation, it was determined that 9 out of 11 reviews exhibited low or critically low quality due to the presence of multiple critical flaws, along with several non-critical weaknesses. Su [\nAMSTAR-2 for methodological quality of the included SRMAs on GQD\nAccording to the ROBIS tool, 27.3% (3/11) of the included SRMAs on GQD were determined to have an overall high risk of bias, while 72.7% (8/11) were found to have an overall low risk of bias (Table\nRisk of bias of the included SRMAs on GQD by ROBIS tool\nConsequently, only one SRMA of pre-existing GRADE evaluations met our inclusion criteria for direct incorporation. For the remaining comparisons, we performed a new critical appraisal of the certainty of the evidence based on the data from the relevant reviews.\nFour SRMAs were related to the effectiveness of GQD for GLMD in DM. The risk of bias, imprecision, and reporting bias were the main reasons for downgrading. There was moderate or low-grade evidence to indicate that GQD might improve the effectiveness, blood glucose, blood lipids, and BMI. Assessment of quality of evidence using GRADE and main outcomes were provided in Supplementary Table S8.\nThe main outcomes of GQD for GLMD in DM are shown in Fig.\nMain outcomes of GQD for GLMD in DM\nThis overview only used the existing subgroup analysis in SRMAs for high-efficiency analysis (Table\nSubgroup analyses for the effects of GQD on glucolipid metabolic disorders in DM\nLike GQD, there was low-grade to moderate evidence to indicate that berberine might improve the effectiveness, blood glucose, blood lipids, and BMI. Assessment of quality of evidence using GRADE and main outcomes were provided in Supplementary Table S9. The main outcomes of berberine for GLMD in DM are shown in Fig.\nMain outcomes of berberine for GLMD in DM and quality of evidence profiles\nAs for puerarin, only one SRMA [\nTwo SRMAs on GQD [\nMost of SRMAs (7/11) [\nThe results on berberine showed that there was no significant difference in the incidence of adverse reactions between the experimental group and the control group. The tolerable mild gastrointestinal symptoms were discovered to be the common adverse events, including abdominal pain, constipation, diarrhea, and bloating [\nTo the best of our knowledge, this is the first overview of SRMAs reporting on the efficacy of GQD and its main constituents for GLMD in DM. Identifying the most up-to-date evidence is clearly challenging because of overlap between reviews. Our laboratory had proved that GQD could manipulated gut microbiota and metabolites toward a more favorable profile, thereby exert a comprehensive anti-diabetes role [\nBerberine is one of the main constituents of\nHowever, GQD may have less adverse effects than berberine, especially for constipation and other gastrointestinal symptoms. It has been widely accepted that hyperglycemia is a major risk factor for the development and progression of diabetic complications. Berberine also has been proven to be beneficial for patients with DKD in two SRMAs [\nThough some slight symptoms, such as diarrhoea, were observed both in the Chinese medicinal herbs group, and Western medicine control group, reporting of adverse effects was poor and unsystematic, so we could not draw any conclusion about which medicine had more frequent or serious adverse effects. In fact, most Chinese clinical trials on Chinese medicinal herbs do not report adverse effects of the herbs or simply do not mention adverse effects. The toxicity of herbal medicines, especially hepatotoxicity, is increasingly recognised as herbal medicines become more popular. In order to obtain more information on the adverse effects of Chinese herbal medicines, safety needs to be further examined and reported in clinical trials and observational studies such as cohort studies and monitoring studies. Also, quantitative evaluation with scale such as Common Terminology Criteria for Adverse Events (CTCAE) may help to the accuracy of AEs.\nClinical epidemiology and evidence-based medicine have greatly promoted the development and internationalization of traditional Chinese medicine so that the quality of clinical research has been significantly improved [\nThe trials identified in this review were mostly Chinese and all were conducted in Chinese participants, thus publication bias might therefore exist. To help physicians to understand the effectiveness of GQD intervention in treatment of GLMD, we have described the elements and dosages of all the prescriptions included in this review in additional. There were large variations in the formulation, dosage, administration, duration of treatment, and control interventions in the included trials for the Chinese herbal medicines tested. Therefore, it was difficult to undertake subgroup analyses to explore factors that may have an impact on the effects of a treatment regimen. There is still a lack of information about quality control for the development of the herbal preparations or for the manufacture of herbal products. Due to methodological heterogeneity in HOMA-IR measurements across studies—particularly the amplification of SMD values by small sample sizes and irreconcilable unit inconsistencies—we abstained from pooled analysis of this outcome. The complete raw dataset is available in Supplementary Table S11 to facilitate transparency and secondary analyses. Future trials should provide information about standardisation, including composition, quality control, a detailed regimen, and duration of treatment.\nMechanistically, GQD alleviates hyperglycemia and lipid dysregulation in T2DM by orchestrating gut microbiota-bile acid crosstalk and inflammatory resolution, as evidenced by translational studies in murine models and human cohorts. GQD restructures the gut microbiome to selectively enrich the butyrogenic symbiont\nBerberine (BBR), a natural isoquinoline alkaloid, demonstrates multi-modal therapeutic efficacy against diabetic glucolipid metabolic disorders through coordinated regulation of insulin secretion, energy homeostasis, and cholesterol metabolism. BBR functions as a glucose-dependent insulin secretagogue by targeting pancreatic β-cell KCNH6 potassium channels, thereby reducing hyperglycemia without hypoglycemic risk [\nResearch endeavors centered on ethnopharmacology provide a scientific basis for establishing optimal dosages and assessing potential toxicological effects within local communities. Furthermore, such studies hold significant promise for the development of more efficacious multitarget pharmaceuticals designed to prevent and treat various ailments, including DM. Drawing from the successful research experience with Artemisinin, the selection of efficacious drug targets from TCM, exemplified by GQD and its main constituents, guided by the distinctive principles of TCM, expedites the drug discovery process. Furthermore, TCM and natural products contribute to the enhancement of anti-diabetes medications’ efficacy and play a significant role in the prevention and management of complications, thereby offering a novel perspective. This work offers a more focused analysis and contains precise conclusions rather than general formulations. Additionally, we propose several directions for future research, including the following: (1) Development of more new bioactive ingredients from medicine food homology and medicinal health food species for DM patients-centered treatment. (2) Explore safety and palatability. (3) Employ innovative technology like probiotic fermentation to increase efficiency and reduce toxicity of TCM (e.g. reduce gastrointestinal adverse reactions and improve bioavailability). (4) Transition from experience-based to evidence-based approaches in TCM. (5) Clinical trials. An advance registration contributes to improving transparency and minimizes potential bias. (6) More comprehensive and high-quality verification experiments.\nIn conclusion, existing very low to moderate quality evidence suggests that GQD may have potential benefits for people with diabetes in terms of long-term (HbA1c) and short-term blood glucose control (FBG, 2 h BG), weight management (BMI), insulin resistance levels (HOMA-IR), and lipid regulation (TC, TG, LDL-c, HDL-c). When used with anti-diabetes medications, the efficacy and safety outcomes were relatively better than used alone. However, its specific advantageous populations and long-term safety still need to be verified. The promotion of standardized preparations and high-quality, large-scale RCTs may be conducive to the acquisition of more advanced evidence. Further high-quality systematic reviews to support the conclusions summarized are also still necessary.\n\nAdditional file 1: Table S1. Identification of GQD and its main components using LC-MS/MS; Table S2. Blood consitutents of GQD; Table S3. Search terms and number of papers identified; Table S4. Basic characteristics of the included SRMAs on berberine for GLMD in DM; Table S5. Basic characteristics of the included SRMAs on puerarin for GLMD in DM; Table S6. AMSTAR-2 for methodological quality of the included SRMAs on berberine and puerarin; Table S7. Risk of bias of the included SRMAs on berberine and puerarin by ROBIS tool; Table S8. Details of GRADE scores for quality of studies on GQD; Table S9. Details of GRADE scores for quality of studies on berberine; Table S10. Details of GQD forms, withdral cases, and adverse events of GQD in original trials; Table S11. The effects of GQD for HOMA-IR.", "content_for_embedding": "With the improvement of our lifestyle, the global prevalence of metabolic diseases, represented by diabetes mellitus (DM) and diabetic complications, poses a serious threat to human life and health [\nTraditional Chinese medicine (TCM), primarily recognized for its use of functional herbs, encompasses a diverse array of bioactive compounds and has been extensively employed in China for millennia to treat metabolic diseases such as DM and obesity [\nDespite the explosion of review literature on the use of GQD and its main constituents for GLMD in DM, the overall efficacy of this approach was evaluated across different populations and focused on different outcome measures. Furthermore, few of these review studies were prepared strictly following the standards and the conclusions were limited by the quality of included trials and high heterogeneity. The reliability of the results was greatly affected by the quality of included trials. To provide patients and clinicians with a current and comprehensive reference on the efficacy, safety, and tolerability of GQD and its primary constituents as a treatment for GLMD in DM, we undertook an extensive analysis of pertinent systematic reviews. The principal aim of this overview of SRMAs is to synthesize evidence from SRMAs and deliver a succinct summary of the effects of GQD and its main constituents on GLMD in DM.\nThis overview was conducted following the guidance of the Cochrane Handbook 6.0 and reported this analyses in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines [\nInclusion and exclusion criteria are listed in Table\nInclusion and exclusion criteria\nAccording to the COMET (Core Outcome Measures in Effectiveness Trials) initiative [\nA comprehensive search was conducted using the electronic databases PubMed, EMBASE, Cochrane Library, Ovid, China National Knowledge Infrastructure (CNKI), and Wanfang Database, from their respective inception dates until July 15th, 2024 (last update). Provided that an appropriate English translation could be obtained, we placed no restriction on language of the review. To identify relevant meta-analysis studies, we used key search terms and variations of text words related to GQD and its main constituents, diabetes and meta-analysis study design: (“Gegen Qinlian Decoction” OR “GQD” OR “\nWe conducted the study selection process involved with the collaborative efforts of the authors (C.X. and Y.C.). Two reviewers screened the titles and abstracts of each identified report to determine eligibility for inclusion. Reports that were not excluded by both reviewers were further assessed through a full-text review, adhering to the predetermined inclusion criteria. We included the most recent and/or largest meta-analysis study when multiple pooled analyses were available for the same outcome. Any disagreements were resolved through discussion or consultation with a third author (H.W.) to establish a consensus. The extracted information mainly included the following: (1) basic information: authors, publication date, sample size, the interventions of experimental and control group, clinical outcomes, and adverse events; (2) Key elements of methodological evaluation; (3) Key elements of reported quality evaluation.\nA Measurement Tool to Assess Systematic Reviews Version 2.0 (AMSTAR 2) was used for the methodological quality assessment [\nThe heatmaps showed the relationship between included SRMAs and the original RCTs were created using GraphPad Prism 10. The value of CCA is effective to quantify the degree of overlap between two or more studies and to help decide how to deal with overlap. A graph crosstab (citation matrix) for SRMAs was created to help calculate CCA, providing the percentage of major research overlaps and evaluating the degree of overlap between them [\nCCAs are expressed as percentages, where\nWhen overlap between reviews was noted, two overview authors (C.X. and Y.C.) discussed the overlap with consideration of each review question and comparisons explored, the date of the last search and key aspects of methodological quality. To optimize the SRMA selection criteria, the following priority hierarchy and exclusion rules are proposed. Priority criteria include: (1) Quality assessment: Prioritize SRMAs with high quality elevated by AMSTAR2 tool. (2) Comprehensiveness: Favor SRMAs covering ≥ 80% of relevant primary studies, then 50–79%, and exclude those with < 50%. (3) Timeliness: Prioritize studies published within the past 5 years, then 5–10 years, and exclude older studies unless they are seminal works. (4) Topic specificity: Prioritize SRMAs whose research questions fully align with the current umbrella analysis over partially aligned ones. Exclusion rules: (1) For SRMAs with > 50% overlap in included primary studies, retain the one with a higher AMSTAR score or broader coverage of primary studies. (2) Exclude SRMAs whose primary studies are entirely covered by another SRMA.\nAll statistical syntheses were performed using Stata software (version MP 14.1). The aim of this review is to present a detailed summary of treatment-effect-safety data for GQD and its main constituents interventions aimed at GLMD management. An updated synthesis was used for evidence mapping. Operating according to previous methods and guidance, we used a random effects meta-analysis model to reanalyse the effect estimates for each outcome. We also calculated 95% prediction intervals and assessed excess significance bias as part of our reanalysis. For dichotomous outcomes, we calculated the RR as the effect size and used the Mantel‐Haenszel random‐effects method for the analysis. For continuous outcomes, we used the MD when the results were presented using the same scale/instruments. The performance test was 0.05.\nSelective reporting of adverse events was a common source of bias in meta-analyses and could lead to an underestimation of the true risk of a drug or intervention. Investigators/journals tend to report “positive results” (e.g. significant efficacy) and ignore adverse effects (analysed in the discussion). Therefore, we tracked the data from the primary studies, extracted standardised data, performed statistical analyses of adverse effects type, severity, frequency, causal relationship with the study intervention, and reported AEs. Those clinical trial platforms such as Clinical Trials.gov were searched to check whether the original study declared a monitoring plan for adverse reactions at the time of registration. The reports of regulatory agencies (such as FDA and EMA drug review reports) and authoritative high-level large-sample RCT studies were also checked. When obtaining unpublished data, it is necessary to comply with data sharing agreements and privacy regulations, and contact the authors to obtain the results of the data. Newly published studies will be regularly documented and updated the results of meta-analysis.\nWe collected information from review reports for the following prespecified subgroups: (1) typical form GQD or modified GQD; (2) concomitant medications; (3) duration. Overviews optimize evidence synthesis efficiency by utilizing existing meta-analytical outputs, minimizing redundant data reprocessing while maintaining validity.\nAs of July 15th, 2024, the literature search yielded 608 results. After removing duplicates and excluding papers based on title and abstract screening, 34 records matching the inclusion criteria were identified. On February 20th, 2025, the search had been updated, and 1 record of berberine was included in this analysis (Fig.\nThe PRISMA flow diagram of literature search and study selection process\nThe 11 reviews included were published between 2017 and 2024. Four reviews were published in English, while the remaining seven were published in Chinese. This comprehensive overview encompassed a total of 68 original studies involving 16,518 participants. The number of RCTs included in each SRMA varied from 5 to 33 studies. It is worth noting that all SRMAs employed appropriate tools to assess the methodological quality and risk of bias in the included RCTs. Specifically, 90.9% of the SRMAs (10 out of 11) utilized the Cochrane Risk of Bias (ROB) tool, while the remaining 9.1% (1 out of 11) employed the five-point Oxford Quality Scale, also known as the Jadad score. For the certainty of evidence, and the reporting standards, no SRMAs reported GRADE score, STRICTA or recommended that future studies should apply the STRICTA statement. Details of characteristics of included SRMAs see Table\nBasic characteristics of the included SRMAs on GQD for glucolipid metabolic disorders in DM\nThe overview included a total of 11 SRMAs, and the number of all original studies included was 180, 68 after the removal of duplicate literature. According to the formula, CCA = (182–68)/(68 × 10–68) × 100% = 18.63%, which indicated a large overlap. This reflects the unnecessary duplication of SRMAs on GQD. Future SRMAs should only be conducted when the research objectives are significantly different. Figure\nPairwise CCA assessment of the included SRMAs on GQD\nAfter selection of eligible SRMAs on main constituents of GQD for GLMD in DM, berberine and puerarin were identified as the only two constituents of GQD reported for clinical therapy or dietary supplementation for patients with DM. Three overviews of SRMAs on the role of berberine has been published, thus keeping an updated and analysis based on those studies could be a better choice to compare the difference of efficacy and safety among those interventions. Details of characteristics of included SRMAs see Supplementary Table S4-5. In brief, 22 SRMAs on berberine and 1 on puerarin were included. For SRMAs on berberine, 12 reviews and 3 overviews of SRMAs were published in English, and 8 were published in Chinese. Three comprehensive overviews cover less than half of SRMAs (9/20), while the results and conclusions in those SRMAs were almost consistent. In the only SRMA on puerarin published in Chinese, all puerarin were used as the injection for diabetic kidney disease (DKD). The prevention and management of diabetic microvascular complications with puerarin, rather than control of GLMD, might be put more emphasis on.\nThe methodological quality of the studies on GQD was assessed using AMSTAR-2. Upon critical evaluation, it was determined that 9 out of 11 reviews exhibited low or critically low quality due to the presence of multiple critical flaws, along with several non-critical weaknesses. Su [\nAMSTAR-2 for methodological quality of the included SRMAs on GQD\nAccording to the ROBIS tool, 27.3% (3/11) of the included SRMAs on GQD were determined to have an overall high risk of bias, while 72.7% (8/11) were found to have an overall low risk of bias (Table\nRisk of bias of the included SRMAs on GQD by ROBIS tool\nConsequently, only one SRMA of pre-existing GRADE evaluations met our inclusion criteria for direct incorporation. For the remaining comparisons, we performed a new critical appraisal of the certainty of the evidence based on the data from the relevant reviews.\nFour SRMAs were related to the effectiveness of GQD for GLMD in DM. The risk of bias, imprecision, and reporting bias were the main reasons for downgrading. There was moderate or low-grade evidence to indicate that GQD might improve the effectiveness, blood glucose, blood lipids, and BMI. Assessment of quality of evidence using GRADE and main outcomes were provided in Supplementary Table S8.\nThe main outcomes of GQD for GLMD in DM are shown in Fig.\nMain outcomes of GQD for GLMD in DM\nThis overview only used the existing subgroup analysis in SRMAs for high-efficiency analysis (Table\nSubgroup analyses for the effects of GQD on glucolipid metabolic disorders in DM\nLike GQD, there was low-grade to moderate evidence to indicate that berberine might improve the effectiveness, blood glucose, blood lipids, and BMI. Assessment of quality of evidence using GRADE and main outcomes were provided in Supplementary Table S9. The main outcomes of berberine for GLMD in DM are shown in Fig.\nMain outcomes of berberine for GLMD in DM and quality of evidence profiles\nAs for puerarin, only one SRMA [\nTwo SRMAs on GQD [\nMost of SRMAs (7/11) [\nThe results on berberine showed that there was no significant difference in the incidence of adverse reactions between the experimental group and the control group. The tolerable mild gastrointestinal symptoms were discovered to be the common adverse events, including abdominal pain, constipation, diarrhea, and bloating [\nTo the best of our knowledge, this is the first overview of SRMAs reporting on the efficacy of GQD and its main constituents for GLMD in DM. Identifying the most up-to-date evidence is clearly challenging because of overlap between reviews. Our laboratory had proved that GQD could manipulated gut microbiota and metabolites toward a more favorable profile, thereby exert a comprehensive anti-diabetes role [\nBerberine is one of the main constituents of\nHowever, GQD may have less adverse effects than berberine, especially for constipation and other gastrointestinal symptoms. It has been widely accepted that hyperglycemia is a major risk factor for the development and progression of diabetic complications. Berberine also has been proven to be beneficial for patients with DKD in two SRMAs [\nThough some slight symptoms, such as diarrhoea, were observed both in the Chinese medicinal herbs group, and Western medicine control group, reporting of adverse effects was poor and unsystematic, so we could not draw any conclusion about which medicine had more frequent or serious adverse effects. In fact, most Chinese clinical trials on Chinese medicinal herbs do not report adverse effects of the herbs or simply do not mention adverse effects. The toxicity of herbal medicines, especially hepatotoxicity, is increasingly recognised as herbal medicines become more popular. In order to obtain more information on the adverse effects of Chinese herbal medicines, safety needs to be further examined and reported in clinical trials and observational studies such as cohort studies and monitoring studies. Also, quantitative evaluation with scale such as Common Terminology Criteria for Adverse Events (CTCAE) may help to the accuracy of AEs.\nClinical epidemiology and evidence-based medicine have greatly promoted the development and internationalization of traditional Chinese medicine so that the quality of clinical research has been significantly improved [\nThe trials identified in this review were mostly Chinese and all were conducted in Chinese participants, thus publication bias might therefore exist. To help physicians to understand the effectiveness of GQD intervention in treatment of GLMD, we have described the elements and dosages of all the prescriptions included in this review in additional. There were large variations in the formulation, dosage, administration, duration of treatment, and control interventions in the included trials for the Chinese herbal medicines tested. Therefore, it was difficult to undertake subgroup analyses to explore factors that may have an impact on the effects of a treatment regimen. There is still a lack of information about quality control for the development of the herbal preparations or for the manufacture of herbal products. Due to methodological heterogeneity in HOMA-IR measurements across studies—particularly the amplification of SMD values by small sample sizes and irreconcilable unit inconsistencies—we abstained from pooled analysis of this outcome. The complete raw dataset is available in Supplementary Table S11 to facilitate transparency and secondary analyses. Future trials should provide information about standardisation, including composition, quality control, a detailed regimen, and duration of treatment.\nMechanistically, GQD alleviates hyperglycemia and lipid dysregulation in T2DM by orchestrating gut microbiota-bile acid crosstalk and inflammatory resolution, as evidenced by translational studies in murine models and human cohorts. GQD restructures the gut microbiome to selectively enrich the butyrogenic symbiont\nBerberine (BBR), a natural isoquinoline alkaloid, demonstrates multi-modal therapeutic efficacy against diabetic glucolipid metabolic disorders through coordinated regulation of insulin secretion, energy homeostasis, and cholesterol metabolism. BBR functions as a glucose-dependent insulin secretagogue by targeting pancreatic β-cell KCNH6 potassium channels, thereby reducing hyperglycemia without hypoglycemic risk [\nResearch endeavors centered on ethnopharmacology provide a scientific basis for establishing optimal dosages and assessing potential toxicological effects within local communities. Furthermore, such studies hold significant promise for the development of more efficacious multitarget pharmaceuticals designed to prevent and treat various ailments, including DM. Drawing from the successful research experience with Artemisinin, the selection of efficacious drug targets from TCM, exemplified by GQD and its main constituents, guided by the distinctive principles of TCM, expedites the drug discovery process. Furthermore, TCM and natural products contribute to the enhancement of anti-diabetes medications’ efficacy and play a significant role in the prevention and management of complications, thereby offering a novel perspective. This work offers a more focused analysis and contains precise conclusions rather than general formulations. Additionally, we propose several directions for future research, including the following: (1) Development of more new bioactive ingredients from medicine food homology and medicinal health food species for DM patients-centered treatment. (2) Explore safety and palatability. (3) Employ innovative technology like probiotic fermentation to increase efficiency and reduce toxicity of TCM (e.g. reduce gastrointestinal adverse reactions and improve bioavailability). (4) Transition from experience-based to evidence-based approaches in TCM. (5) Clinical trials. An advance registration contributes to improving transparency and minimizes potential bias. (6) More comprehensive and high-quality verification experiments.\nIn conclusion, existing very low to moderate quality evidence suggests that GQD may have potential benefits for people with diabetes in terms of long-term (HbA1c) and short-term blood glucose control (FBG, 2 h BG), weight management (BMI), insulin resistance levels (HOMA-IR), and lipid regulation (TC, TG, LDL-c, HDL-c). When used with anti-diabetes medications, the efficacy and safety outcomes were relatively better than used alone. However, its specific advantageous populations and long-term safety still need to be verified. The promotion of standardized preparations and high-quality, large-scale RCTs may be conducive to the acquisition of more advanced evidence. Further high-quality systematic reviews to support the conclusions summarized are also still necessary.\n\nAdditional file 1: Table S1. Identification of GQD and its main components using LC-MS/MS; Table S2. Blood consitutents of GQD; Table S3. Search terms and number of papers identified; Table S4. Basic characteristics of the included SRMAs on berberine for GLMD in DM; Table S5. Basic characteristics of the included SRMAs on puerarin for GLMD in DM; Table S6. AMSTAR-2 for methodological quality of the included SRMAs on berberine and puerarin; Table S7. Risk of bias of the included SRMAs on berberine and puerarin by ROBIS tool; Table S8. Details of GRADE scores for quality of studies on GQD; Table S9. Details of GRADE scores for quality of studies on berberine; Table S10. Details of GQD forms, withdral cases, and adverse events of GQD in original trials; Table S11. The effects of GQD for HOMA-IR.", "topic": "Brain"}
{"pmid": "39256424", "pmcid": "12300402", "title": "Exploring the Co-Structure of Physical Activity and Dietary Patterns in Relation to Emotional Well-Being: A Tanglegram-Based Multivariate Approach", "publication_year": "N/A", "abstract": "", "full_text": "The relationship between physical activity (PA), dietary intake (DI), and emotional stress in young people is complex and interrelated [\nThe quality of diet also plays an important role in maintaining mental well-being. Eating unhealthy foods is associated with greater stress, while following proper eating patterns can reduce stress symptoms [\nOf particular interest are studies conducted during the COVID-19 lockdown. This period led to a reduction in work-related physical activity and an increase in sedentary time, changes in eating habits, and a deterioration in mental well-being among the Florentine academic community [\nThe interplay between PA, DI, and emotional stress has gained increasing attention in health psychology and behavioral science. Numerous studies have highlighted the protective effects of regular PA on mental health, showing that physical activity—particularly moderate to vigorous intensity—can reduce symptoms of anxiety, depression, and stress through physiological (e.g., endorphin release, HPA axis modulation) and psychological mechanisms (e.g., improved self-esteem, distraction from rumination) [\nDietary behaviors are also closely linked to emotional well-being. Emotional stress has been associated with both undereating and overeating, depending on individual traits and context [\nEmerging research suggests that PA and DI behaviors often do not operate independently but rather form complex, compensatory patterns—particularly in individuals under stress. For example, some individuals may offset unhealthy eating habits by increasing physical activity, while others may reduce both PA and diet quality simultaneously during high-stress periods [\nA gap in our knowledge about the effects of PA and DI on emotional stress is their interaction. For older adolescents/young adults, combinations of PA and diet given their hormone levels and physiological development may have a range of effects depending on PA level, dietary habits, and other factors that can affect well-being. Therefore, the main objective of this study was to identify and compare complex patterns of lifestyle-related behaviors—defined by the integration of PA and DI variables—among university students experiencing varying levels of psychological stress. Accordingly, our study focuses on emotional well-being rather than clinical diagnosis. Specifically, we aimed to (1) examine overall multivariate differences in PA–DI behavioral configurations between low-distress (LD) and high-distress (HD) students using Permutational Multivariate Analysis of Variance (PERMANOVA) analysis; (2) construct and analyze the structural relationships between PA and DI variables in each group through co-phylogenetic analysis and tanglegram-based comparisons, evaluating the strength and coherence of these inter-domain associations; (3) visualize and compare distinct within-domain behavioral linkages (i.e., PA-to-PA and DI-to-DI) using two-face tanglegrams, highlighting patterns that are unique or divergent between low-distress (LD) and high-distress (HD) groups and to identify key behavior clusters that contribute most to group differentiation using dendrogram difference analysis, focusing on which specific PA and DI behaviors are structurally most dissimilar across distress levels; and (4) evaluate the predictive value of the identified behaviors using multiple logistic regression, estimating odds ratios (ORs) to quantify the likelihood of being in the HD group (whether the presence of particular behaviors meaningfully increases the odds of distress, thus offering potential behavioral signatures or markers that may help identify vulnerable individuals).\nA cross-sectional, non-random sample of university students participated in the study. Study recruitment and methodology are detailed elsewhere [\nEthical approval was obtained from the Senate Research Ethics Committee of Wroclaw University of Health and Sport Sciences (No. 13/2022). Participants gave informed e-consent after receiving full study information.\nSample size was determined using standard heuristics for exploratory multivariate analysis. To support clustering of 16 dietary and 4 physical activity variables, and logistic regression stratified by sex, a target of ≥240 participants was set—ensuring ≥30 cases per cluster or 10–20 cases per predictor for stable odds estimation [\nA total of 237 healthy first-year students (44% male) from physical education and physiotherapy programs at Wroclaw University of Health and Sport Sciences (2023) were initially recruited. To confirm that participants were healthy, a brief structured interview was conducted prior to enrollment. Students reported whether they regularly attended physical education classes and were medically cleared to participate. They were also asked if they had experienced any illness lasting more than seven days or sustained any injury within the past two months. Only those with no recent illness or injury and with medical clearance were included in the study. Participants were recruited as volunteers in response to an invitation presented during regular school activities. They did not receive any course credit or financial compensation for their participation. After excluding 28 participants due to missing data across multiple domains, the final analytical sample included 209 students (91 males and 118 females).\nEligibility required attendance in on-campus courses, age under 22, and informed consent. Exclusion criteria included university-level athletic training, medical leave (>3 weeks), or current illness/injury. Of 237 eligible students, 28 were excluded for missing data across multiple domains. Minor gaps in dietary intake data (\nThe final sample included 209 students: 91 males and 118 females. Male participants had a mean height of 183.64 cm (SD = 7.09), weight of 79.17 kg (SD = 10.03), and BMI of 23.56 kg/m\nA total of 28 students were excluded from the initial sample. Reasons for exclusion included incomplete dietary and psychological questionnaires, missing anthropometric data, and refusal to participate. The final sample consisted of students who met all inclusion criteria and provided complete data for analysis.\nDespite the reduced final sample (\nData were collected through the Family Lifestyle Patterns project (FAST-PAT23), which assessed physical activity, dietary habits, health-related attitudes, and socio-economic factors. Closed-ended questionnaires were administered via Google Forms immediately following a lecture (Human Anatomy) taught by one of the authors. Recruitment, data collection, and entry were conducted by the same author. Anthropometric measurements were taken over four consecutive weeks in March 2023, following a prearranged schedule.\nTo reduce individual-level variance attributable to sex or other demographic factors, all continuous variables were z-transformed (standardized) within the entire sample before analysis. This allowed us to investigate relative behavioral and psychological patterns across the whole cohort rather than focus on absolute group differences. This technique is commonly used in mixed-group physiological, anthropological, and behavioral studies to enable pattern-level comparisons across heterogeneous populations.\nAnthropometric measurements were performed in the Biokinetics Research Laboratory at Wroclaw University of Health and Sport Sciences, part of the Central Research Laboratory. The facility is certified under the Quality Management System standards PN-EN ISO 9001:2009 (Reg. No. PW-48606-10E) and PN-EN ISO 9001:2015 (Reg. No. PW-15105-22X). Body height was measured twice to the nearest 0.1 cm using a GPM anthropometer. Body weight and composition were assessed with the InBody230 analyzer. Body mass index (BMI) was subsequently calculated using formula: BMI = body weight [kg]/body height\nStudents’ physical activity levels were assessed using the Polish version of the IPAQ-Long Form, administered online via Google Forms following standardized guidelines. The questionnaire includes 11 items covering four domains—work/study, transport, domestic/gardening, and leisure—and one item on sitting time. Responses were converted to MET-min/week, with analyses including total PA, domain-specific activity, and sitting duration.\nDietary intake over the prior 12 months was assessed using the self-administered Questionnaire of Eating and Behaviors (QEB) [\nFood group consumption was reported on a six-point frequency scale, from “never” to “several times per day.” Responses were converted to average daily intake values: never = 0, 1–3/month = 0.06, once/week = 0.14, several/week = 0.5, daily = 1, and several/day = 2.\nDietary quality was assessed using two composite indices: (1) a pro-healthy index including wholegrain bread, milk, fermented dairy, curd cheese, fish, legumes, fruits, and vegetables, and (2) an unhealthy index comprising fast food, fried foods, cream cheese, sweets, canned meat/fish, sweetened beverages, energy drinks, and alcohol.\nThe Depression, Anxiety, and Stress Scale-21 Items (DASS-21) is a validated self-report tool measuring three related emotional states: depression, anxiety, and stress [\nThe questionnaire comprises 21 items divided evenly into three subscales [\nDepression (7 items): low mood, hopelessness, low self-esteem, and loss of motivation or interest;\nAnxiety (7 items): physiological arousal, situational fear, and panic-like symptoms;\nStress (7 items): chronic tension, irritability, and difficulty relaxing.\nParticipants were classified into high or low psychological distress groups based on established DASS-21 thresholds. The high distress group included individuals who either\nHad a total DASS-21 score > 60;\nScored in the severe/extremely severe range on at least one subscale—Depression > 21, Anxiety > 15, or Stress > 26 [\nAll others were categorized as low distress.\nAlthough the DASS-21 is primarily a screening measure and not a diagnostic tool, we selected it due to its brevity, strong psychometric properties, and suitability for adolescent school-based research. Its use enables a multidimensional assessment of emotional well-being (across depression, anxiety, and stress) within a limited timeframe, which was crucial given the large sample and school setting. We acknowledge that DASS-21 does not cover all behavioral or functional aspects of stress (e.g., sleep issues, academic performance, or eating problems beyond sugar intake). However, we also assessed selected health-related behaviors such as physical activity and dietary habits, including added sugar consumption, which are commonly linked with emotional distress. These complementary data provide partial behavioral context for interpreting DASS-21 scores. The limitations of relying on a single psychological instrument are further discussed in the Discussion section.\nNo missing data were found in anthropometric, body composition, or physical activity datasets. However, four participants had incomplete responses in the dietary intake questionnaire (QEB). Since principal component analysis (PCA) requires complete data, multiple imputation was applied. The missingness mechanism was identified as missing completely at random (MCAR) [\nResponses from the DASS-21, IPAQ, and QEB questionnaires were transformed using the Yeo–Johnson power transformation to approximate normal distributions [\nInitially, Permutational Multivariate Analysis of Variance (PERMANOVA) was applied to assess whether multivariate lifestyle behavior patterns—integrating physical activity (PA) and dietary intake (DI)—differed between low-distress (LD) and high-distress (HD) groups. This method was chosen for its robustness to violations of multivariate normality and its ability to evaluate group-level structure using Euclidean distances. Next, relationships between PA and DI patterns were explored within and between distress groups using tanglegrams and cophenetic statistics, derived from a cophylogenetic framework. Within each group (LD and HD), tanglegrams were generated to visualize clustering of PA and DI variables based on Euclidean distances and Ward’s linkage method. Procrustes and Procrustean Approach to Cophylogeny (PACo) tests were then performed to formally assess structural similarity between PA and DI configurations in each group. Tanglegrams and cophenetic statistics have also found applications in the social sciences focused on human behavior, including sociology, psychology, and education [\nThe significance level for all statistical tests was set at α = 0.05. All analyses were performed using RStudio (v.2024.11.0) and Statistica 13.0 (StatSoft, Cracow, Poland, 2018).\nTo enable meaningful analysis across the entire study population—including both men and women—the data were normalized and transformed. This step was necessary due to inherent sex-based differences in physical activity levels, dietary intake, and psychological responses, which could otherwise obscure genuine behavioral patterns when analyzed collectively. By applying normalization and appropriate transformation procedures (e.g., z-score standardization or Yeo–Johnson transformation), we ensured that all variables were placed on a comparable scale, reducing the influence of distributional differences related to sex.\nAnalyzing the combined group is theoretically justified, as it allows for the identification of generalizable behavioral structures and interaction patterns that transcend gender. This approach is particularly relevant in public health and educational contexts, where interventions are often designed and implemented at the population level rather than separately by sex. Moreover, integrating data from men and women increases statistical power and improves the robustness of multivariate pattern detection methods, such as clustering or co-structure analysis. Therefore, normalization was a crucial step to uncover shared PA–DI–distress dynamics across the full sample, without being confounded by baseline sex-related disparities.\nBased on the DASS-21 classification criteria, a total of 130 students were categorized as having low psychological distress (males: 67, females: 63), and 79 students were classified as having high psychological distress (males: 24, females: 55). These groupings were used in subsequent analyses comparing psychological distress levels across physical activity and dietary behavior variables. Significant differences were found between LD and HD groups across all DASS-21 components (\nPERMANOVA based on Euclidean distances revealed a significant difference in the overall multivariate structure of physical activity and dietary intake between LD and HD groups (F = 3.91, R\nTanglegram visualizations of physical activity (PA) and dietary intake (DI) revealed distinct behavioral pattern structures between the LD and HD groups (\nIn the LD group, PA variables formed three clusters: (1) moderate activity, domestic and gardening, transport, walking, and working; (2) vigorous and leisure-time activity; and (3) sitting time. The first cluster was linked to fried meals and sweetened beverages. Working, walking, and transport were associated with curd and yellow cheese intake, while vigorous leisure-time activity correlated with higher consumption of sweets and legumes. Increased sitting time was most often associated with fermented milk intake.\nIn the HD group, four PA clusters emerged: (1) transport and walking; (2) moderate activity with working and domestic gardening; (3) vigorous leisure-time activity (as in LD); and (4) sitting time, forming its own cluster. The first cluster was linked to healthier dietary habits, including higher fruit and vegetable intake. In contrast, moderate activity related to work was associated with fried meals and sweetened beverages, while domestic gardening correlated with curd cheese. Vigorous leisure activity was linked to canned meals and yellow cheese. Higher sitting time was associated with increased fast food consumption.\nCross-domain links between PA and DI were more thematically structured in the HD group. Vigorous PA often coincided with fast food intake, and moderate PA with fried meals, suggesting more consolidated—but potentially compensatory—lifestyle patterns among individuals with higher emotional distress.\nThese visual patterns were statistically evaluated using Procrustes analysis, which assessed the alignment between PA and DI behavior profiles within each group. In the LD group, the Procrustes correlation was t\nUnlike Procrustes, PACo analysis evaluates structural congruence based on internal pairwise relationships. It revealed significant PA–DI alignment in both groups. In the LD group, the sum of squares was 126.36 (\nThese findings support the notion that individuals under emotional strain may rely on more rigid or habit-driven combinations of physical activity and dietary choices, whereas those with lower distress levels may maintain more flexible and independently regulated lifestyle patterns.\nSimilarity between analogous behaviors (PA and DI) was assessed in two stages. First, paired dendrograms for LD and HD groups were visually compared side by side, separately for PA and DI, with matching labels directly connected. Second, this visual assessment was supported by cophenetic analysis to quantify structural similarities.\nTanglegrams (\nIn both dendrograms, leisure-time and vigorous activity consistently cluster together, as do moderate activity and domestic gardening, reflecting shared behavioral structures. However, differences emerge in the hierarchical positioning of working and sitting, which fall into separate sub-clades across groups—suggesting variations in activity context or co-occurrence patterns.\nThe cophenetic correlation between the two dendrograms was r = 0.72, indicating moderate structural similarity. This was supported by the Mantel test (r = 0.75,\nDescriptive statistics showed comparable average pairwise distances in PAPs: 13.46 ± 2.46 for LD and 10.47 ± 1.99 for HD, indicating similar overall variability within each group.\nOverall, these findings indicate moderate congruence in physical activity structures between LD and HD groups, with both overlapping and distinct behavioral clustering patterns.\nDespite some matched variable pairs—highlighted by colored lines—the overall dendrogram structures differ between LD and HD groups. For instance, fermented milk and milk cluster together in both groups, but items like fast food, sweets, and energy drinks appear in different sub-clades, reflecting divergence in dietary patterns linked to emotional distress.\nCophenetic correlation between dendrograms was low (r = 0.21), indicating weak structural similarity. Baker’s Gamma (r = 0.11) also showed low, though non-zero, correspondence. The Mantel test found no significant relationship between distance matrices (r = −0.00009,\nDescriptive statistics showed slightly higher mean pairwise distances in the LD group (14.23 ± 1.72) than in the HD group (12.87 ± 1.67), suggesting greater internal variability in dietary behaviors among emotionally healthier individuals.\nOverall, the findings suggest that while some pairwise associations are preserved, the clustering of dietary intake behaviors differs meaningfully between LD and HD groups, likely reflecting distinct co-occurrence patterns shaped by emotional distress.\nBased on dendrogram topology comparisons and clustering differences identified using all.equal() and dend_diff(), a subset of PA and DI behaviors that shifted structurally between LD and HD groups was selected. These variables were entered into a multiple logistic regression model to assess their predictive value for high distress. Odds ratios (ORs) with 95% confidence intervals were calculated to quantify the associated risk for each behavior.\nTo identify candidate variables associated with emotional distress, hierarchical dendrograms were compared using the all.equal() function. For physical activity (PA), the LD and HD dendrograms showed a mean relative difference in branch heights of 0.218, indicating moderate divergence in clustering strength. Topological differences were also noted, with unique edges (edge 7 in LD and edge 6 in HD) reflecting localized reorganization of activity patterns across distress levels.\nFor dietary intake (DI), the mean branch height difference was 0.141, suggesting a smaller overall shift than in PA. However, topological differences were more pronounced, with 12 unique edges in each group’s dendrogram. This indicates substantial re-clustering of dietary behaviors associated with emotional distress.\nBased on structural differences, candidate variables were selected for stepwise multiple logistic regression (MLR). Priority was given to behaviors involved in topological changes—particularly those re-clustered across groups or shifting between major branches. For PA, selected variables included the following: PA_sitting (differently positioned), PA_vigorous, and PA_leisure.time (more closely clustered in HD). For DI, key behaviors were DI_fastfood and DI_sweets (tightly clustered in HD), DI_alcoholic.drinks (shifted from peripheral to central), and DI_yellow.cheese (paired with fast food in HD). These variables were used to estimate the likelihood of HD classification, with odds ratios (ORs) quantifying the strength of each association.\nA multivariate logistic regression (MLR) was conducted to examine associations between lifestyle behaviors and psychological distress, comparing low-distress (LD) and high-distress (HD) groups, with HD modeled as the outcome (HD = 1). The initial model included all candidate variables, and predictor significance was evaluated. Given high model variability and an AIC of 265.35, a stepwise approach was used to remove weaker correlates and identify the most discriminative behaviors. Final results are shown in\nVigorous physical activity (PA_vigorous) was significantly associated with higher odds of high distress; individuals reporting such activity had 1.80 times higher odds of belonging to the HD group (OR = 1.80, 95% CI: 1.33–2.50,\nTaken together, these findings suggest that individuals with higher psychological distress tend to exhibit a complex behavioral profile—combining unhealthy dietary habits (e.g., increased fast food and sweets consumption) with compensatory behaviors like vigorous physical activity. This pattern may reflect an attempt to manage distress through mixed lifestyle strategies.\nIn this work, we were interested in studying the associations between physical activity and dietary patterns in participants with different levels of emotional well-being. This study provides novel insights into the relationship between lifestyle behaviors and psychological distress in university students, revealing that individuals with high distress exhibit both distinct and more structurally consolidated patterns of physical activity and dietary intake. Multivariate and dendrogram-based analyses confirmed significant, though modest, differences in overall behavioral structure between high- and low-distress groups. Notably, emotionally distressed individuals tended to engage in compensatory lifestyle strategies, combining unhealthy dietary habits (e.g., increased fast food and sweets intake) with vigorous physical activity. These patterns—supported by both structural clustering and logistic regression results—suggest a more rigid or thematically organized lifestyle configuration under distress, possibly reflecting efforts to self-regulate through health-compromising and health-promoting behaviors in tandem.\nNumerous studies have shown that frequent and strenuous physical activity positively affects a person health and well-being in general [\nSystematic review and meta-analysis of 59 studies on undergraduate students found that physical activity interventions significantly reduce symptoms of anxiety (standardized mean difference [SMD] = −0.88), depression (SMD = −0.73), and stress (SMD = −0.61). Despite considerable heterogeneity and some methodological limitations, the evidence supports physical activity as an effective means to enhance mental health in this population. However, more rigorously designed interventions are needed for stronger conclusions [\nA second meta-analysis published in 2024 showed that moderate and vigorous physical activity to replace or intermittent sedentary behavior (SB), which could effectively prevent or improve the harm of SB to physical and mental health [\nThe regulatory benefits of PA in mood may be related to the hippocampus and neurotransmitters because the hippocampus is the central brain region that regulates anxiety, and PA could effectively promote the growth of the hippocampus nerve [\nThe results of studies conducted in recent years suggest that diet plays a key role in mental health [\nAdherence to healthy or Mediterranean dietary patterns—high intake of fruits, vegetables, nuts and legumes; moderate consumption of poultry, eggs and dairy products; and only occasional consumption of red meat—has been shown to be associated with a reduced risk of depression. A systematic review and meta-analysis of observational studies by Lassale et al., 2019 [\nThe relationship between diet and mental status is undoubtedly complex and multidirectional. An increased intake of fruit, vegetables, and nuts, as well as a re-education of the intake of pro-inflammatory foods such as processed meat and trans fats, and alcohol, can reduce inflammation in the body. The consumption of foods with antioxidant and anti-inflammatory properties may provide important protection and reduce neuronal damage caused by oxidative stress [\nUnhealthy foods, sugary desserts and drinks, and low intake of fruit, vegetables, and whole-grain products are associated with elevated levels of inflammatory markers in the blood [\nA study of Iranian students found that a “Western diet” dietary pattern was not associated with mental health problems, strong inverse association was observed between the “plant-based” dietary pattern and depression [\nParticipation in physical activity is also an important factor for good health behavior. The group of students studied showed high and medium levels of physical activity resulting, among other things, from participation in sports activities implemented as part of the study program. Physical activity, as emphasized by other authors, is an important mediator that influences both nutritional choices and improves mental health [\nAlthough females were overrepresented in the high-distress group, subgroup analyses stratified by both sex and stress level were not feasible due to small sample sizes, especially among high-distress males. We addressed this limitation by standardizing continuous variables and analyzing the full cohort as a whole. This approach allowed us to examine general behavioral and emotional patterns across adolescents while minimizing the influence of unequal subgroup sizes. Nevertheless, we acknowledge that the gender imbalance may still act as a potential confounder and recommend future studies with larger and more balanced samples.\nStrengths and limitations. This study has few strengths. This case–control study is uniquely involving physically actively males and females examined to the relation between prominent eating patterns and the severity of emotional well-being through depression, anxiety, and stress levels with a mediation role of body composition and odds of the low emotional well-being. Nevertheless, a few limitations also need to be considered. One of the limitations of the present study was the cross-sectional design, which prohibits causal inference. The results of our study cannot be generalized to the population. The study group consisted of students at the University of Public Health, who have a higher physical activity level than the Polish population of their age, in addition to having a greater number of classes in the study program on the subject of good nutritional habits and physical activity. In addition, the unequal gender distribution across distress groups—particularly the small number of males in the high-distress category—prevented statistically reliable subgroup analyses stratified by both sex and psychological distress level. Consequently, we analyzed the full sample as a unified cohort. To minimize potential confounding effects related to gender and individual variability, all continuous variables were standardized prior to analysis. This normalization-based approach, commonly used in anthropometric and developmental physiology research, allowed us to focus on relative behavioral structures across the adolescent population. Another limitation was the small number of participants covering only the minimum requirements of exploratory methods. Moreover, dropout observed during recruitment lowered the sample size in comparison to the needed. Also slightly bias toward a greater number of females was a certain limitation. Additionally, first-year university students undergo numerous lifestyle transitions, including changes in diet, physical activity, and stress levels. Greater autonomy in food choices, exposure to new physical activity options, peer influence, and adaptation to a new academic environment may have affected participants’ behaviors and psychological responses. Since the data collection took place a few weeks after the beginning of the academic year, future studies should consider longitudinal approaches to monitor how these behaviors evolve over time. Such designs could help identify whether initial changes persist, stabilize, or intensify during the academic year.\nIn the future, more precise research tools for both eating habits and levels of physical activity and psychological well-being should also be used for this group of subjects. Comparative studies should also have been carried out in overweight and obese people with low levels of physical activity and less knowledge of proper eating habits. Additional factors worth addressing in future research include body image, dieting behaviors, participation in sport, and access to healthier food and exercise opportunities, all of which may influence student well-being during their transition to university life.\nThis study demonstrates that emotional distress in university students is linked to distinct lifestyle behavior profiles, characterized by both unhealthy eating patterns and elevated levels of vigorous physical activity. Multivariate analyses revealed meaningful differences in the structure and co-occurrence of physical activity and dietary behaviors between low- and high-distress groups. Individuals with high distress showed more tightly coupled, potentially compensatory behavior patterns, suggesting that emotional strain may drive more habitual or reactive lifestyle strategies.\nThese findings highlight the need for targeted interventions that address both domains of behavior simultaneously. Programs aimed at student well-being should not only promote healthy eating and physical activity separately but should also recognize the complexity of how these behaviors cluster in response to psychological distress. Screening tools that identify individuals with mixed behavior profiles—such as high exercise levels combined with high processed food intake—may help flag those at an elevated risk of emotional strain.\nWe recommend integrating mental health screening into campus health promotion efforts, especially for physically active students. Additionally, interventions should account for compensatory behavior patterns, recognizing that health-promoting and risk behaviors may co-occur. Finally, using cluster-based or pattern-recognition approaches—rather than focusing on isolated habits—can help better tailor behavioral interventions.", "content_for_embedding": "The relationship between physical activity (PA), dietary intake (DI), and emotional stress in young people is complex and interrelated [\nThe quality of diet also plays an important role in maintaining mental well-being. Eating unhealthy foods is associated with greater stress, while following proper eating patterns can reduce stress symptoms [\nOf particular interest are studies conducted during the COVID-19 lockdown. This period led to a reduction in work-related physical activity and an increase in sedentary time, changes in eating habits, and a deterioration in mental well-being among the Florentine academic community [\nThe interplay between PA, DI, and emotional stress has gained increasing attention in health psychology and behavioral science. Numerous studies have highlighted the protective effects of regular PA on mental health, showing that physical activity—particularly moderate to vigorous intensity—can reduce symptoms of anxiety, depression, and stress through physiological (e.g., endorphin release, HPA axis modulation) and psychological mechanisms (e.g., improved self-esteem, distraction from rumination) [\nDietary behaviors are also closely linked to emotional well-being. Emotional stress has been associated with both undereating and overeating, depending on individual traits and context [\nEmerging research suggests that PA and DI behaviors often do not operate independently but rather form complex, compensatory patterns—particularly in individuals under stress. For example, some individuals may offset unhealthy eating habits by increasing physical activity, while others may reduce both PA and diet quality simultaneously during high-stress periods [\nA gap in our knowledge about the effects of PA and DI on emotional stress is their interaction. For older adolescents/young adults, combinations of PA and diet given their hormone levels and physiological development may have a range of effects depending on PA level, dietary habits, and other factors that can affect well-being. Therefore, the main objective of this study was to identify and compare complex patterns of lifestyle-related behaviors—defined by the integration of PA and DI variables—among university students experiencing varying levels of psychological stress. Accordingly, our study focuses on emotional well-being rather than clinical diagnosis. Specifically, we aimed to (1) examine overall multivariate differences in PA–DI behavioral configurations between low-distress (LD) and high-distress (HD) students using Permutational Multivariate Analysis of Variance (PERMANOVA) analysis; (2) construct and analyze the structural relationships between PA and DI variables in each group through co-phylogenetic analysis and tanglegram-based comparisons, evaluating the strength and coherence of these inter-domain associations; (3) visualize and compare distinct within-domain behavioral linkages (i.e., PA-to-PA and DI-to-DI) using two-face tanglegrams, highlighting patterns that are unique or divergent between low-distress (LD) and high-distress (HD) groups and to identify key behavior clusters that contribute most to group differentiation using dendrogram difference analysis, focusing on which specific PA and DI behaviors are structurally most dissimilar across distress levels; and (4) evaluate the predictive value of the identified behaviors using multiple logistic regression, estimating odds ratios (ORs) to quantify the likelihood of being in the HD group (whether the presence of particular behaviors meaningfully increases the odds of distress, thus offering potential behavioral signatures or markers that may help identify vulnerable individuals).\nA cross-sectional, non-random sample of university students participated in the study. Study recruitment and methodology are detailed elsewhere [\nEthical approval was obtained from the Senate Research Ethics Committee of Wroclaw University of Health and Sport Sciences (No. 13/2022). Participants gave informed e-consent after receiving full study information.\nSample size was determined using standard heuristics for exploratory multivariate analysis. To support clustering of 16 dietary and 4 physical activity variables, and logistic regression stratified by sex, a target of ≥240 participants was set—ensuring ≥30 cases per cluster or 10–20 cases per predictor for stable odds estimation [\nA total of 237 healthy first-year students (44% male) from physical education and physiotherapy programs at Wroclaw University of Health and Sport Sciences (2023) were initially recruited. To confirm that participants were healthy, a brief structured interview was conducted prior to enrollment. Students reported whether they regularly attended physical education classes and were medically cleared to participate. They were also asked if they had experienced any illness lasting more than seven days or sustained any injury within the past two months. Only those with no recent illness or injury and with medical clearance were included in the study. Participants were recruited as volunteers in response to an invitation presented during regular school activities. They did not receive any course credit or financial compensation for their participation. After excluding 28 participants due to missing data across multiple domains, the final analytical sample included 209 students (91 males and 118 females).\nEligibility required attendance in on-campus courses, age under 22, and informed consent. Exclusion criteria included university-level athletic training, medical leave (>3 weeks), or current illness/injury. Of 237 eligible students, 28 were excluded for missing data across multiple domains. Minor gaps in dietary intake data (\nThe final sample included 209 students: 91 males and 118 females. Male participants had a mean height of 183.64 cm (SD = 7.09), weight of 79.17 kg (SD = 10.03), and BMI of 23.56 kg/m\nA total of 28 students were excluded from the initial sample. Reasons for exclusion included incomplete dietary and psychological questionnaires, missing anthropometric data, and refusal to participate. The final sample consisted of students who met all inclusion criteria and provided complete data for analysis.\nDespite the reduced final sample (\nData were collected through the Family Lifestyle Patterns project (FAST-PAT23), which assessed physical activity, dietary habits, health-related attitudes, and socio-economic factors. Closed-ended questionnaires were administered via Google Forms immediately following a lecture (Human Anatomy) taught by one of the authors. Recruitment, data collection, and entry were conducted by the same author. Anthropometric measurements were taken over four consecutive weeks in March 2023, following a prearranged schedule.\nTo reduce individual-level variance attributable to sex or other demographic factors, all continuous variables were z-transformed (standardized) within the entire sample before analysis. This allowed us to investigate relative behavioral and psychological patterns across the whole cohort rather than focus on absolute group differences. This technique is commonly used in mixed-group physiological, anthropological, and behavioral studies to enable pattern-level comparisons across heterogeneous populations.\nAnthropometric measurements were performed in the Biokinetics Research Laboratory at Wroclaw University of Health and Sport Sciences, part of the Central Research Laboratory. The facility is certified under the Quality Management System standards PN-EN ISO 9001:2009 (Reg. No. PW-48606-10E) and PN-EN ISO 9001:2015 (Reg. No. PW-15105-22X). Body height was measured twice to the nearest 0.1 cm using a GPM anthropometer. Body weight and composition were assessed with the InBody230 analyzer. Body mass index (BMI) was subsequently calculated using formula: BMI = body weight [kg]/body height\nStudents’ physical activity levels were assessed using the Polish version of the IPAQ-Long Form, administered online via Google Forms following standardized guidelines. The questionnaire includes 11 items covering four domains—work/study, transport, domestic/gardening, and leisure—and one item on sitting time. Responses were converted to MET-min/week, with analyses including total PA, domain-specific activity, and sitting duration.\nDietary intake over the prior 12 months was assessed using the self-administered Questionnaire of Eating and Behaviors (QEB) [\nFood group consumption was reported on a six-point frequency scale, from “never” to “several times per day.” Responses were converted to average daily intake values: never = 0, 1–3/month = 0.06, once/week = 0.14, several/week = 0.5, daily = 1, and several/day = 2.\nDietary quality was assessed using two composite indices: (1) a pro-healthy index including wholegrain bread, milk, fermented dairy, curd cheese, fish, legumes, fruits, and vegetables, and (2) an unhealthy index comprising fast food, fried foods, cream cheese, sweets, canned meat/fish, sweetened beverages, energy drinks, and alcohol.\nThe Depression, Anxiety, and Stress Scale-21 Items (DASS-21) is a validated self-report tool measuring three related emotional states: depression, anxiety, and stress [\nThe questionnaire comprises 21 items divided evenly into three subscales [\nDepression (7 items): low mood, hopelessness, low self-esteem, and loss of motivation or interest;\nAnxiety (7 items): physiological arousal, situational fear, and panic-like symptoms;\nStress (7 items): chronic tension, irritability, and difficulty relaxing.\nParticipants were classified into high or low psychological distress groups based on established DASS-21 thresholds. The high distress group included individuals who either\nHad a total DASS-21 score > 60;\nScored in the severe/extremely severe range on at least one subscale—Depression > 21, Anxiety > 15, or Stress > 26 [\nAll others were categorized as low distress.\nAlthough the DASS-21 is primarily a screening measure and not a diagnostic tool, we selected it due to its brevity, strong psychometric properties, and suitability for adolescent school-based research. Its use enables a multidimensional assessment of emotional well-being (across depression, anxiety, and stress) within a limited timeframe, which was crucial given the large sample and school setting. We acknowledge that DASS-21 does not cover all behavioral or functional aspects of stress (e.g., sleep issues, academic performance, or eating problems beyond sugar intake). However, we also assessed selected health-related behaviors such as physical activity and dietary habits, including added sugar consumption, which are commonly linked with emotional distress. These complementary data provide partial behavioral context for interpreting DASS-21 scores. The limitations of relying on a single psychological instrument are further discussed in the Discussion section.\nNo missing data were found in anthropometric, body composition, or physical activity datasets. However, four participants had incomplete responses in the dietary intake questionnaire (QEB). Since principal component analysis (PCA) requires complete data, multiple imputation was applied. The missingness mechanism was identified as missing completely at random (MCAR) [\nResponses from the DASS-21, IPAQ, and QEB questionnaires were transformed using the Yeo–Johnson power transformation to approximate normal distributions [\nInitially, Permutational Multivariate Analysis of Variance (PERMANOVA) was applied to assess whether multivariate lifestyle behavior patterns—integrating physical activity (PA) and dietary intake (DI)—differed between low-distress (LD) and high-distress (HD) groups. This method was chosen for its robustness to violations of multivariate normality and its ability to evaluate group-level structure using Euclidean distances. Next, relationships between PA and DI patterns were explored within and between distress groups using tanglegrams and cophenetic statistics, derived from a cophylogenetic framework. Within each group (LD and HD), tanglegrams were generated to visualize clustering of PA and DI variables based on Euclidean distances and Ward’s linkage method. Procrustes and Procrustean Approach to Cophylogeny (PACo) tests were then performed to formally assess structural similarity between PA and DI configurations in each group. Tanglegrams and cophenetic statistics have also found applications in the social sciences focused on human behavior, including sociology, psychology, and education [\nThe significance level for all statistical tests was set at α = 0.05. All analyses were performed using RStudio (v.2024.11.0) and Statistica 13.0 (StatSoft, Cracow, Poland, 2018).\nTo enable meaningful analysis across the entire study population—including both men and women—the data were normalized and transformed. This step was necessary due to inherent sex-based differences in physical activity levels, dietary intake, and psychological responses, which could otherwise obscure genuine behavioral patterns when analyzed collectively. By applying normalization and appropriate transformation procedures (e.g., z-score standardization or Yeo–Johnson transformation), we ensured that all variables were placed on a comparable scale, reducing the influence of distributional differences related to sex.\nAnalyzing the combined group is theoretically justified, as it allows for the identification of generalizable behavioral structures and interaction patterns that transcend gender. This approach is particularly relevant in public health and educational contexts, where interventions are often designed and implemented at the population level rather than separately by sex. Moreover, integrating data from men and women increases statistical power and improves the robustness of multivariate pattern detection methods, such as clustering or co-structure analysis. Therefore, normalization was a crucial step to uncover shared PA–DI–distress dynamics across the full sample, without being confounded by baseline sex-related disparities.\nBased on the DASS-21 classification criteria, a total of 130 students were categorized as having low psychological distress (males: 67, females: 63), and 79 students were classified as having high psychological distress (males: 24, females: 55). These groupings were used in subsequent analyses comparing psychological distress levels across physical activity and dietary behavior variables. Significant differences were found between LD and HD groups across all DASS-21 components (\nPERMANOVA based on Euclidean distances revealed a significant difference in the overall multivariate structure of physical activity and dietary intake between LD and HD groups (F = 3.91, R\nTanglegram visualizations of physical activity (PA) and dietary intake (DI) revealed distinct behavioral pattern structures between the LD and HD groups (\nIn the LD group, PA variables formed three clusters: (1) moderate activity, domestic and gardening, transport, walking, and working; (2) vigorous and leisure-time activity; and (3) sitting time. The first cluster was linked to fried meals and sweetened beverages. Working, walking, and transport were associated with curd and yellow cheese intake, while vigorous leisure-time activity correlated with higher consumption of sweets and legumes. Increased sitting time was most often associated with fermented milk intake.\nIn the HD group, four PA clusters emerged: (1) transport and walking; (2) moderate activity with working and domestic gardening; (3) vigorous leisure-time activity (as in LD); and (4) sitting time, forming its own cluster. The first cluster was linked to healthier dietary habits, including higher fruit and vegetable intake. In contrast, moderate activity related to work was associated with fried meals and sweetened beverages, while domestic gardening correlated with curd cheese. Vigorous leisure activity was linked to canned meals and yellow cheese. Higher sitting time was associated with increased fast food consumption.\nCross-domain links between PA and DI were more thematically structured in the HD group. Vigorous PA often coincided with fast food intake, and moderate PA with fried meals, suggesting more consolidated—but potentially compensatory—lifestyle patterns among individuals with higher emotional distress.\nThese visual patterns were statistically evaluated using Procrustes analysis, which assessed the alignment between PA and DI behavior profiles within each group. In the LD group, the Procrustes correlation was t\nUnlike Procrustes, PACo analysis evaluates structural congruence based on internal pairwise relationships. It revealed significant PA–DI alignment in both groups. In the LD group, the sum of squares was 126.36 (\nThese findings support the notion that individuals under emotional strain may rely on more rigid or habit-driven combinations of physical activity and dietary choices, whereas those with lower distress levels may maintain more flexible and independently regulated lifestyle patterns.\nSimilarity between analogous behaviors (PA and DI) was assessed in two stages. First, paired dendrograms for LD and HD groups were visually compared side by side, separately for PA and DI, with matching labels directly connected. Second, this visual assessment was supported by cophenetic analysis to quantify structural similarities.\nTanglegrams (\nIn both dendrograms, leisure-time and vigorous activity consistently cluster together, as do moderate activity and domestic gardening, reflecting shared behavioral structures. However, differences emerge in the hierarchical positioning of working and sitting, which fall into separate sub-clades across groups—suggesting variations in activity context or co-occurrence patterns.\nThe cophenetic correlation between the two dendrograms was r = 0.72, indicating moderate structural similarity. This was supported by the Mantel test (r = 0.75,\nDescriptive statistics showed comparable average pairwise distances in PAPs: 13.46 ± 2.46 for LD and 10.47 ± 1.99 for HD, indicating similar overall variability within each group.\nOverall, these findings indicate moderate congruence in physical activity structures between LD and HD groups, with both overlapping and distinct behavioral clustering patterns.\nDespite some matched variable pairs—highlighted by colored lines—the overall dendrogram structures differ between LD and HD groups. For instance, fermented milk and milk cluster together in both groups, but items like fast food, sweets, and energy drinks appear in different sub-clades, reflecting divergence in dietary patterns linked to emotional distress.\nCophenetic correlation between dendrograms was low (r = 0.21), indicating weak structural similarity. Baker’s Gamma (r = 0.11) also showed low, though non-zero, correspondence. The Mantel test found no significant relationship between distance matrices (r = −0.00009,\nDescriptive statistics showed slightly higher mean pairwise distances in the LD group (14.23 ± 1.72) than in the HD group (12.87 ± 1.67), suggesting greater internal variability in dietary behaviors among emotionally healthier individuals.\nOverall, the findings suggest that while some pairwise associations are preserved, the clustering of dietary intake behaviors differs meaningfully between LD and HD groups, likely reflecting distinct co-occurrence patterns shaped by emotional distress.\nBased on dendrogram topology comparisons and clustering differences identified using all.equal() and dend_diff(), a subset of PA and DI behaviors that shifted structurally between LD and HD groups was selected. These variables were entered into a multiple logistic regression model to assess their predictive value for high distress. Odds ratios (ORs) with 95% confidence intervals were calculated to quantify the associated risk for each behavior.\nTo identify candidate variables associated with emotional distress, hierarchical dendrograms were compared using the all.equal() function. For physical activity (PA), the LD and HD dendrograms showed a mean relative difference in branch heights of 0.218, indicating moderate divergence in clustering strength. Topological differences were also noted, with unique edges (edge 7 in LD and edge 6 in HD) reflecting localized reorganization of activity patterns across distress levels.\nFor dietary intake (DI), the mean branch height difference was 0.141, suggesting a smaller overall shift than in PA. However, topological differences were more pronounced, with 12 unique edges in each group’s dendrogram. This indicates substantial re-clustering of dietary behaviors associated with emotional distress.\nBased on structural differences, candidate variables were selected for stepwise multiple logistic regression (MLR). Priority was given to behaviors involved in topological changes—particularly those re-clustered across groups or shifting between major branches. For PA, selected variables included the following: PA_sitting (differently positioned), PA_vigorous, and PA_leisure.time (more closely clustered in HD). For DI, key behaviors were DI_fastfood and DI_sweets (tightly clustered in HD), DI_alcoholic.drinks (shifted from peripheral to central), and DI_yellow.cheese (paired with fast food in HD). These variables were used to estimate the likelihood of HD classification, with odds ratios (ORs) quantifying the strength of each association.\nA multivariate logistic regression (MLR) was conducted to examine associations between lifestyle behaviors and psychological distress, comparing low-distress (LD) and high-distress (HD) groups, with HD modeled as the outcome (HD = 1). The initial model included all candidate variables, and predictor significance was evaluated. Given high model variability and an AIC of 265.35, a stepwise approach was used to remove weaker correlates and identify the most discriminative behaviors. Final results are shown in\nVigorous physical activity (PA_vigorous) was significantly associated with higher odds of high distress; individuals reporting such activity had 1.80 times higher odds of belonging to the HD group (OR = 1.80, 95% CI: 1.33–2.50,\nTaken together, these findings suggest that individuals with higher psychological distress tend to exhibit a complex behavioral profile—combining unhealthy dietary habits (e.g., increased fast food and sweets consumption) with compensatory behaviors like vigorous physical activity. This pattern may reflect an attempt to manage distress through mixed lifestyle strategies.\nIn this work, we were interested in studying the associations between physical activity and dietary patterns in participants with different levels of emotional well-being. This study provides novel insights into the relationship between lifestyle behaviors and psychological distress in university students, revealing that individuals with high distress exhibit both distinct and more structurally consolidated patterns of physical activity and dietary intake. Multivariate and dendrogram-based analyses confirmed significant, though modest, differences in overall behavioral structure between high- and low-distress groups. Notably, emotionally distressed individuals tended to engage in compensatory lifestyle strategies, combining unhealthy dietary habits (e.g., increased fast food and sweets intake) with vigorous physical activity. These patterns—supported by both structural clustering and logistic regression results—suggest a more rigid or thematically organized lifestyle configuration under distress, possibly reflecting efforts to self-regulate through health-compromising and health-promoting behaviors in tandem.\nNumerous studies have shown that frequent and strenuous physical activity positively affects a person health and well-being in general [\nSystematic review and meta-analysis of 59 studies on undergraduate students found that physical activity interventions significantly reduce symptoms of anxiety (standardized mean difference [SMD] = −0.88), depression (SMD = −0.73), and stress (SMD = −0.61). Despite considerable heterogeneity and some methodological limitations, the evidence supports physical activity as an effective means to enhance mental health in this population. However, more rigorously designed interventions are needed for stronger conclusions [\nA second meta-analysis published in 2024 showed that moderate and vigorous physical activity to replace or intermittent sedentary behavior (SB), which could effectively prevent or improve the harm of SB to physical and mental health [\nThe regulatory benefits of PA in mood may be related to the hippocampus and neurotransmitters because the hippocampus is the central brain region that regulates anxiety, and PA could effectively promote the growth of the hippocampus nerve [\nThe results of studies conducted in recent years suggest that diet plays a key role in mental health [\nAdherence to healthy or Mediterranean dietary patterns—high intake of fruits, vegetables, nuts and legumes; moderate consumption of poultry, eggs and dairy products; and only occasional consumption of red meat—has been shown to be associated with a reduced risk of depression. A systematic review and meta-analysis of observational studies by Lassale et al., 2019 [\nThe relationship between diet and mental status is undoubtedly complex and multidirectional. An increased intake of fruit, vegetables, and nuts, as well as a re-education of the intake of pro-inflammatory foods such as processed meat and trans fats, and alcohol, can reduce inflammation in the body. The consumption of foods with antioxidant and anti-inflammatory properties may provide important protection and reduce neuronal damage caused by oxidative stress [\nUnhealthy foods, sugary desserts and drinks, and low intake of fruit, vegetables, and whole-grain products are associated with elevated levels of inflammatory markers in the blood [\nA study of Iranian students found that a “Western diet” dietary pattern was not associated with mental health problems, strong inverse association was observed between the “plant-based” dietary pattern and depression [\nParticipation in physical activity is also an important factor for good health behavior. The group of students studied showed high and medium levels of physical activity resulting, among other things, from participation in sports activities implemented as part of the study program. Physical activity, as emphasized by other authors, is an important mediator that influences both nutritional choices and improves mental health [\nAlthough females were overrepresented in the high-distress group, subgroup analyses stratified by both sex and stress level were not feasible due to small sample sizes, especially among high-distress males. We addressed this limitation by standardizing continuous variables and analyzing the full cohort as a whole. This approach allowed us to examine general behavioral and emotional patterns across adolescents while minimizing the influence of unequal subgroup sizes. Nevertheless, we acknowledge that the gender imbalance may still act as a potential confounder and recommend future studies with larger and more balanced samples.\nStrengths and limitations. This study has few strengths. This case–control study is uniquely involving physically actively males and females examined to the relation between prominent eating patterns and the severity of emotional well-being through depression, anxiety, and stress levels with a mediation role of body composition and odds of the low emotional well-being. Nevertheless, a few limitations also need to be considered. One of the limitations of the present study was the cross-sectional design, which prohibits causal inference. The results of our study cannot be generalized to the population. The study group consisted of students at the University of Public Health, who have a higher physical activity level than the Polish population of their age, in addition to having a greater number of classes in the study program on the subject of good nutritional habits and physical activity. In addition, the unequal gender distribution across distress groups—particularly the small number of males in the high-distress category—prevented statistically reliable subgroup analyses stratified by both sex and psychological distress level. Consequently, we analyzed the full sample as a unified cohort. To minimize potential confounding effects related to gender and individual variability, all continuous variables were standardized prior to analysis. This normalization-based approach, commonly used in anthropometric and developmental physiology research, allowed us to focus on relative behavioral structures across the adolescent population. Another limitation was the small number of participants covering only the minimum requirements of exploratory methods. Moreover, dropout observed during recruitment lowered the sample size in comparison to the needed. Also slightly bias toward a greater number of females was a certain limitation. Additionally, first-year university students undergo numerous lifestyle transitions, including changes in diet, physical activity, and stress levels. Greater autonomy in food choices, exposure to new physical activity options, peer influence, and adaptation to a new academic environment may have affected participants’ behaviors and psychological responses. Since the data collection took place a few weeks after the beginning of the academic year, future studies should consider longitudinal approaches to monitor how these behaviors evolve over time. Such designs could help identify whether initial changes persist, stabilize, or intensify during the academic year.\nIn the future, more precise research tools for both eating habits and levels of physical activity and psychological well-being should also be used for this group of subjects. Comparative studies should also have been carried out in overweight and obese people with low levels of physical activity and less knowledge of proper eating habits. Additional factors worth addressing in future research include body image, dieting behaviors, participation in sport, and access to healthier food and exercise opportunities, all of which may influence student well-being during their transition to university life.\nThis study demonstrates that emotional distress in university students is linked to distinct lifestyle behavior profiles, characterized by both unhealthy eating patterns and elevated levels of vigorous physical activity. Multivariate analyses revealed meaningful differences in the structure and co-occurrence of physical activity and dietary behaviors between low- and high-distress groups. Individuals with high distress showed more tightly coupled, potentially compensatory behavior patterns, suggesting that emotional strain may drive more habitual or reactive lifestyle strategies.\nThese findings highlight the need for targeted interventions that address both domains of behavior simultaneously. Programs aimed at student well-being should not only promote healthy eating and physical activity separately but should also recognize the complexity of how these behaviors cluster in response to psychological distress. Screening tools that identify individuals with mixed behavior profiles—such as high exercise levels combined with high processed food intake—may help flag those at an elevated risk of emotional strain.\nWe recommend integrating mental health screening into campus health promotion efforts, especially for physically active students. Additionally, interventions should account for compensatory behavior patterns, recognizing that health-promoting and risk behaviors may co-occur. Finally, using cluster-based or pattern-recognition approaches—rather than focusing on isolated habits—can help better tailor behavioral interventions.", "topic": "Brain"}
{"pmid": "39196261", "pmcid": "12309348", "title": "Dementia literacy and awareness of risk reduction: A systematic review of South Asian and Southeast Asian regions", "publication_year": "N/A", "abstract": "", "full_text": "\nOur review urges policymakers to implement culturally sensitive education programs, reduce stigma, raise awareness of dementia risk factors, develop and validate appropriate screening tools, and strengthen health‐care responses to achieve immediate improvements and reduce the long‐term burden of dementia‐related health issues in South and Southeast Asia.\nDementia impacts 55 million people globally, mostly in low‐ to middle‐income countries (LMICs).\nA key contributor to the rising dementia rates in LMICs is the prevalence of modifiable risk factors, including hearing impairment, depression, physical inactivity, reduced social interaction, diabetes, smoking in later life, hypertension, obesity and lower educational attainment.\nDespite the WHO's global framework, the effectiveness of education and awareness campaigns in these regions has yet to be thoroughly assessed. Existing government initiatives have only recently emerged in a few countries, and their impact on dementia literacy remains underexplored.\nThus, this systematic review aimed to (i) synthesise current knowledge of dementia literacy, including understanding of risk factors and strategies for reduction in South and Southeast Asia; (ii) determine how population type (e.g. student, health‐care worker, older person, family caregiver) differs in their knowledge and attitudes towards dementia and risk management; and (iii) explore attitudes towards dementia risk reduction, including perceived barriers and facilitators. Findings of this review will inform future interventions and policies, contributing to improved dementia literacy and more effective public health strategies in these high‐burden regions.\nThe protocol for this systematic review was preregistered with the international prospective register of systematic reviews (PROSPERO: CRD42022369886).\nStudies were identified through Medline via Ovid, PubMed, PsycINFO, EMBASE, Cochrane (from inception‐2024) and Google Scholar using specific search terms (see Appendix\nResults were exported to Endnote, duplicates removed, and titles and abstracts imported to Rayyan.\nStudy populations with average age over 18 years from South Asian nations (India, Bangladesh, Bhutan, Pakistan, Nepal, Sri Lanka, Afghanistan and the Maldives) and Southeast Asian nations (Brunei, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Thailand and Vietnam).\nStudies of communities from the above‐named nations, including immigrant diasporas defined as people who were born and raised in one of the countries of interest and lived in a different country at the time of the study.\nQualitative (e.g. interviews and focus groups) and quantitative assessment (e.g. responses to standardised surveys or tools) of dementia knowledge, attitudes, perceptions and misconceptions towards dementia and/or barriers and facilitators to dementia literacy.\nStudies failing to report age data were included if the population type and context of participant recruitment justified the assumption that the average age of the sample is over 18 years. For example, caregivers and undergraduate students are unlikely to include participants under 18 years. Non‐peer‐reviewed and non‐English language studies were excluded. A reference snowballing process was conducted on previous review papers to identify any missed and relevant studies.\nStudy characteristics were extracted into a tabulated form, including study type, measures used and characteristics of study samples (e.g. country, sample size and mean age). Both quantitative and qualitative findings were synthesised to provide a comprehensive understanding of the topic. For the quantitative data, we reported the proportion of studies that discussed specific outcome measures to compare results between nations, populations (immigrants vs. non‐immigrants) and demographic groups. Scores from dementia literacy and awareness surveys were categorised into general awareness and symptoms, causes and risk factors. Study sample responses to standardised quantitative tools were categorised into low, medium and high literacy to support understanding, and were based on evidence‐based metrics for the Alzheimer's Disease Knowledge Scale (AKDS).\nAppendix\nQualitative findings were extracted into a standardised form, focusing on key themes, such as symptoms, causes, risk factors and barriers/enablers to literacy. These themes were synthesised across studies to capture commonalities and differences. The quantitative and qualitative data were narratively compared and reported using narrative synthesis.\nThe Mixed Methods Appraisal Tool (MMAT) was used to critically appraise studies and was conducted independently by two reviewers, with a third reviewer randomly cross‐checking data extraction in 10% of studies.\nElectronic searching of databases resulted in 11,547 records with 73 articles determined suitable for inclusion (Figure\nPRISMA flow diagram.\nCharacteristics of included quantitative and mixed‐methods studies (\nGeneral public (58%)\nHealth‐care workers (42%)\nIndian (40:60)\nVietnamese (58:43)\nFilipino (70:30)\nPhilippines (43:58)\nCambodia (71:29)\nDKAGP\nGPACS‐D\n\nMixed methods study.\nCharacteristics of included qualitative studies (\n\nMixed methods study.\nOver a third of the quantitative and mixed‐methods studies (\nQuantitative findings indicated that most studies (29/37; 78%) reported low levels of overall dementia literacy, with only a fifth (8/37; 22%) indicating medium literacy (Table\nLiteracy levels reported by quantitative studies.\nIndia\nPakistan\nVietnam\nIndia\nPhilippines\nPhilippines\nCambodia\nVietnam\nIndia\nPhilippines\nIndia\nPakistan\nBangladesh\n\nGeneral population had a negative attitude while health‐care workers had a positive attitude.\nResults were inconsistent between different scales, with DKAS results demonstrating low literacy and DAS demonstrating medium literacy.\nScores for individual categories not reported.\nQualitative data identified common misconceptions (Table\nThemes identified by qualitative analysis.\nPakistan\nIndia\nPakistan\nIndia\nPakistan\nIndia\nBangladesh\nVietnam\nIndia\nIndia\nSri Lanka\nPakistan\nNepal\nPakistan\nIndia\nBangladesh\nPakistan\nIndia\nSri Lanka\n\nStudies were coded positive for stigma if there was specific mention of stigma or stigmatising language such as ‘crazy’ or ‘mad’ were used. Studies were coded positive for shame if there was specific mention of shame towards dementia patients (e.g. being ashamed of having a relative with dementia).\nA theme of stigma was evident in more than half (24/41; 59%) of the qualitative studies, with participants using stigmatising language, such as ‘crazy’, ‘dirty’ and ‘mad’ to consistently describe individuals with dementia.\nOne study conducted on a sample of Indian people living in the UK further highlighted beliefs in a ‘possessed person’ (dementia patient) spreading symptoms through a ‘curse’ (dementia) within the family, impacting future generations and marriage prospects.\nAmong Indian, Pakistani and Vietnamese participants, the belief that dementia is a normal part of ageing was highest among Vietnamese (6/9; 67%), followed by Indian (10/18; 56%) and Pakistani (7/13; 54%) participants. There were a few that disagreed with this notion (e.g. two Pakistani studies\nImmigrants and non‐immigrant populations agreed with the statement ‘dementia is a normal part of ageing’ to a similar extent (15/26; 58%, 8/14; 57%, respectively). Both population types had similar reports for low knowledge of dementia symptoms (13/26; 50% in immigrant, 7/14; 50% in non‐immigrant). Supernatural causes of dementia were reported more frequently in studies with immigrant populations (10/26; 39%) than non‐immigrant populations (4/14; 29%). However, stigma was more prevalent in non‐immigrant populations (13/26; 50% in immigrant, 11/14; 79% in non‐immigrant). A slightly higher proportion of non‐immigrant studies compared to immigrant studies reported shame (4/26; 1% in immigrant, 3/14; 21% in non‐immigrant).\nImmigrant populations were more likely to report dementia education barriers, such as lack of education programs, poor education from doctors at time of diagnosis and poor availability of culturally appropriate education materials and services.\nThe general population, health‐care workers and students showed similarly low levels of dementia literacy, with 81% (13/16) of the general population, 71% (5/7) of health‐care workers and 70% (7/10) of students demonstrating low knowledge. Qualitative data indicated that informal/family caregivers and the general population agreed with the statement ‘dementia is a normal part of ageing’ to a similar extent (15/25; 60% in family caregivers, 7/11; 64% in general population), whereas older persons and people with dementia appeared to agree with it to a lesser extent (5/9; 56%). Family carers reported higher stigma rates compared to the general population (15/25; 60% in family caregivers, 6/11; 55% in general population). Similarly, shame was identified in 20% (5/25) studies of family caregivers and was not reported in any studies which included the general population.\nMost studies met basic criteria related to quality (Appendix\nThis review is the first to consolidate existing qualitative and quantitative knowledge on dementia literacy and attitudes in South and Southeast Asia, and investigate factors hindering and supporting dementia literacy and risk reduction. Our results identified widespread misconceptions about dementia across multiple domains, including symptoms, causes and risk factors. Barriers to improving dementia literacy included the lack of culturally and linguistically appropriate resources, inadequate education from health‐care providers and pervasive stigma. These findings have important implications for shaping targeted interventions and policy strategies to improve dementia literacy and reduce associated risks.\nMisconceptions that dementia is a natural consequence of ageing align with findings from previous reviews internationally.\nCultural norms, including religious philosophies, family responsibilities and language, appear to shape how dementia is understood at a community level in the region.\nOur review highlighted low dementia literacy among employee and student cohorts within the health‐care sector. This poses a challenge to the workforces' readiness to care and support people with dementia. Lee et al.\nDespite low knowledge of causes and risk factors, life stressors (more commonly referred to as ‘tension’) were readily identified as a cause rather than a risk factor by participants in the included studies.\nThe prevalence of stigma and misconceptions evident in this review may act as a barrier for education about dementia. Many studies reported family members and informal caregivers hiding their loved one's dementia diagnosis and socially isolating them due to shame.\nThe lack of linguistically and culturally appropriate resources and education programs about dementia was clearly articulated in the studies. Unsurprisingly, this barrier was more frequently reported in immigrant populations.\nThe quantitative scales used in the included studies, such as the ADKS, DKAS and DAS, demonstrated varying sensitivities. While these scales have been validated for assessing dementia literacy and attitudes, some studies using multiple scales showed discrepancies.\nThe most notable merit of the current review is the extensive and systematic nature of the database search. The inclusion of qualitative studies allowed for a contextual understanding and synthesis of the current knowledge of dementia, which was lacking in previous reviews.\nHowever, the systematic approach undertaken ensured that the results were robust. Secondly, we excluded non‐English studies, potentially leading to the omission of insights from non‐English‐speaking populations, although several studies consisted of non‐English‐speaking participants with the assistance of an interpreter or translation of verbatim transcripts and the use of translated standardised scales.\nWe need education and awareness programs targeting the specific misconceptions evident in this review. Cheng et al.\nThe literature also describes community‐based dementia education, which is planned and conducted in collaboration between community representatives, government representatives, health‐care professionals, dementia service providers and volunteer agencies.\nGiven our findings of misconceptions rooted in religion, it is important to consider that religion may pose an additional challenge to future educational interventions aiming to enhance dementia literacy.\nHost countries have an increasingly important role in facilitating dementia awareness within their immigrant communities.\nFuture research should aim to expand the geographical scope of current evidence by including studies from underrepresented countries in South and Southeast Asia, such as Bhutan, Afghanistan and Thailand. Such inclusion would provide a more comprehensive understanding of regional variations in dementia literacy. There is also a clear need for longitudinal studies to evaluate the long‐term effectiveness of dementia literacy interventions, particularly those identified as promising in this review.\nTo strengthen the methodological rigour of future reviews, studies employing consistent sampling strategies and achieving high response rates should be prioritised. Additionally, the current review identified a paucity of high‐quality mixed‐methods studies. Given the capacity of mixed‐methods research to integrate quantitative measures with qualitative insights, future work should more fully embrace this approach to capture both the extent and the underlying drivers of dementia literacy across diverse populations.\nWe also acknowledge the absence of statistical analyses comparing regional differences or demographic subgroups. This was primarily due to the heterogeneity in study design, outcome measures and reporting practices, which limited the feasibility of such analyses. Nonetheless, these limitations reflect the importance of future research adopting standardised methodologies and reporting frameworks that facilitate comparative analysis, including subgroup analyses by age, gender, occupation and health‐care access.\nFinally, existing dementia literacy assessment tools are largely developed in Western contexts and may lack cultural and contextual relevance for South and Southeast Asian populations. Future efforts should focus on the development and validation of culturally adapted instruments to ensure more accurate and meaningful assessment in these settings.\nThis review provides the first comprehensive synthesis of qualitative and quantitative data regarding dementia literacy levels in South Asia and Southeast Asia and highlights significant gaps in knowledge, barriers to education and prevalent misconceptions. These findings offer a foundation for the development of targeted interventions and policy frameworks to enhance dementia literacy, reduce stigma and improve dementia care. The challenges identified in this review, particularly those related to cultural and linguistic factors, must be carefully considered in the design of future education and awareness campaigns.\nThe authors declare that Joyce Siette is an Associate Editor for the Australasian Journal of Ageing. All authors declare no other relationships, including financial or professional, that may pose a competing interest.\nAppendices S1–S5", "content_for_embedding": "\nOur review urges policymakers to implement culturally sensitive education programs, reduce stigma, raise awareness of dementia risk factors, develop and validate appropriate screening tools, and strengthen health‐care responses to achieve immediate improvements and reduce the long‐term burden of dementia‐related health issues in South and Southeast Asia.\nDementia impacts 55 million people globally, mostly in low‐ to middle‐income countries (LMICs).\nA key contributor to the rising dementia rates in LMICs is the prevalence of modifiable risk factors, including hearing impairment, depression, physical inactivity, reduced social interaction, diabetes, smoking in later life, hypertension, obesity and lower educational attainment.\nDespite the WHO's global framework, the effectiveness of education and awareness campaigns in these regions has yet to be thoroughly assessed. Existing government initiatives have only recently emerged in a few countries, and their impact on dementia literacy remains underexplored.\nThus, this systematic review aimed to (i) synthesise current knowledge of dementia literacy, including understanding of risk factors and strategies for reduction in South and Southeast Asia; (ii) determine how population type (e.g. student, health‐care worker, older person, family caregiver) differs in their knowledge and attitudes towards dementia and risk management; and (iii) explore attitudes towards dementia risk reduction, including perceived barriers and facilitators. Findings of this review will inform future interventions and policies, contributing to improved dementia literacy and more effective public health strategies in these high‐burden regions.\nThe protocol for this systematic review was preregistered with the international prospective register of systematic reviews (PROSPERO: CRD42022369886).\nStudies were identified through Medline via Ovid, PubMed, PsycINFO, EMBASE, Cochrane (from inception‐2024) and Google Scholar using specific search terms (see Appendix\nResults were exported to Endnote, duplicates removed, and titles and abstracts imported to Rayyan.\nStudy populations with average age over 18 years from South Asian nations (India, Bangladesh, Bhutan, Pakistan, Nepal, Sri Lanka, Afghanistan and the Maldives) and Southeast Asian nations (Brunei, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Thailand and Vietnam).\nStudies of communities from the above‐named nations, including immigrant diasporas defined as people who were born and raised in one of the countries of interest and lived in a different country at the time of the study.\nQualitative (e.g. interviews and focus groups) and quantitative assessment (e.g. responses to standardised surveys or tools) of dementia knowledge, attitudes, perceptions and misconceptions towards dementia and/or barriers and facilitators to dementia literacy.\nStudies failing to report age data were included if the population type and context of participant recruitment justified the assumption that the average age of the sample is over 18 years. For example, caregivers and undergraduate students are unlikely to include participants under 18 years. Non‐peer‐reviewed and non‐English language studies were excluded. A reference snowballing process was conducted on previous review papers to identify any missed and relevant studies.\nStudy characteristics were extracted into a tabulated form, including study type, measures used and characteristics of study samples (e.g. country, sample size and mean age). Both quantitative and qualitative findings were synthesised to provide a comprehensive understanding of the topic. For the quantitative data, we reported the proportion of studies that discussed specific outcome measures to compare results between nations, populations (immigrants vs. non‐immigrants) and demographic groups. Scores from dementia literacy and awareness surveys were categorised into general awareness and symptoms, causes and risk factors. Study sample responses to standardised quantitative tools were categorised into low, medium and high literacy to support understanding, and were based on evidence‐based metrics for the Alzheimer's Disease Knowledge Scale (AKDS).\nAppendix\nQualitative findings were extracted into a standardised form, focusing on key themes, such as symptoms, causes, risk factors and barriers/enablers to literacy. These themes were synthesised across studies to capture commonalities and differences. The quantitative and qualitative data were narratively compared and reported using narrative synthesis.\nThe Mixed Methods Appraisal Tool (MMAT) was used to critically appraise studies and was conducted independently by two reviewers, with a third reviewer randomly cross‐checking data extraction in 10% of studies.\nElectronic searching of databases resulted in 11,547 records with 73 articles determined suitable for inclusion (Figure\nPRISMA flow diagram.\nCharacteristics of included quantitative and mixed‐methods studies (\nGeneral public (58%)\nHealth‐care workers (42%)\nIndian (40:60)\nVietnamese (58:43)\nFilipino (70:30)\nPhilippines (43:58)\nCambodia (71:29)\nDKAGP\nGPACS‐D\n\nMixed methods study.\nCharacteristics of included qualitative studies (\n\nMixed methods study.\nOver a third of the quantitative and mixed‐methods studies (\nQuantitative findings indicated that most studies (29/37; 78%) reported low levels of overall dementia literacy, with only a fifth (8/37; 22%) indicating medium literacy (Table\nLiteracy levels reported by quantitative studies.\nIndia\nPakistan\nVietnam\nIndia\nPhilippines\nPhilippines\nCambodia\nVietnam\nIndia\nPhilippines\nIndia\nPakistan\nBangladesh\n\nGeneral population had a negative attitude while health‐care workers had a positive attitude.\nResults were inconsistent between different scales, with DKAS results demonstrating low literacy and DAS demonstrating medium literacy.\nScores for individual categories not reported.\nQualitative data identified common misconceptions (Table\nThemes identified by qualitative analysis.\nPakistan\nIndia\nPakistan\nIndia\nPakistan\nIndia\nBangladesh\nVietnam\nIndia\nIndia\nSri Lanka\nPakistan\nNepal\nPakistan\nIndia\nBangladesh\nPakistan\nIndia\nSri Lanka\n\nStudies were coded positive for stigma if there was specific mention of stigma or stigmatising language such as ‘crazy’ or ‘mad’ were used. Studies were coded positive for shame if there was specific mention of shame towards dementia patients (e.g. being ashamed of having a relative with dementia).\nA theme of stigma was evident in more than half (24/41; 59%) of the qualitative studies, with participants using stigmatising language, such as ‘crazy’, ‘dirty’ and ‘mad’ to consistently describe individuals with dementia.\nOne study conducted on a sample of Indian people living in the UK further highlighted beliefs in a ‘possessed person’ (dementia patient) spreading symptoms through a ‘curse’ (dementia) within the family, impacting future generations and marriage prospects.\nAmong Indian, Pakistani and Vietnamese participants, the belief that dementia is a normal part of ageing was highest among Vietnamese (6/9; 67%), followed by Indian (10/18; 56%) and Pakistani (7/13; 54%) participants. There were a few that disagreed with this notion (e.g. two Pakistani studies\nImmigrants and non‐immigrant populations agreed with the statement ‘dementia is a normal part of ageing’ to a similar extent (15/26; 58%, 8/14; 57%, respectively). Both population types had similar reports for low knowledge of dementia symptoms (13/26; 50% in immigrant, 7/14; 50% in non‐immigrant). Supernatural causes of dementia were reported more frequently in studies with immigrant populations (10/26; 39%) than non‐immigrant populations (4/14; 29%). However, stigma was more prevalent in non‐immigrant populations (13/26; 50% in immigrant, 11/14; 79% in non‐immigrant). A slightly higher proportion of non‐immigrant studies compared to immigrant studies reported shame (4/26; 1% in immigrant, 3/14; 21% in non‐immigrant).\nImmigrant populations were more likely to report dementia education barriers, such as lack of education programs, poor education from doctors at time of diagnosis and poor availability of culturally appropriate education materials and services.\nThe general population, health‐care workers and students showed similarly low levels of dementia literacy, with 81% (13/16) of the general population, 71% (5/7) of health‐care workers and 70% (7/10) of students demonstrating low knowledge. Qualitative data indicated that informal/family caregivers and the general population agreed with the statement ‘dementia is a normal part of ageing’ to a similar extent (15/25; 60% in family caregivers, 7/11; 64% in general population), whereas older persons and people with dementia appeared to agree with it to a lesser extent (5/9; 56%). Family carers reported higher stigma rates compared to the general population (15/25; 60% in family caregivers, 6/11; 55% in general population). Similarly, shame was identified in 20% (5/25) studies of family caregivers and was not reported in any studies which included the general population.\nMost studies met basic criteria related to quality (Appendix\nThis review is the first to consolidate existing qualitative and quantitative knowledge on dementia literacy and attitudes in South and Southeast Asia, and investigate factors hindering and supporting dementia literacy and risk reduction. Our results identified widespread misconceptions about dementia across multiple domains, including symptoms, causes and risk factors. Barriers to improving dementia literacy included the lack of culturally and linguistically appropriate resources, inadequate education from health‐care providers and pervasive stigma. These findings have important implications for shaping targeted interventions and policy strategies to improve dementia literacy and reduce associated risks.\nMisconceptions that dementia is a natural consequence of ageing align with findings from previous reviews internationally.\nCultural norms, including religious philosophies, family responsibilities and language, appear to shape how dementia is understood at a community level in the region.\nOur review highlighted low dementia literacy among employee and student cohorts within the health‐care sector. This poses a challenge to the workforces' readiness to care and support people with dementia. Lee et al.\nDespite low knowledge of causes and risk factors, life stressors (more commonly referred to as ‘tension’) were readily identified as a cause rather than a risk factor by participants in the included studies.\nThe prevalence of stigma and misconceptions evident in this review may act as a barrier for education about dementia. Many studies reported family members and informal caregivers hiding their loved one's dementia diagnosis and socially isolating them due to shame.\nThe lack of linguistically and culturally appropriate resources and education programs about dementia was clearly articulated in the studies. Unsurprisingly, this barrier was more frequently reported in immigrant populations.\nThe quantitative scales used in the included studies, such as the ADKS, DKAS and DAS, demonstrated varying sensitivities. While these scales have been validated for assessing dementia literacy and attitudes, some studies using multiple scales showed discrepancies.\nThe most notable merit of the current review is the extensive and systematic nature of the database search. The inclusion of qualitative studies allowed for a contextual understanding and synthesis of the current knowledge of dementia, which was lacking in previous reviews.\nHowever, the systematic approach undertaken ensured that the results were robust. Secondly, we excluded non‐English studies, potentially leading to the omission of insights from non‐English‐speaking populations, although several studies consisted of non‐English‐speaking participants with the assistance of an interpreter or translation of verbatim transcripts and the use of translated standardised scales.\nWe need education and awareness programs targeting the specific misconceptions evident in this review. Cheng et al.\nThe literature also describes community‐based dementia education, which is planned and conducted in collaboration between community representatives, government representatives, health‐care professionals, dementia service providers and volunteer agencies.\nGiven our findings of misconceptions rooted in religion, it is important to consider that religion may pose an additional challenge to future educational interventions aiming to enhance dementia literacy.\nHost countries have an increasingly important role in facilitating dementia awareness within their immigrant communities.\nFuture research should aim to expand the geographical scope of current evidence by including studies from underrepresented countries in South and Southeast Asia, such as Bhutan, Afghanistan and Thailand. Such inclusion would provide a more comprehensive understanding of regional variations in dementia literacy. There is also a clear need for longitudinal studies to evaluate the long‐term effectiveness of dementia literacy interventions, particularly those identified as promising in this review.\nTo strengthen the methodological rigour of future reviews, studies employing consistent sampling strategies and achieving high response rates should be prioritised. Additionally, the current review identified a paucity of high‐quality mixed‐methods studies. Given the capacity of mixed‐methods research to integrate quantitative measures with qualitative insights, future work should more fully embrace this approach to capture both the extent and the underlying drivers of dementia literacy across diverse populations.\nWe also acknowledge the absence of statistical analyses comparing regional differences or demographic subgroups. This was primarily due to the heterogeneity in study design, outcome measures and reporting practices, which limited the feasibility of such analyses. Nonetheless, these limitations reflect the importance of future research adopting standardised methodologies and reporting frameworks that facilitate comparative analysis, including subgroup analyses by age, gender, occupation and health‐care access.\nFinally, existing dementia literacy assessment tools are largely developed in Western contexts and may lack cultural and contextual relevance for South and Southeast Asian populations. Future efforts should focus on the development and validation of culturally adapted instruments to ensure more accurate and meaningful assessment in these settings.\nThis review provides the first comprehensive synthesis of qualitative and quantitative data regarding dementia literacy levels in South Asia and Southeast Asia and highlights significant gaps in knowledge, barriers to education and prevalent misconceptions. These findings offer a foundation for the development of targeted interventions and policy frameworks to enhance dementia literacy, reduce stigma and improve dementia care. The challenges identified in this review, particularly those related to cultural and linguistic factors, must be carefully considered in the design of future education and awareness campaigns.\nThe authors declare that Joyce Siette is an Associate Editor for the Australasian Journal of Ageing. All authors declare no other relationships, including financial or professional, that may pose a competing interest.\nAppendices S1–S5", "topic": "Brain"}
{"pmid": "39141670", "pmcid": "12307965", "title": "Practitioner Perspectives on the Association Between Mental Fatigue and Injury Risk in High‐Performance Sport: A Mixed Methods Study", "publication_year": "N/A", "abstract": "This study aimed to investigate the perceptions of practitioners involved in injury management within high‐performance sport on the potential interaction between mental fatigue and injury risk, including perceived mechanisms. A sequential explanatory design was used, with phase one implementing a cross‐sectional survey and phase two utilising semi‐structured interviews. An electronic survey of multi‐disciplinary practitioners working in high‐performance sport, specifically invasion‐based team sports, was conducted. Topics included the mechanisms by which mental fatigue may influence risk of injury, potential sex differences, mental fatigue and injury prevention and areas for future research. Preliminary data analysis guided the development of the phase two interview schedule, which aimed to gain a deeper understanding of the association and perceived mechanisms. Forty‐five participants completed the phase one survey, and eight participants completed the phase two semi‐structured interviews. The primary findings of this study suggest that practitioners working in high‐performance sport perceive a link between mental fatigue and risk of injury, primarily acute noncontact injuries. Proposed mechanisms include impaired motor control, poor biomechanics and reduced cognitive function. However, isolating mental fatigue as a direct factor is difficult, due to challenges distinguishing between mental and physical fatigue. The findings of this study indicate practitioners perceive an association between mental fatigue and risk of injury. Future research focused on the mechanisms linking mental fatigue to injury risk is required to empirically examine and determine the validity of this perception. However, athlete management strategies regarding mental fatigue may be incorporated into practice to potentially limit the risk of athlete injury.\n", "full_text": "Fatigue is a complex and multifaceted phenomenon (Halson\nReported impacts of mental fatigue on human performance include impairments in response time, decision‐making and sport‐specific motor performance, evidencing a significant influence on movement patterns (Habay et al.\nThe perceptual‐cognitive demands of invasion‐based sports have been highlighted as extremely challenging (Badin et al.\nExploratory research, by means of qualitative methodologies, can offer valuable insights into emerging topics related to sports performance practices, which can inform practitioners (Dietrich and Ehrlenspiel\nThis study aimed to investigate the knowledge and perceptions of practitioners working in high‐performance sport on potential interactions between mental fatigue and injury risk, types of injury and their mechanisms and directions for future research. Given the absence of research to date, this project utilised a sequential explanatory mixed‐methodology approach to provide novel insights. The findings will aid in the design of future research focused on identifying the underlying mechanisms linking mental fatigue to risk of injury.\nA mixed‐methods approach with a sequential explanatory design was used. Phase one implemented a cross‐sectional survey design to obtain a broad understanding on knowledge and perceptions on the perceived mechanisms of mental fatigue in relation to injury. Phase two utilised semi‐structured interviews to gain a deeper understanding on the perceived influence and mechanisms of mental fatigue on injury risk. Ethical approval was provided by the university's Human Research Ethics Committee (approval number: 2022‐2742E).\nSports practitioners involved in injury management working with professional invasion‐based team‐sport athletes were invited to participate in this study. Participants were purposefully recruited through social media platforms (X/Twitter and LinkedIn) to facilitate international outreach, alongside direct email communication to national sporting bodies and word‐of‐mouth within professional networks. Eligible participants were required to be practitioners working at a professional or high‐performance level in invasion‐based team sports, with experience in injury management. Relevant eligible practitioner roles included high‐performance managers, physiotherapists, strength and conditioning/physical performance coaches, medical personnel and coaches. All participants provided their informed consent to participate in this study. The athletes whom participants work with were classified according to the high‐performance framework by McKay et al. (\nThe questionnaire was built with REDCap (Research Electronic Data Capture, USA) and distributed via a generated public survey link. All authors contributed to the development of the research questions and distribution of the public survey link. Following piloting amongst the full research team, a subset of authors (\nDescriptive outputs and quantitative analyses were performed using RStudio (Posit, Boston, Massachusetts, USA) with the R statistical programming language (4.2.2, R Foundation, Vienna, Austria). A one‐way ANOVA (\nOn completion of phase one, participants were invited to express their interest to participate in phase two, which include semi‐structured interviews. Participants who accepted the invitation completed an additional consent form via REDCap, and an interview time was scheduled by the lead author.\nTo establish face validity, topics of discussion were developed by a preliminary analysis of phase one survey results, which were discussed by members of the research team (\nThe semi‐structured interviews were conducted and recorded using Zoom software (Zoom Video Communications Inc., San Jose, California). Immediately prior to the commencement of the interview, a standardised preamble was read verbatim to all participants, which provided clear instructions on the interview process and allowed participants to seek clarification. This ensured that all participants had a consistent comprehension of the interview process to support comparable responses. Following the interview, audio (mp3) was exported from Zoom and transcribed. Unique identifiers were assigned to each participant (e.g., P01, P02) and transcripts sent back to the participants for final approval prior to being exported for data analysis.\nThe research team adopted a positivist philosophy, which guided the approach to analysing the data scientifically (Hassmén et al.,\nIn phase one, 45 participants (\nSummary of key participant demographics in phase one.\n\nFigure\nDistribution of responses on the knowledge of mental fatigue (a) and injury (b), perceptions on the association between mental fatigue and injury (c), whether the mechanisms are modifiable (d), whether the association is different between genders (e) and whether mental fatigue affects athlete availability beyond injury (f).\nSummary of thematic analysis on the modifiability of mechanisms and mitigation strategies, gender differences and athlete availability practice.\n\nWhen asked about the strength of the perceived association between injury risk and mental fatigue on a VAS, with the scale anchored by none at all (0) to maximal (100), respondents indicated an average association of 64.1% (Figure\nVisual analogue scale responses showing the mean ± SD of the perceived strength of the association between mental fatigue and injury (grey) and for specific injury types. An asterisk (*) denotes injury types that are significantly different from one another.\nSummary of thematic analysis on the mechanisms between mental fatigue and injury type.\n\nFigure\nDistribution of respondents on the extent to which evidence from future research on these topics would inform their practice with athletes. ‘Awareness' indicates respondents’ familiarity with existing scientific literature on mental fatigue, mental recovery and injury within athletic populations.\nParticipants in phase two highlighted a perceived association between mental fatigue and an increased risk of injury, identifying various mechanisms through which this relationship may occur. One of the main mechanisms discussed was the link between mental fatigue and noncontact injuries. Five participants (62.5%) suggested that mental fatigue impairs an athlete's ability to execute movements with proper biomechanics, thereby increasing the likelihood of noncontact injuries such as ACL tears or ankle sprains.\nI think certainly non‐contact injuries like cruciate ligaments. That’s the biggest injury we see in rugby, and I think non‐contact injuries can be linked with mentally fatigued athletes.\nAnother frequently mentioned mechanism involved gradual onset injuries. Four participants (50%) discussed that chronic exposure to mental fatigue could lead to gradual onset injuries, such as tendinopathy, due to the cumulative stress placed on the body over time.\nIf mental fatigue stays for a long term and is too high, then the muscles will react to it gradually. So, the tendinopathy fatigue will gradually increase.\nIndirect and direct contact injuries were also discussed, however, less frequently. In the context of indirect contact injuries, four participants (50%) noted that mental fatigue might lower reflexes and cognitive sharpness. Discussion suggests this could make athletes more vulnerable during scenarios that require quick responses, potentially leading to injuries. Regarding direct contact injuries, the discussions were mixed. Three participants (37.5%) suggested that mental fatigue might lead to a lapse in judgement or slower reaction times during physical encounters, potentially leading to injury. However, two participants (25%) were sceptical of a direct link between mental fatigue and contact injuries, emphasising that these injuries are more likely to occur due to physical rather than mental fatigue.\nMental fatigue was linked to observable changes in an athlete's posture and technique, as noted by three participants. They observed that as athletes become mentally fatigued, their physical execution can deteriorate.\nWhen they start to hinge over more when they’re upright or in football terms, their technique is sharp. When they start to get sloppy with their technique or posture, it can be physical but also you see decisions attached to that that are wrong.\nCognitive impairments, such as decreased focus and poor decision‐making, were discussed as signs of mental fatigue by four participants (50%). Mentally fatigued athletes were reported to struggle with tasks requiring high levels of concentration.\nWhen players aren’t moving into position at the right time, it could be showing fatigue. This could also be physical, but the decision‐making elements they get wrong a lot more.\nBehavioural changes, such as disengagement from social interactions and a general flatness in mood, were also identified as indicators of mental fatigue by three participants (37.5%).\nIf you see them either getting by themselves or not really engaging with others. that’s one of the signs.\nHowever, despite these observable indicators, five participants (62.5%) highlighted the difficulty in measuring mental fatigue objectively, with most relying on subjective assessments.\nWe assess mental fatigue through our self‐reported wellness questionnaires, but from observation alone, there’s nothing that really suggests whether an athlete is mentally fatigued or not.\nTwo participants (25%) expressed uncertainty regarding sex‐specific differences in the relationship between mental fatigue and injury risk. Three participants (37.5%) did not have the experience with both male and female athletes to provide comments.\nI want to say no, but then I also don’t know what I’m basing that off of to be honest.\nThree participants (37.5%) perceived that female athletes experience mental fatigue differently than males and that females often focus on maintaining health to perform well. One participant (12.5%) perceived that males prioritise overcoming fatigue to enhance performance. However, these discussions did not directly address the relationship between mental fatigue and injury. Instead, they explored indirect factors like general conditioning, biological differences and cognitive capacity, which may contribute to injury but were not explicitly linked.\nProbably one aspect is conditioning. That's not just physical conditioning, but also cognitive capacity. Do they have the capacity to handle the number of decisions or an overload of decisions in a game?\nMale and female athletes are exposed to different stimuli across a day and are biologically different. They will respond to stresses differently leading to different responses to mental fatigue.\nParticipants discussed the influence of mental fatigue on athlete availability, extending beyond its impact on injuries. Four participants (50%) suggested that mental fatigue can reduce an athlete's ability to participate in training and competition.\nThey come into training tired mentally and physically, affecting participation. We make adjustments or give them the day off.\nConversely, three participants (37.5%) expressed uncertainty about the extent to which mental fatigue alone impacts athlete availability.\nI don’t think mental fatigue on its own has led to time‐loss injuries. there are always other factors at play.\nParticipants shared diverse perspectives on whether and how mental fatigue is deliberately factored into training. Three participants (37.5%) acknowledged that while mental fatigue is not always directly targeted, it is considered within the broader context of training design. This suggests that mental fatigue is managed through strategies aimed at enhancing overall performance.\nFor me, it’s about performance. It’s less about injury risk. If mental fatigue or reducing mental fatigue helps with that, then I’ve done my job.\nWe factor in mental fatigue in the timing of injury prevention strategies. On a day they have had significant cognitive and physical stimuli, you would probably do less work because you will lose them mentally.\nHowever, no participants reported deliberately factoring of mental fatigue into their training regimens. This reflects a more traditional focus on physical conditioning, with mental fatigue often being overlooked or managed incidentally rather than deliberately integrated into training protocols.\nWe only do more physical prevention. But no, we don’t have an aspect for mental fatigue prevention.\nIn contrast, there was a broader recognition of the need to mitigate mental fatigue, particularly to enhance athlete well‐being and performance. Five participants (62.5%) described various strategies employed to reduce mental fatigue, ranging from adjusting training schedules to providing psychological support. Adjustments were made to prevent cognitive overload right before a game, thus aiming to keep athletes mentally fresh. However, these were not done specifically in relation to injury, but rather for performance.\nWe try to do it but it’s really basic. We ask our players if they are mentally fatigued or physically fatigued with just a questionnaire at the beginning of each training camp.\nWe do the tactical analysis on a Thursday when the game is on a Saturday. because your athletes get mentally fatigued.\nAll eight participants (100%) identified two key areas for future research. Five participants (62.5%) emphasised the need for objective measures to assess mental fatigue, with a focus on reliable tools beyond self‐reported questionnaires.\nIt would be good to have an objective measure that’s not just players answering a questionnaire that can indicate mental fatigue.\nThree participants (37.5%) highlighted the importance of understanding the correlation between mental fatigue and injury, emphasising the need for evidence to support the role mental fatigue plays in injury risk.\nIt would be really interesting to look at how much of a role mental fatigue plays in injury. We’re not really thinking about mental fatigue leading to injury.\nSeven participants (87.5%) discussed the barriers to addressing mental fatigue in sports, grouping them into time constraints (\nThe fact that I don’t really know the signs or the background of mental fatigue. That’s holding me back a little bit to implement it.\nThe battle would be getting the players to consent. If you just presented a battery of tests without understanding that it’s real. They might not want to do it.\nAll eight participants (100%) expressed a preference for a variety of research presentation formats. Five participants (62.5%) favoured workshops, conferences and seminars as effective methods. Three participants (\nEveryone who is surrounding the team. Both the field staff and the medical staff.\nThe aim of this study was to explore practitioners' knowledge and perceptions on the potential association between mental fatigue and injury risk in invasion sports. To our knowledge, this is the first study to investigate such associations. The primary findings of this study suggest that practitioners working in invasion sports perceive there to be a link between mental fatigue and risk of injury, primarily noncontact injuries. The mechanisms proposed by participants include impaired motor control, poor biomechanics and reduced cognitive function, all of which might contribute to an increased risk of injury. However, distinguishing mental fatigue as an isolated factor is challenging, as it often presents concurrently with physical fatigue, making it difficult to make conclusions regarding its direct impact on injury. The results of this study provide a foundation for future research focused on the physiological mechanisms linking cognitive demands to injury risk.\nFindings from both phases of this study suggest that participants working in invasion sports perceive elevated mental fatigue as most likely to increase the risk of noncontact injuries. These results align with the review by Smith et al. (\nIn addition to noncontact injuries, participants in phase two expressed a potential relationship with mental fatigue on gradual onset injuries. Interestingly, this was considered less likely to be associated with mental fatigue in phase one, yet still prevalent. In‐depth discussions in phase two and free‐text responses in phase one suggest a perception that chronic exposure to mental fatigue could lead to prolonged stress and biomechanical changes placed on the body over time. Given that physical fatigue and mental fatigue are linked (Marcora et al.\nIndicators of mental fatigue that may result in injury, identified by participants in phase two, included noticeable physical and cognitive changes. The physical observations are consistent with existing research, which suggests that mental fatigue can result in postural changes, such as increased trunk flexion while walking (Qu et al.\nIn the current study, participants provided varied perspectives on whether mental fatigue differs between sex (assigned at birth, male and female) and the potential resulting injury risk. There was no discussion on direct links, and perceived risks of injuries were indirect, such as general conditioning, biological differences and cognitive capacity. Hardaker et al. (\nRegarding gender and mental fatigue, limited research has investigated differences in mental fatigue between male and females in an athletic population. Díaz García et al. (\nParticipants in phase one highlighted the broader impact of mental fatigue on athlete availability, suggesting that mental fatigue influences both preparation and the likelihood of illness. These results are consistent with research by Strand and Samuelson (\nNone of the participants in this study consider mental fatigue in their training design with the specific intention of reducing injury. This reflects a more traditional focus on physical conditioning, where mental fatigue was managed incidentally (Russell et al.,\nOne major limitation of this study is that all survey respondents held the perception that mental fatigue and injury are linked. Additionally, there is potential for a response bias, as practitioners who chose to engage with the survey might have had a prior interest in the study's topics, despite the survey link being publicly accessible. A Hawthorne effect may also be present, with participants possibly responding in a way they thought aligned with the study's purpose. Future research could benefit from maximum variation sampling to provide a broader understanding of this association across different sports and populations. Nevertheless, our findings suggest that participants perceive an association between mental fatigue and injury, which may reflect a consensus within the field. Another limitation of this study is that participants were from a limited number of countries and potentially specific sports within those countries, which may restrict the generalisability of the findings to a broader population.\nParticipants emphasised the need for more research into objective measures of mental fatigue. Subsequently, a clearer understanding of the association between mental fatigue and injury risk is necessary, as highlighted by several participants, to justify its inclusion within training programmes. Future research could benefit from longitudinal study designs that collect mental fatigue data alongside injury data during training and competition to investigate potential associations. Further, future research should explore methods to systematically incorporate mental fatigue in training with the aim to reduce injury risk.\nThe primary findings of this novel study suggest that practitioners involved in injury management within high‐performance invasion sports perceive there to be a link between mental fatigue and injury risk. Participants suggested that noncontact injuries are more likely to occur under periods of elevated mental fatigue. Potential mechanisms proposed by practitioners to link mental fatigue and risk of injury include impaired motor control, poor biomechanics and reduced cognitive function. The challenge in distinguishing mental fatigue as a directly contributing factor to injury is acknowledged, given mental fatigue typically occurs alongside physical fatigue. Future research focused on the mechanisms linking mental fatigue demands to injury risk is required to empirically examine and determine the validity of the perceptions discussed in this study. Nevertheless, practitioners who perceive mental fatigue as a potential risk factor for injury might consider incorporating athlete management strategies, such as mental fatigue screening or implementing targeted training breaks.\n\nPractitioners perceive that mental fatigue may increase the risk of injuries, in particular noncontact injuries, in high‐performance invasion sport athletes.\nPerceived indicators of mental fatigue which may relate to injury risk include deterioration in athlete posture and technique, decreased focus and poor decision‐making and behavioural changes such as disengagement.\nPractitioners could consider implementing strategies to reduce mental fatigue when indicators are observed, potentially mitigating injury risk. Approaches include brief rest breaks, caffeine‐maltodextrin mouth rinses or exposure to pleasant odours.\nThe authors declare no conflicts of interest.\nSupporting Information S1", "content_for_embedding": "Fatigue is a complex and multifaceted phenomenon (Halson\nReported impacts of mental fatigue on human performance include impairments in response time, decision‐making and sport‐specific motor performance, evidencing a significant influence on movement patterns (Habay et al.\nThe perceptual‐cognitive demands of invasion‐based sports have been highlighted as extremely challenging (Badin et al.\nExploratory research, by means of qualitative methodologies, can offer valuable insights into emerging topics related to sports performance practices, which can inform practitioners (Dietrich and Ehrlenspiel\nThis study aimed to investigate the knowledge and perceptions of practitioners working in high‐performance sport on potential interactions between mental fatigue and injury risk, types of injury and their mechanisms and directions for future research. Given the absence of research to date, this project utilised a sequential explanatory mixed‐methodology approach to provide novel insights. The findings will aid in the design of future research focused on identifying the underlying mechanisms linking mental fatigue to risk of injury.\nA mixed‐methods approach with a sequential explanatory design was used. Phase one implemented a cross‐sectional survey design to obtain a broad understanding on knowledge and perceptions on the perceived mechanisms of mental fatigue in relation to injury. Phase two utilised semi‐structured interviews to gain a deeper understanding on the perceived influence and mechanisms of mental fatigue on injury risk. Ethical approval was provided by the university's Human Research Ethics Committee (approval number: 2022‐2742E).\nSports practitioners involved in injury management working with professional invasion‐based team‐sport athletes were invited to participate in this study. Participants were purposefully recruited through social media platforms (X/Twitter and LinkedIn) to facilitate international outreach, alongside direct email communication to national sporting bodies and word‐of‐mouth within professional networks. Eligible participants were required to be practitioners working at a professional or high‐performance level in invasion‐based team sports, with experience in injury management. Relevant eligible practitioner roles included high‐performance managers, physiotherapists, strength and conditioning/physical performance coaches, medical personnel and coaches. All participants provided their informed consent to participate in this study. The athletes whom participants work with were classified according to the high‐performance framework by McKay et al. (\nThe questionnaire was built with REDCap (Research Electronic Data Capture, USA) and distributed via a generated public survey link. All authors contributed to the development of the research questions and distribution of the public survey link. Following piloting amongst the full research team, a subset of authors (\nDescriptive outputs and quantitative analyses were performed using RStudio (Posit, Boston, Massachusetts, USA) with the R statistical programming language (4.2.2, R Foundation, Vienna, Austria). A one‐way ANOVA (\nOn completion of phase one, participants were invited to express their interest to participate in phase two, which include semi‐structured interviews. Participants who accepted the invitation completed an additional consent form via REDCap, and an interview time was scheduled by the lead author.\nTo establish face validity, topics of discussion were developed by a preliminary analysis of phase one survey results, which were discussed by members of the research team (\nThe semi‐structured interviews were conducted and recorded using Zoom software (Zoom Video Communications Inc., San Jose, California). Immediately prior to the commencement of the interview, a standardised preamble was read verbatim to all participants, which provided clear instructions on the interview process and allowed participants to seek clarification. This ensured that all participants had a consistent comprehension of the interview process to support comparable responses. Following the interview, audio (mp3) was exported from Zoom and transcribed. Unique identifiers were assigned to each participant (e.g., P01, P02) and transcripts sent back to the participants for final approval prior to being exported for data analysis.\nThe research team adopted a positivist philosophy, which guided the approach to analysing the data scientifically (Hassmén et al.,\nIn phase one, 45 participants (\nSummary of key participant demographics in phase one.\n\nFigure\nDistribution of responses on the knowledge of mental fatigue (a) and injury (b), perceptions on the association between mental fatigue and injury (c), whether the mechanisms are modifiable (d), whether the association is different between genders (e) and whether mental fatigue affects athlete availability beyond injury (f).\nSummary of thematic analysis on the modifiability of mechanisms and mitigation strategies, gender differences and athlete availability practice.\n\nWhen asked about the strength of the perceived association between injury risk and mental fatigue on a VAS, with the scale anchored by none at all (0) to maximal (100), respondents indicated an average association of 64.1% (Figure\nVisual analogue scale responses showing the mean ± SD of the perceived strength of the association between mental fatigue and injury (grey) and for specific injury types. An asterisk (*) denotes injury types that are significantly different from one another.\nSummary of thematic analysis on the mechanisms between mental fatigue and injury type.\n\nFigure\nDistribution of respondents on the extent to which evidence from future research on these topics would inform their practice with athletes. ‘Awareness' indicates respondents’ familiarity with existing scientific literature on mental fatigue, mental recovery and injury within athletic populations.\nParticipants in phase two highlighted a perceived association between mental fatigue and an increased risk of injury, identifying various mechanisms through which this relationship may occur. One of the main mechanisms discussed was the link between mental fatigue and noncontact injuries. Five participants (62.5%) suggested that mental fatigue impairs an athlete's ability to execute movements with proper biomechanics, thereby increasing the likelihood of noncontact injuries such as ACL tears or ankle sprains.\nI think certainly non‐contact injuries like cruciate ligaments. That’s the biggest injury we see in rugby, and I think non‐contact injuries can be linked with mentally fatigued athletes.\nAnother frequently mentioned mechanism involved gradual onset injuries. Four participants (50%) discussed that chronic exposure to mental fatigue could lead to gradual onset injuries, such as tendinopathy, due to the cumulative stress placed on the body over time.\nIf mental fatigue stays for a long term and is too high, then the muscles will react to it gradually. So, the tendinopathy fatigue will gradually increase.\nIndirect and direct contact injuries were also discussed, however, less frequently. In the context of indirect contact injuries, four participants (50%) noted that mental fatigue might lower reflexes and cognitive sharpness. Discussion suggests this could make athletes more vulnerable during scenarios that require quick responses, potentially leading to injuries. Regarding direct contact injuries, the discussions were mixed. Three participants (37.5%) suggested that mental fatigue might lead to a lapse in judgement or slower reaction times during physical encounters, potentially leading to injury. However, two participants (25%) were sceptical of a direct link between mental fatigue and contact injuries, emphasising that these injuries are more likely to occur due to physical rather than mental fatigue.\nMental fatigue was linked to observable changes in an athlete's posture and technique, as noted by three participants. They observed that as athletes become mentally fatigued, their physical execution can deteriorate.\nWhen they start to hinge over more when they’re upright or in football terms, their technique is sharp. When they start to get sloppy with their technique or posture, it can be physical but also you see decisions attached to that that are wrong.\nCognitive impairments, such as decreased focus and poor decision‐making, were discussed as signs of mental fatigue by four participants (50%). Mentally fatigued athletes were reported to struggle with tasks requiring high levels of concentration.\nWhen players aren’t moving into position at the right time, it could be showing fatigue. This could also be physical, but the decision‐making elements they get wrong a lot more.\nBehavioural changes, such as disengagement from social interactions and a general flatness in mood, were also identified as indicators of mental fatigue by three participants (37.5%).\nIf you see them either getting by themselves or not really engaging with others. that’s one of the signs.\nHowever, despite these observable indicators, five participants (62.5%) highlighted the difficulty in measuring mental fatigue objectively, with most relying on subjective assessments.\nWe assess mental fatigue through our self‐reported wellness questionnaires, but from observation alone, there’s nothing that really suggests whether an athlete is mentally fatigued or not.\nTwo participants (25%) expressed uncertainty regarding sex‐specific differences in the relationship between mental fatigue and injury risk. Three participants (37.5%) did not have the experience with both male and female athletes to provide comments.\nI want to say no, but then I also don’t know what I’m basing that off of to be honest.\nThree participants (37.5%) perceived that female athletes experience mental fatigue differently than males and that females often focus on maintaining health to perform well. One participant (12.5%) perceived that males prioritise overcoming fatigue to enhance performance. However, these discussions did not directly address the relationship between mental fatigue and injury. Instead, they explored indirect factors like general conditioning, biological differences and cognitive capacity, which may contribute to injury but were not explicitly linked.\nProbably one aspect is conditioning. That's not just physical conditioning, but also cognitive capacity. Do they have the capacity to handle the number of decisions or an overload of decisions in a game?\nMale and female athletes are exposed to different stimuli across a day and are biologically different. They will respond to stresses differently leading to different responses to mental fatigue.\nParticipants discussed the influence of mental fatigue on athlete availability, extending beyond its impact on injuries. Four participants (50%) suggested that mental fatigue can reduce an athlete's ability to participate in training and competition.\nThey come into training tired mentally and physically, affecting participation. We make adjustments or give them the day off.\nConversely, three participants (37.5%) expressed uncertainty about the extent to which mental fatigue alone impacts athlete availability.\nI don’t think mental fatigue on its own has led to time‐loss injuries. there are always other factors at play.\nParticipants shared diverse perspectives on whether and how mental fatigue is deliberately factored into training. Three participants (37.5%) acknowledged that while mental fatigue is not always directly targeted, it is considered within the broader context of training design. This suggests that mental fatigue is managed through strategies aimed at enhancing overall performance.\nFor me, it’s about performance. It’s less about injury risk. If mental fatigue or reducing mental fatigue helps with that, then I’ve done my job.\nWe factor in mental fatigue in the timing of injury prevention strategies. On a day they have had significant cognitive and physical stimuli, you would probably do less work because you will lose them mentally.\nHowever, no participants reported deliberately factoring of mental fatigue into their training regimens. This reflects a more traditional focus on physical conditioning, with mental fatigue often being overlooked or managed incidentally rather than deliberately integrated into training protocols.\nWe only do more physical prevention. But no, we don’t have an aspect for mental fatigue prevention.\nIn contrast, there was a broader recognition of the need to mitigate mental fatigue, particularly to enhance athlete well‐being and performance. Five participants (62.5%) described various strategies employed to reduce mental fatigue, ranging from adjusting training schedules to providing psychological support. Adjustments were made to prevent cognitive overload right before a game, thus aiming to keep athletes mentally fresh. However, these were not done specifically in relation to injury, but rather for performance.\nWe try to do it but it’s really basic. We ask our players if they are mentally fatigued or physically fatigued with just a questionnaire at the beginning of each training camp.\nWe do the tactical analysis on a Thursday when the game is on a Saturday. because your athletes get mentally fatigued.\nAll eight participants (100%) identified two key areas for future research. Five participants (62.5%) emphasised the need for objective measures to assess mental fatigue, with a focus on reliable tools beyond self‐reported questionnaires.\nIt would be good to have an objective measure that’s not just players answering a questionnaire that can indicate mental fatigue.\nThree participants (37.5%) highlighted the importance of understanding the correlation between mental fatigue and injury, emphasising the need for evidence to support the role mental fatigue plays in injury risk.\nIt would be really interesting to look at how much of a role mental fatigue plays in injury. We’re not really thinking about mental fatigue leading to injury.\nSeven participants (87.5%) discussed the barriers to addressing mental fatigue in sports, grouping them into time constraints (\nThe fact that I don’t really know the signs or the background of mental fatigue. That’s holding me back a little bit to implement it.\nThe battle would be getting the players to consent. If you just presented a battery of tests without understanding that it’s real. They might not want to do it.\nAll eight participants (100%) expressed a preference for a variety of research presentation formats. Five participants (62.5%) favoured workshops, conferences and seminars as effective methods. Three participants (\nEveryone who is surrounding the team. Both the field staff and the medical staff.\nThe aim of this study was to explore practitioners' knowledge and perceptions on the potential association between mental fatigue and injury risk in invasion sports. To our knowledge, this is the first study to investigate such associations. The primary findings of this study suggest that practitioners working in invasion sports perceive there to be a link between mental fatigue and risk of injury, primarily noncontact injuries. The mechanisms proposed by participants include impaired motor control, poor biomechanics and reduced cognitive function, all of which might contribute to an increased risk of injury. However, distinguishing mental fatigue as an isolated factor is challenging, as it often presents concurrently with physical fatigue, making it difficult to make conclusions regarding its direct impact on injury. The results of this study provide a foundation for future research focused on the physiological mechanisms linking cognitive demands to injury risk.\nFindings from both phases of this study suggest that participants working in invasion sports perceive elevated mental fatigue as most likely to increase the risk of noncontact injuries. These results align with the review by Smith et al. (\nIn addition to noncontact injuries, participants in phase two expressed a potential relationship with mental fatigue on gradual onset injuries. Interestingly, this was considered less likely to be associated with mental fatigue in phase one, yet still prevalent. In‐depth discussions in phase two and free‐text responses in phase one suggest a perception that chronic exposure to mental fatigue could lead to prolonged stress and biomechanical changes placed on the body over time. Given that physical fatigue and mental fatigue are linked (Marcora et al.\nIndicators of mental fatigue that may result in injury, identified by participants in phase two, included noticeable physical and cognitive changes. The physical observations are consistent with existing research, which suggests that mental fatigue can result in postural changes, such as increased trunk flexion while walking (Qu et al.\nIn the current study, participants provided varied perspectives on whether mental fatigue differs between sex (assigned at birth, male and female) and the potential resulting injury risk. There was no discussion on direct links, and perceived risks of injuries were indirect, such as general conditioning, biological differences and cognitive capacity. Hardaker et al. (\nRegarding gender and mental fatigue, limited research has investigated differences in mental fatigue between male and females in an athletic population. Díaz García et al. (\nParticipants in phase one highlighted the broader impact of mental fatigue on athlete availability, suggesting that mental fatigue influences both preparation and the likelihood of illness. These results are consistent with research by Strand and Samuelson (\nNone of the participants in this study consider mental fatigue in their training design with the specific intention of reducing injury. This reflects a more traditional focus on physical conditioning, where mental fatigue was managed incidentally (Russell et al.,\nOne major limitation of this study is that all survey respondents held the perception that mental fatigue and injury are linked. Additionally, there is potential for a response bias, as practitioners who chose to engage with the survey might have had a prior interest in the study's topics, despite the survey link being publicly accessible. A Hawthorne effect may also be present, with participants possibly responding in a way they thought aligned with the study's purpose. Future research could benefit from maximum variation sampling to provide a broader understanding of this association across different sports and populations. Nevertheless, our findings suggest that participants perceive an association between mental fatigue and injury, which may reflect a consensus within the field. Another limitation of this study is that participants were from a limited number of countries and potentially specific sports within those countries, which may restrict the generalisability of the findings to a broader population.\nParticipants emphasised the need for more research into objective measures of mental fatigue. Subsequently, a clearer understanding of the association between mental fatigue and injury risk is necessary, as highlighted by several participants, to justify its inclusion within training programmes. Future research could benefit from longitudinal study designs that collect mental fatigue data alongside injury data during training and competition to investigate potential associations. Further, future research should explore methods to systematically incorporate mental fatigue in training with the aim to reduce injury risk.\nThe primary findings of this novel study suggest that practitioners involved in injury management within high‐performance invasion sports perceive there to be a link between mental fatigue and injury risk. Participants suggested that noncontact injuries are more likely to occur under periods of elevated mental fatigue. Potential mechanisms proposed by practitioners to link mental fatigue and risk of injury include impaired motor control, poor biomechanics and reduced cognitive function. The challenge in distinguishing mental fatigue as a directly contributing factor to injury is acknowledged, given mental fatigue typically occurs alongside physical fatigue. Future research focused on the mechanisms linking mental fatigue demands to injury risk is required to empirically examine and determine the validity of the perceptions discussed in this study. Nevertheless, practitioners who perceive mental fatigue as a potential risk factor for injury might consider incorporating athlete management strategies, such as mental fatigue screening or implementing targeted training breaks.\n\nPractitioners perceive that mental fatigue may increase the risk of injuries, in particular noncontact injuries, in high‐performance invasion sport athletes.\nPerceived indicators of mental fatigue which may relate to injury risk include deterioration in athlete posture and technique, decreased focus and poor decision‐making and behavioural changes such as disengagement.\nPractitioners could consider implementing strategies to reduce mental fatigue when indicators are observed, potentially mitigating injury risk. Approaches include brief rest breaks, caffeine‐maltodextrin mouth rinses or exposure to pleasant odours.\nThe authors declare no conflicts of interest.\nSupporting Information S1", "topic": "Brain"}
{"pmid": "39088005", "pmcid": "12309689", "title": "Revealing rhythm categorization in human brain activity", "publication_year": "2025", "abstract": "Humans across cultures show an outstanding capacity to perceive, learn, and produce musical rhythms. These skills rely on mapping the infinite space of possible rhythmic sensory inputs onto a finite set of internal rhythm categories. What is the nature of the brain processes underlying rhythm categorization? We used electroencephalography to measure brain activity as human participants listened to a continuum of rhythmic sequences characterized by repeating patterns of two interonset intervals. Using frequency and representational similarity analyses, we show that brain activity does not merely track the temporal structure of rhythmic inputs but, instead, produces categorical representation of rhythms. These neural rhythm categories arise automatically, independent of any motor- or timing-related tasks, yet exhibit strong similarity with categorization observed in overt behavior. Together, these results and methodological advances constitute a critical step toward understanding the biological roots and diversity of musical behaviors across cultures.\nHuman brain activity shows automatic categorization of rhythm, independent of but similar to categories produced in behavior.", "full_text": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "content_for_embedding": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "topic": "Brain"}
{"pmid": "39031088", "pmcid": "12309886", "title": "Effect of inaccurate b‐values from imaging gradients on intravoxel incoherent motion", "publication_year": "N/A", "abstract": "", "full_text": "Since the early days of diffusion MRI, intravoxel incoherent motion (IVIM) modeling has faced several challenges, most prominently its poor repeatability and reproducibility.\nAnother often‐neglected bias in DWI is the effect of imaging gradients on the diffusion weighting (b‐value). Indeed, imaging gradients can both increase and decrease the desired b‐value depending on their interaction with the diffusion encoding gradients.\nBecause b‐values of exactly 0 s/mm\nThe effect of imaging gradients has been studied for ADC,\nAlthough well studied for other DWI applications, the literature is scarce for IVIM. Yuan et al.\nThe b‐value is given by the trace of the b‐tensor (also known as the B matrix),\nThe b‐tensor of a full pulse sequence is given by the dephasing vector as\nGiven a set of imaging parameters, an imaging pulse sequence can be generated. These could be invariant during the image acquisition, yielding a constant\nAccording to Equation (\nPrewinders and rewinders are gradients that revert the additional dephasing caused by imaging gradients in order to return to the center of the k‐space.\nThe position of the prewinder and rewinder gradients are important in terms of the diffusion weighting. Because diffusion weighting accumulates for as long as\nTo evaluate the range of errors on the reported b‐values and resulting IVIM parameter estimates imposed by neglecting imaging gradients, three scenarios were conceptualized using Equation (\nThe three scenarios for b‐value calculations used in this work. They were chosen to isolate and study the effect of considering only a subset of contributions to the b‐tensor as described by Equation (\n\nThe simulations were limited to slice‐selective, crusher, and pre‐/rewinder gradients because these are the gradients that cause meaningful cross‐term interactions with the diffusion gradients that are propagated to the resulting b‐value.\nA sequence with large cross‐terms. The excitation rewinder was merged into the crusher prewinder, combined with crusher gradients that were always applied regardless of the diffusion gradients.\nA sequence with minimal cross‐terms, for which rewinding was performed immediately after excitation, and crushers were only applied when the diffusion gradients alone did not provide enough dephasing for crushing of spurious echoes.\nBoth sequence designs are commercially available by major vendors, making them relevant options for evaluation.\nThe gradient waveforms of both pulse sequences were generated using an in‐house Python script. For guidance, two commercial DWI sequences were studied (not shown) to make our evaluation applicable to current real‐world scenarios.\nThe gradient amplitude for slice selection during the excitation and refocusing RF‐pulses was defined as\nTriangular shapes were used for the crushers and pre‐/rewinders, with a target de‐/rephasing\nFigure\nSignal deviations of two different pulse sequence designs: a sequence with large cross‐terms (A), and a sequence with minimal cross‐terms (B). The [\nFor diffusion encoding, 200 well‐distributed directions over a sphere\nIn addition to the 200 well‐distributed directions, two direction sets were simulated to study directionally averaged signals: a standard three‐directional set (\nSimulations were performed for a range of slice thicknesses and in‐plane resolutions to evaluate the effect of each of the imaging parameters on the b‐value separately. For varying slice thickness, the in‐plane resolution was kept constant at 2 × 2 mm\nThe scenarios described in Table\nSignals were calculated using the bi‐exponential IVIM model\nFor a more general evaluation of errors, signals were calculated with three different parameter sets, each with one variable adjusted in 50 steps, whereas the others remained constant. The\nTo estimate the bias in IVIM parameter estimates, the signals were mapped to the assumed b‐values of the scenarios in Table\nFive patients with biopsy‐confirmed prostate cancer were scanned longitudinally on three separate occasions: a baseline scan before treatment, and two during treatment with a gonadotropin‐releasing hormone agonist and radiotherapy. The patients were classified as very high risk according to at least two of the following criteria: T3, Gleason grade 8, prostate‐specific antigen 20–49 μg/L or Gleason score 9–10 and/or prostate‐specific antigen ≥40 μg/L. The study was approved by The Swedish Ethical Review Authority (no. 2023–00088).\nIn vivo measurements were performed on a 3 T Signa Architect (GE Healthcare, Milwaukee, WI), software version MR30.0_R01. A commercial DWI‐SE‐EPI sequence with reduced FOV, spatially selective excitation, and chemical shift selective fat saturation was used. Axial FOV were used with matrix size 160 × 80, in‐plane resolution 1.5 × 1.5 mm\nThe gradient amplitudes of the full pulse sequence, that is, the sum of both imaging and diffusion gradients, were recorded for each excitation during phantom scans and saved as discretized\nThe actual b‐values were calculated with the effective\nImage analysis was performed using Hero version 2024.2.0 (Hero Imaging, Umeå, Sweden) and Python 3.11.5. The DWI volumes were reconstructed with the vendor‐specific denoising reconstruction AIR Recon DL (GE Healthcare, Milwaukee, WI).\nContrary to the simulations, due to the noise in the in vivo data, IVIM parameters were estimated using a segmented fitting approach\nThe fitting algorithm was implemented as follows. First, an estimate of\nFor a quantitative evaluation, 4000 voxels from whole‐prostate region of interest were randomly sampled from the total dataset of 15 exams (46,713 voxels in the complete dataset) for a comparison between the parameter estimates from the fits with the nominal and actual b‐values.\nImaging gradients caused the signal to deviate from its assumed nominal shape (Figure\nThe relative deviations found in the b‐values depended on the b‐values themselves, where the errors decreased with increasing b‐value (Figure\nRelative (upper row) and absolute (lower row) b‐value deviations as a function of b‐value, shown for both the sequence with large cross‐terms, and the sequence with minimal cross‐terms. The possible errors are shown as ranges between the highest resolution (1 mm isotropic) and lowest resolution (4 mm isotropic). (A) Deviations with the diffusion gradients on the\nDepending on image resolution, Figure\nThe actual b‐value of an “unweighted” diffusion image (b0) as a function of image resolution, simulated for both a sequence with minimal and large cross‐terms. This figure shows the minimal achievable b‐value for these pulse sequences at the given image resolutions.\nFigure\nThe relative deviation from the nominal b‐value for a subset of the 34 simulated b‐values as a function of isotropic resolution for a sequence with large cross‐terms (A) and sequence with minimal cross‐terms (B). The b‐values were chosen as representative for the low (50 s/mm\nIVIM analysis with nominal b‐values resulted in both positive and negative errors in the parameter estimates. The magnitude of these errors depended on pulse sequence design, diffusion directions, slice thickness (Figure\nIVIM errors versus slice thickness. Simulated IVIM parameter estimates produced from a pulse sequence with large cross‐terms (A) and minimal cross‐terms (B) as a function of slice thickness using different sets of diffusion directions and b‐value calculation methods. The in‐plane resolution was fixed to 2 × 2 mm\nIVIM errors versus in‐plane resolution. Simulated IVIM parameter estimates produced from a pulse sequence with large cross‐terms (A) and minimal cross‐terms (B) as a function of in‐plane resolution using different sets of diffusion directions and b‐value calculation methods. The slice thickness was fixed 4 mm. The lines show the fits to geometrically averaged signals for two different diffusion direction sets, with (orange) and without (blue) experimental cross‐term correction. The reference line shows fits performed with the actual b‐values including both imaging and cross‐terms, which was assumed as the ground truth. The signals were calculated using a large set of 34 b‐values between 0 and 800 s/mm\nThe relative errors of IVIM parameter estimates for different parameter sets for simulated sequences with large cross‐terms (A) and minimal cross‐terms (B). Signals were calculated with one varying parameter at a time, keeping the other two constant. The default values were\nWhen the imaging and the cross‐terms were not considered, IVIM parameter estimates were biased by varying sign and magnitude, which depended on the pulse sequence design (Figure\nThe relative b‐value error was approximately 6% across all b‐values (Table\nThe actual b‐values for each patient. The patient‐to‐patient variation is in the order of the first decimal with a positive trend between patient weight and b‐value.\nIn the prostate dataset, quantitative differences were found for all three IVIM parameters when estimated using both nominal and actual b‐values (Figure\nIVIM parameter distributions of 4000 voxels randomly sampled from 15 different prostate exams (total 46,713 voxels). Both the direction and the order of magnitude of the ΔMedians are consistent with the simulations of the sequence with large cross‐terms in Figures\nThe use of accurate b‐values has been recommended for IVIM analysis in the past.\nWe have studied the effect of PGSE imaging gradients on the b‐value. We simulated imaging scenarios with three relevant variables: in‐plane resolution, slice thickness, and pulse sequence design with respect to cross‐term interactions between imaging and diffusion gradients. The errors were evaluated from the b‐value to the final IVIM parameter estimates. We have shown that all three of these experimental variables contribute to errors, prompting the need for data harmonization by accounting for all the gradients in the calculation of the b‐value. Furthermore, we demonstrated the effect of using the true b‐values in IVIM analysis in vivo on prostate cancer in a clinical setting, which largely agreed with the results of the simulations. Our results highlight the importance of using actual b‐values for IVIM analysis, which would be greatly simplified if vendors made them accessible to users of DWI pulse sequences.\nFrom the simulations, we learned that pulse sequence design plays an important role in the accuracy of the b‐value. This corroborates previous works in which pulse sequence design has been demonstrated to affect the b‐value and subsequent DWI analysis if only diffusion gradients are considered.\nThe deviation of the directionally averaged signals can become negligibly small if a pulse sequence with minimal cross‐terms is used. This emphasizes the importance of pulse sequence design, and is indeed in line with previous studies in which cross‐terms have been highlighted as a major source of error.\nThe only scenario in our simulations in which data was fully harmonized, and negligible errors were seen due to pulse sequence design or image resolution, was when both imaging gradients and cross‐terms were included in the analysis. We showed that by performing experimental cross‐term correction using antipodal diffusion directions together with the simple additive\nThe errors in the perfusion‐related parameters\nA limitation of the simulations was that only two PGSE sequence designs were evaluated. Because gradients can be arranged in numerous different ways, we decided to limit this work to realistic best‐ and worst‐case scenarios. For example, an in‐between configuration could be one in which the readout prewinder is merged with the crushers rather than being performed immediately before readout. However, we ignored the EPI‐readout portion of the pulse sequence because it has previously shown negligible contributions to the b‐value\nA future prospect beyond the scope of this paper is to investigate the effect on advanced diffusion encoding and modeling. Current research into velocity‐compensated IVIM modeling utilizes bipolar velocity‐compensated diffusion encoding gradients,\nThe first finding of the in vivo measurements was the fact that the actual b‐values we obtained using Equation (\nNotably, the errors in the b‐values (Table\nIn addition to the common error in the b‐value, there was a slight variation between subjects, likely caused by the different body weights. Because built‐in specific absorption rate calculations preceded the scans, adjustments of the RF pulse properties likely affected the imaging gradients according to Equation (\nWhereas the errors in the experimental b‐values were smaller than those seen in the simulations, the shift in the medians of the parameter estimates was similar to that implied by the simulations. This demonstrates that even small errors in the b‐values can lead to quantitative errors in the parameters. Therefore, the usage of actual b‐values does matter whenever quantitative values are of interest, especially when comparisons are made between studies with different pulse sequence implementations and imaging resolution, where the errors could be of varying sign and magnitude. However, using actual b‐values does not solve the precision problem in IVIM. The parameter distributions in Figure\nA limitation in our in vivo evaluation is that we only performed measurements on a single MR scanner and pulse sequence implementation. However, our simulations cover several different scenarios to showcase the possible differences in such a comparison. Additionally, we believe the comparison with Schneider et al.\nThere are additional possibilities for further investigation into IVIM parameter estimation. Different fitting algorithms can have varying sensitivities to biased b‐values when estimating parameters. Additionally, algorithms based on deep learning could also be impacted by this error, for example, when discrepancies in bias occur between training data and inference data. These aspects of IVIM parameter estimation remain to be explored.\nThe imaging gradients and their cross‐terms with the diffusion gradients cause errors in the b‐values which, when not accounted for, result in a loss of accuracy in the IVIM parameters. The sign and magnitude of these errors depend on the pulse sequence design and the setup of imaging parameters, which can be highly heterogeneous in a multi‐site setting. Including all the imaging gradients in the calculation of the b‐value improves the accuracy, and therefore the reproducibility and comparability, of IVIM results.\nFunding support was provided by The Swedish Cancer Society (grants 21 1910S, 22 2011 Pj, 22 0592 JIA), with additional support provided by\nPatrik Brynolfsson is a cofounder and shareholder of Hero Imaging AB and is an active developer of the Hero platform.", "content_for_embedding": "Since the early days of diffusion MRI, intravoxel incoherent motion (IVIM) modeling has faced several challenges, most prominently its poor repeatability and reproducibility.\nAnother often‐neglected bias in DWI is the effect of imaging gradients on the diffusion weighting (b‐value). Indeed, imaging gradients can both increase and decrease the desired b‐value depending on their interaction with the diffusion encoding gradients.\nBecause b‐values of exactly 0 s/mm\nThe effect of imaging gradients has been studied for ADC,\nAlthough well studied for other DWI applications, the literature is scarce for IVIM. Yuan et al.\nThe b‐value is given by the trace of the b‐tensor (also known as the B matrix),\nThe b‐tensor of a full pulse sequence is given by the dephasing vector as\nGiven a set of imaging parameters, an imaging pulse sequence can be generated. These could be invariant during the image acquisition, yielding a constant\nAccording to Equation (\nPrewinders and rewinders are gradients that revert the additional dephasing caused by imaging gradients in order to return to the center of the k‐space.\nThe position of the prewinder and rewinder gradients are important in terms of the diffusion weighting. Because diffusion weighting accumulates for as long as\nTo evaluate the range of errors on the reported b‐values and resulting IVIM parameter estimates imposed by neglecting imaging gradients, three scenarios were conceptualized using Equation (\nThe three scenarios for b‐value calculations used in this work. They were chosen to isolate and study the effect of considering only a subset of contributions to the b‐tensor as described by Equation (\n\nThe simulations were limited to slice‐selective, crusher, and pre‐/rewinder gradients because these are the gradients that cause meaningful cross‐term interactions with the diffusion gradients that are propagated to the resulting b‐value.\nA sequence with large cross‐terms. The excitation rewinder was merged into the crusher prewinder, combined with crusher gradients that were always applied regardless of the diffusion gradients.\nA sequence with minimal cross‐terms, for which rewinding was performed immediately after excitation, and crushers were only applied when the diffusion gradients alone did not provide enough dephasing for crushing of spurious echoes.\nBoth sequence designs are commercially available by major vendors, making them relevant options for evaluation.\nThe gradient waveforms of both pulse sequences were generated using an in‐house Python script. For guidance, two commercial DWI sequences were studied (not shown) to make our evaluation applicable to current real‐world scenarios.\nThe gradient amplitude for slice selection during the excitation and refocusing RF‐pulses was defined as\nTriangular shapes were used for the crushers and pre‐/rewinders, with a target de‐/rephasing\nFigure\nSignal deviations of two different pulse sequence designs: a sequence with large cross‐terms (A), and a sequence with minimal cross‐terms (B). The [\nFor diffusion encoding, 200 well‐distributed directions over a sphere\nIn addition to the 200 well‐distributed directions, two direction sets were simulated to study directionally averaged signals: a standard three‐directional set (\nSimulations were performed for a range of slice thicknesses and in‐plane resolutions to evaluate the effect of each of the imaging parameters on the b‐value separately. For varying slice thickness, the in‐plane resolution was kept constant at 2 × 2 mm\nThe scenarios described in Table\nSignals were calculated using the bi‐exponential IVIM model\nFor a more general evaluation of errors, signals were calculated with three different parameter sets, each with one variable adjusted in 50 steps, whereas the others remained constant. The\nTo estimate the bias in IVIM parameter estimates, the signals were mapped to the assumed b‐values of the scenarios in Table\nFive patients with biopsy‐confirmed prostate cancer were scanned longitudinally on three separate occasions: a baseline scan before treatment, and two during treatment with a gonadotropin‐releasing hormone agonist and radiotherapy. The patients were classified as very high risk according to at least two of the following criteria: T3, Gleason grade 8, prostate‐specific antigen 20–49 μg/L or Gleason score 9–10 and/or prostate‐specific antigen ≥40 μg/L. The study was approved by The Swedish Ethical Review Authority (no. 2023–00088).\nIn vivo measurements were performed on a 3 T Signa Architect (GE Healthcare, Milwaukee, WI), software version MR30.0_R01. A commercial DWI‐SE‐EPI sequence with reduced FOV, spatially selective excitation, and chemical shift selective fat saturation was used. Axial FOV were used with matrix size 160 × 80, in‐plane resolution 1.5 × 1.5 mm\nThe gradient amplitudes of the full pulse sequence, that is, the sum of both imaging and diffusion gradients, were recorded for each excitation during phantom scans and saved as discretized\nThe actual b‐values were calculated with the effective\nImage analysis was performed using Hero version 2024.2.0 (Hero Imaging, Umeå, Sweden) and Python 3.11.5. The DWI volumes were reconstructed with the vendor‐specific denoising reconstruction AIR Recon DL (GE Healthcare, Milwaukee, WI).\nContrary to the simulations, due to the noise in the in vivo data, IVIM parameters were estimated using a segmented fitting approach\nThe fitting algorithm was implemented as follows. First, an estimate of\nFor a quantitative evaluation, 4000 voxels from whole‐prostate region of interest were randomly sampled from the total dataset of 15 exams (46,713 voxels in the complete dataset) for a comparison between the parameter estimates from the fits with the nominal and actual b‐values.\nImaging gradients caused the signal to deviate from its assumed nominal shape (Figure\nThe relative deviations found in the b‐values depended on the b‐values themselves, where the errors decreased with increasing b‐value (Figure\nRelative (upper row) and absolute (lower row) b‐value deviations as a function of b‐value, shown for both the sequence with large cross‐terms, and the sequence with minimal cross‐terms. The possible errors are shown as ranges between the highest resolution (1 mm isotropic) and lowest resolution (4 mm isotropic). (A) Deviations with the diffusion gradients on the\nDepending on image resolution, Figure\nThe actual b‐value of an “unweighted” diffusion image (b0) as a function of image resolution, simulated for both a sequence with minimal and large cross‐terms. This figure shows the minimal achievable b‐value for these pulse sequences at the given image resolutions.\nFigure\nThe relative deviation from the nominal b‐value for a subset of the 34 simulated b‐values as a function of isotropic resolution for a sequence with large cross‐terms (A) and sequence with minimal cross‐terms (B). The b‐values were chosen as representative for the low (50 s/mm\nIVIM analysis with nominal b‐values resulted in both positive and negative errors in the parameter estimates. The magnitude of these errors depended on pulse sequence design, diffusion directions, slice thickness (Figure\nIVIM errors versus slice thickness. Simulated IVIM parameter estimates produced from a pulse sequence with large cross‐terms (A) and minimal cross‐terms (B) as a function of slice thickness using different sets of diffusion directions and b‐value calculation methods. The in‐plane resolution was fixed to 2 × 2 mm\nIVIM errors versus in‐plane resolution. Simulated IVIM parameter estimates produced from a pulse sequence with large cross‐terms (A) and minimal cross‐terms (B) as a function of in‐plane resolution using different sets of diffusion directions and b‐value calculation methods. The slice thickness was fixed 4 mm. The lines show the fits to geometrically averaged signals for two different diffusion direction sets, with (orange) and without (blue) experimental cross‐term correction. The reference line shows fits performed with the actual b‐values including both imaging and cross‐terms, which was assumed as the ground truth. The signals were calculated using a large set of 34 b‐values between 0 and 800 s/mm\nThe relative errors of IVIM parameter estimates for different parameter sets for simulated sequences with large cross‐terms (A) and minimal cross‐terms (B). Signals were calculated with one varying parameter at a time, keeping the other two constant. The default values were\nWhen the imaging and the cross‐terms were not considered, IVIM parameter estimates were biased by varying sign and magnitude, which depended on the pulse sequence design (Figure\nThe relative b‐value error was approximately 6% across all b‐values (Table\nThe actual b‐values for each patient. The patient‐to‐patient variation is in the order of the first decimal with a positive trend between patient weight and b‐value.\nIn the prostate dataset, quantitative differences were found for all three IVIM parameters when estimated using both nominal and actual b‐values (Figure\nIVIM parameter distributions of 4000 voxels randomly sampled from 15 different prostate exams (total 46,713 voxels). Both the direction and the order of magnitude of the ΔMedians are consistent with the simulations of the sequence with large cross‐terms in Figures\nThe use of accurate b‐values has been recommended for IVIM analysis in the past.\nWe have studied the effect of PGSE imaging gradients on the b‐value. We simulated imaging scenarios with three relevant variables: in‐plane resolution, slice thickness, and pulse sequence design with respect to cross‐term interactions between imaging and diffusion gradients. The errors were evaluated from the b‐value to the final IVIM parameter estimates. We have shown that all three of these experimental variables contribute to errors, prompting the need for data harmonization by accounting for all the gradients in the calculation of the b‐value. Furthermore, we demonstrated the effect of using the true b‐values in IVIM analysis in vivo on prostate cancer in a clinical setting, which largely agreed with the results of the simulations. Our results highlight the importance of using actual b‐values for IVIM analysis, which would be greatly simplified if vendors made them accessible to users of DWI pulse sequences.\nFrom the simulations, we learned that pulse sequence design plays an important role in the accuracy of the b‐value. This corroborates previous works in which pulse sequence design has been demonstrated to affect the b‐value and subsequent DWI analysis if only diffusion gradients are considered.\nThe deviation of the directionally averaged signals can become negligibly small if a pulse sequence with minimal cross‐terms is used. This emphasizes the importance of pulse sequence design, and is indeed in line with previous studies in which cross‐terms have been highlighted as a major source of error.\nThe only scenario in our simulations in which data was fully harmonized, and negligible errors were seen due to pulse sequence design or image resolution, was when both imaging gradients and cross‐terms were included in the analysis. We showed that by performing experimental cross‐term correction using antipodal diffusion directions together with the simple additive\nThe errors in the perfusion‐related parameters\nA limitation of the simulations was that only two PGSE sequence designs were evaluated. Because gradients can be arranged in numerous different ways, we decided to limit this work to realistic best‐ and worst‐case scenarios. For example, an in‐between configuration could be one in which the readout prewinder is merged with the crushers rather than being performed immediately before readout. However, we ignored the EPI‐readout portion of the pulse sequence because it has previously shown negligible contributions to the b‐value\nA future prospect beyond the scope of this paper is to investigate the effect on advanced diffusion encoding and modeling. Current research into velocity‐compensated IVIM modeling utilizes bipolar velocity‐compensated diffusion encoding gradients,\nThe first finding of the in vivo measurements was the fact that the actual b‐values we obtained using Equation (\nNotably, the errors in the b‐values (Table\nIn addition to the common error in the b‐value, there was a slight variation between subjects, likely caused by the different body weights. Because built‐in specific absorption rate calculations preceded the scans, adjustments of the RF pulse properties likely affected the imaging gradients according to Equation (\nWhereas the errors in the experimental b‐values were smaller than those seen in the simulations, the shift in the medians of the parameter estimates was similar to that implied by the simulations. This demonstrates that even small errors in the b‐values can lead to quantitative errors in the parameters. Therefore, the usage of actual b‐values does matter whenever quantitative values are of interest, especially when comparisons are made between studies with different pulse sequence implementations and imaging resolution, where the errors could be of varying sign and magnitude. However, using actual b‐values does not solve the precision problem in IVIM. The parameter distributions in Figure\nA limitation in our in vivo evaluation is that we only performed measurements on a single MR scanner and pulse sequence implementation. However, our simulations cover several different scenarios to showcase the possible differences in such a comparison. Additionally, we believe the comparison with Schneider et al.\nThere are additional possibilities for further investigation into IVIM parameter estimation. Different fitting algorithms can have varying sensitivities to biased b‐values when estimating parameters. Additionally, algorithms based on deep learning could also be impacted by this error, for example, when discrepancies in bias occur between training data and inference data. These aspects of IVIM parameter estimation remain to be explored.\nThe imaging gradients and their cross‐terms with the diffusion gradients cause errors in the b‐values which, when not accounted for, result in a loss of accuracy in the IVIM parameters. The sign and magnitude of these errors depend on the pulse sequence design and the setup of imaging parameters, which can be highly heterogeneous in a multi‐site setting. Including all the imaging gradients in the calculation of the b‐value improves the accuracy, and therefore the reproducibility and comparability, of IVIM results.\nFunding support was provided by The Swedish Cancer Society (grants 21 1910S, 22 2011 Pj, 22 0592 JIA), with additional support provided by\nPatrik Brynolfsson is a cofounder and shareholder of Hero Imaging AB and is an active developer of the Hero platform.", "topic": "Brain"}
{"pmid": "38976183", "pmcid": "12305257", "title": "TONEFACT: Can even advanced hemorrhoids be treated without surgery? A paradigm shift in the management of hemorrhoids", "publication_year": "N/A", "abstract": "Hemorrhoids are one of the most common anorectal disorders. Early hemorrhoids are treated conservatively, but advanced hemorrhoids are usually treated with surgery. However, in the last decade, we have worked extensively in the field of conservative management of hemorrhoids. From our experience, we could manage a large proportion of advanced hemorrhoids without surgery by a treatment concept (TONEFACT) with a high satisfaction rate. Evidence for the TONEFACT approach primarily comes from observational studies and a prospective, non-randomized study of 85 patients. This has been shown to improve defecation time and reduce prolapse symptoms in early-stage hemorrhoids, leading to fewer surgical interventions. Although promising, these observations lack validation from more extensive randomized controlled trials to draw firm conclusions. In this opinion review, without using much data, we will discuss our viewpoint based on our experience as specified by the journal guidelines.", "full_text": "\nHemorrhoids are quite prevalent, and it is estimated that about 22%-30% of the population will suffer from hemorrhoids in their lifetime[\nIn recent decades, conservative management strategies for advanced hemorrhoids have received relatively less attention, possibly due to the traditional surgical perspective[\nThe concept and the TONEFACT management concept are summarized in Table\nCategorized description and benefits of TONEFACT regimen\nThe three main causative factors that initiate or worsen the hemorrhoids and are tackled by TONEFACT are: (1) Prolonged defecation time (addressed by T, N, F, A, T); (2) Excessive or repeated straining (addressed by C, E); and (3) Decreased muscle tone (worsen prolapse; addressed by E). The TONEFACT concept effectively addresses these three causative factors (Table\nOverview of TONEFACT treatment.\nAmongst the several causative factors of hemorrhoids (Figure\nThough these factors were known to be causative for a long time and the patients were also routinely advised by their physicians to avoid them, this knowledge was not adequately helping. The reason for this was the lack of a definitive ‘endpoint’ of the treatment goal (‘endpoints’ are values that are not harmful and are considered “normal”). In all chronic diseases, objectively defining the treatment endpoint is fundamental to achieving the desired results and vice versa. To exemplify, let us assume that the endpoint of managing hypertension [blood pressure (BP) < 140/90] is not defined and is not clear to the treating physician. A patient reports with a BP of 220/100. He would be simply advised to decrease his BP without specifying any endpoint. The patient would start the medication, and his BP stabilizes at 180/90. He would then be satisfied that his BP has been reduced, would be content with the treatment, and would not report back to his physician, thereby leading to damage due to high BP. Even reporting back to the physician will not help because the physician will also be satisfied (as the BP has been lowered). After all, the treatment goal was also not clear in his mind. Therefore, without defining a definitive treatment endpoint, it is simply unthinkable to treat any chronic disease, including hypertension, high serum cholesterol in coronary artery disease, diabetes mellitus, or cancers.\nIn patients with chronic constipation and hemorrhoids, the treatment endpoints for defecation time and frequency were never defined. After working extensively with hundreds of patients, we analyzed that the risk of hemorrhoid prolapse, along with its progression, hemorrhoidal rupture leading to bleeding, and hemorrhoidal thrombosis, becomes much less when defecation time < 5 minutes and defecation frequency was not more than once or twice per day. Subsequently, we published the concept of TONEFACT[\nAmongst the listed three factors, the prolonged defection time has been recognized as the main causative factor. Apart from defining the treatment endpoint (3 minutes during defecation), the other components helpful in preventing prolonged defecation time follow.\nThe habit of taking a mobile or newspaper is quite detrimental as it leads to a significant increase in time[\nThis habit is also harmful to the neck and the spine, as the people who used smartphones in the lavatory had considerably larger cervical and spinal flexion angles than those who did not[\nThere is a lot of confusion and a lack of knowledge due to heterogeneity and the overlapping effects among different fibers. It is difficult for laypersons and even physicians to correctly identify the fiber to be consumed. Therefore, the usual recommendation is to increase fiber in the diet, which is most commonly perceived as increased intake of fruits and vegetables. Such broad, non-specific recommendations are more like a cliché and are generally met with poor compliance, especially on a long-term basis[\nIt is difficult to define a particular fiber as an optimum fiber, as none of the fibers possesses all the beneficial characteristics. However, the fiber that comes closest to being optimum is psyllium husk (ispaghula husk)[\nPreviously published studies analyzed the psyllium supplementation at much lower doses (5-10 g/day) in various clinical conditions like chronic constipation and irritable bowel syndrome (IBS)[\nThe second aspect is the amount of water taken along with the fiber. Psyllium, being a soluble fiber, absorbs water and swells, and it takes the absorbed water to the rectum, thereby making stools softer and bulkier. However, for this to happen optimally, adequate water must be taken along with the fiber (25 mL water/g fiber, 500 mL water with 25 g fiber/day)[\nChronic idiopathic constipation (CIC) affects 10%-17% of the world’s population[\nHowever, it was found that in many patients, the decreased water intake played a significant role in CIC, and the genetic component in CIC is likely to be mediated by a lack of thirst (LOT)[\nIt was observed that increasing water intake decreased the intensity and symptoms of CIC[\nHowever, drinking 3.5 L of water can be an uphill task for patients, a majority of whom are used to drinking minimal amounts of water (half to 1 L per day). These patients were recommended to fix time slots to drink water. These time slots may be 30 minutes before and 60 minutes after meals as drinking too much water is not recommended with meals[\nMany people do not attempt to strain at all during defecation due to a prevailing belief in many cultures that straining while defecating is harmful. Though it is correct that excessive straining during defecation is harmful, no/minimal straining also prolongs the defecation time significantly. The key lies in maintaining the optimum balance. Recommending that patients apply pressure while defecating helped to decrease defecation time in most patients.\nIncreasing the hip flexion by sitting in a squatting position or raising the legs in the sitting position straightens the rectoanal canal[\nIt was observed that many people significantly prolong defecation time just to achieve the feeling of 100% evacuation, even though they had cleared most of the rectum in the first few minutes. This led to a significant prolongation of defecation time. They need to be counselled that to avoid sitting for a long time, it is not harmful if they apply proper pressure to evacuate a major part of the rectum in 3-5 minutes and then finish. This helps to decrease the defecation time. The urge for a complete 100% evacuation can be ignored and will pass after a few minutes. This works well as the body gets accustomed to less time during defecation, and gradually, such patients can clear their complete bowels in 3-5 minutes. Though this habit is not present in every patient, it is seen in a good proportion of patients, and proper counselling goes a long way in reducing the defecation time.\nMany patients develop a habit of defecating multiple times, especially in South Asian countries where the diet is high in fiber and the stools are bulkier. This habit of going multiple times puts excessive strain on hemorrhoids, leading to their rupture and even progression. However, this habit can be corrected without much difficulty by advising patients to properly evacuate the rectum by applying adequate pressure (as discussed above) and then ignoring subsequent urges for clearing the bowels. The reason behind this is a cultural belief that if a person has an urge to defecate, then it should not be curbed. The patients need to be counselled that holding the urge to defecate until the next day after already defecating is not harmful. Hence, the subsequent urges can be ignored. This counselling works well, and most patients can decrease frequency from several times to once or twice a day.\nIt happens, not infrequently, that a patient with existing hemorrhoids tries to attempt defecation out of their schedule for social reasons. It leads to excessive straining, leading to hemorrhoidal rupture (bleeding) and even thrombosed hemorrhoids[\nThe strength of pelvic floor muscles and anal tone play an important role in delaying and recovering from the hemorrhoidal prolapse. Therefore, recommending Kegel exercises (KE) to patients with prolapsing hemorrhoids (grade II, III, and IV) helps to decrease the prolapse and stop/slow the progression of prolapse when coupled with other components of TONEFACT.\nKE, also known as pelvic floor muscle training or pelvic floor muscle exercises, is a non-invasive and safe behavioral treatment method shown to be effective in increasing pelvic floor muscle strength[\nA visual flowchart depicting the clinical application pathway of TONEFACT is depicted in Figure\nFlow chart to implement TONEFACT.\nMultiple physiological mechanisms explain how behavioral modifications lead to the regression of hemorrhoidal symptoms. Dietary and defecation habit changes help alleviate inflammation in the hemorrhoidal plexus, contributing to the mechanism of symptom development[\nVarious outpatient procedures, such as sclerotherapy, infrared coagulation, rubber band ligation,\nImplementing the TONEFACT regimen successfully requires a level of motivation and self-discipline in the patient and persistent effort from the treating physician to guide his patient. TONEFACT helps to prevent surgery in a good proportion of such patients.\nAn individualized approach is essential as not all causative factors may be present in all patients. This requires the real-world application of TONEFACT, including data collection practices, follow-up strategies, and the role of structured counseling in enhancing compliance. Precisely the same protocol is followed at our institute, the Garg Fistula Research Institute. Though tremendous efforts are required, the results are equally gratifying when the patients who had been recommended for surgery (late grade II, grade III, thrombosed hemorrhoids patients, and early grade patients with excessive bleeding) no longer need surgery. In our experience implementing TONEFACT in early hemorrhoids is even more gratifying, as it prevents progression in most patients. Therefore, the reduction in disease burden (of advanced hemorrhoids) would be huge, as the patient load in early hemorrhoids is even larger. There are a few centers where the conservative management of hemorrhoids (even advanced ones) is done. However, our experience can help to highlight that the TONEFACT concept is effective in preventing surgery in a large number, if not all, patients with hemorrhoids with excessive bleeding and advanced hemorrhoids.\nAs TONEFACT is a lifestyle modification, the associated costs are negligible, primarily limited to purchasing fiber supplements. In contrast, surgical interventions entail substantial expenses due to hospital admission, operative procedures, anesthesia, and postoperative recovery. This stark cost differential underscores the potential cost-effectiveness of TONEFACT in alleviating the global healthcare burden, particularly in patients who can avoid surgery altogether.\nLimitations of this opinion review are a small sample size and studies included in this review are non-comparative and non-randomized as no comparative study is available for advanced-grade hemorrhoids and nonsurgical measures. Though we took the utmost care, selection bias is still possible. We also understand the skepticism about the ability of behavioral modifications alone to reduce prolapsed hemorrhoids. While we do not claim complete reversal in all cases, our experience, supported by long-term follow-up data, indicates significant symptomatic improvement and avoidance of surgery in a substantial proportion of patients. However, we acknowledge the need for future studies to incorporate larger, stratified cohorts and ideally include randomized controlled trials for more definitive conclusions and to validate these findings. The main strength was that it represents the first review of conservative management for advanced-grade hemorrhoids and has promising results.\nTONEFACT is a promising and potentially transformative approach to treat early and advanced hemorrhoids without surgery. However, prospective randomized controlled trials are required to authenticate and validate the findings of TONEFACT treatment in patients with hemorrhoids.", "content_for_embedding": "\nHemorrhoids are quite prevalent, and it is estimated that about 22%-30% of the population will suffer from hemorrhoids in their lifetime[\nIn recent decades, conservative management strategies for advanced hemorrhoids have received relatively less attention, possibly due to the traditional surgical perspective[\nThe concept and the TONEFACT management concept are summarized in Table\nCategorized description and benefits of TONEFACT regimen\nThe three main causative factors that initiate or worsen the hemorrhoids and are tackled by TONEFACT are: (1) Prolonged defecation time (addressed by T, N, F, A, T); (2) Excessive or repeated straining (addressed by C, E); and (3) Decreased muscle tone (worsen prolapse; addressed by E). The TONEFACT concept effectively addresses these three causative factors (Table\nOverview of TONEFACT treatment.\nAmongst the several causative factors of hemorrhoids (Figure\nThough these factors were known to be causative for a long time and the patients were also routinely advised by their physicians to avoid them, this knowledge was not adequately helping. The reason for this was the lack of a definitive ‘endpoint’ of the treatment goal (‘endpoints’ are values that are not harmful and are considered “normal”). In all chronic diseases, objectively defining the treatment endpoint is fundamental to achieving the desired results and vice versa. To exemplify, let us assume that the endpoint of managing hypertension [blood pressure (BP) < 140/90] is not defined and is not clear to the treating physician. A patient reports with a BP of 220/100. He would be simply advised to decrease his BP without specifying any endpoint. The patient would start the medication, and his BP stabilizes at 180/90. He would then be satisfied that his BP has been reduced, would be content with the treatment, and would not report back to his physician, thereby leading to damage due to high BP. Even reporting back to the physician will not help because the physician will also be satisfied (as the BP has been lowered). After all, the treatment goal was also not clear in his mind. Therefore, without defining a definitive treatment endpoint, it is simply unthinkable to treat any chronic disease, including hypertension, high serum cholesterol in coronary artery disease, diabetes mellitus, or cancers.\nIn patients with chronic constipation and hemorrhoids, the treatment endpoints for defecation time and frequency were never defined. After working extensively with hundreds of patients, we analyzed that the risk of hemorrhoid prolapse, along with its progression, hemorrhoidal rupture leading to bleeding, and hemorrhoidal thrombosis, becomes much less when defecation time < 5 minutes and defecation frequency was not more than once or twice per day. Subsequently, we published the concept of TONEFACT[\nAmongst the listed three factors, the prolonged defection time has been recognized as the main causative factor. Apart from defining the treatment endpoint (3 minutes during defecation), the other components helpful in preventing prolonged defecation time follow.\nThe habit of taking a mobile or newspaper is quite detrimental as it leads to a significant increase in time[\nThis habit is also harmful to the neck and the spine, as the people who used smartphones in the lavatory had considerably larger cervical and spinal flexion angles than those who did not[\nThere is a lot of confusion and a lack of knowledge due to heterogeneity and the overlapping effects among different fibers. It is difficult for laypersons and even physicians to correctly identify the fiber to be consumed. Therefore, the usual recommendation is to increase fiber in the diet, which is most commonly perceived as increased intake of fruits and vegetables. Such broad, non-specific recommendations are more like a cliché and are generally met with poor compliance, especially on a long-term basis[\nIt is difficult to define a particular fiber as an optimum fiber, as none of the fibers possesses all the beneficial characteristics. However, the fiber that comes closest to being optimum is psyllium husk (ispaghula husk)[\nPreviously published studies analyzed the psyllium supplementation at much lower doses (5-10 g/day) in various clinical conditions like chronic constipation and irritable bowel syndrome (IBS)[\nThe second aspect is the amount of water taken along with the fiber. Psyllium, being a soluble fiber, absorbs water and swells, and it takes the absorbed water to the rectum, thereby making stools softer and bulkier. However, for this to happen optimally, adequate water must be taken along with the fiber (25 mL water/g fiber, 500 mL water with 25 g fiber/day)[\nChronic idiopathic constipation (CIC) affects 10%-17% of the world’s population[\nHowever, it was found that in many patients, the decreased water intake played a significant role in CIC, and the genetic component in CIC is likely to be mediated by a lack of thirst (LOT)[\nIt was observed that increasing water intake decreased the intensity and symptoms of CIC[\nHowever, drinking 3.5 L of water can be an uphill task for patients, a majority of whom are used to drinking minimal amounts of water (half to 1 L per day). These patients were recommended to fix time slots to drink water. These time slots may be 30 minutes before and 60 minutes after meals as drinking too much water is not recommended with meals[\nMany people do not attempt to strain at all during defecation due to a prevailing belief in many cultures that straining while defecating is harmful. Though it is correct that excessive straining during defecation is harmful, no/minimal straining also prolongs the defecation time significantly. The key lies in maintaining the optimum balance. Recommending that patients apply pressure while defecating helped to decrease defecation time in most patients.\nIncreasing the hip flexion by sitting in a squatting position or raising the legs in the sitting position straightens the rectoanal canal[\nIt was observed that many people significantly prolong defecation time just to achieve the feeling of 100% evacuation, even though they had cleared most of the rectum in the first few minutes. This led to a significant prolongation of defecation time. They need to be counselled that to avoid sitting for a long time, it is not harmful if they apply proper pressure to evacuate a major part of the rectum in 3-5 minutes and then finish. This helps to decrease the defecation time. The urge for a complete 100% evacuation can be ignored and will pass after a few minutes. This works well as the body gets accustomed to less time during defecation, and gradually, such patients can clear their complete bowels in 3-5 minutes. Though this habit is not present in every patient, it is seen in a good proportion of patients, and proper counselling goes a long way in reducing the defecation time.\nMany patients develop a habit of defecating multiple times, especially in South Asian countries where the diet is high in fiber and the stools are bulkier. This habit of going multiple times puts excessive strain on hemorrhoids, leading to their rupture and even progression. However, this habit can be corrected without much difficulty by advising patients to properly evacuate the rectum by applying adequate pressure (as discussed above) and then ignoring subsequent urges for clearing the bowels. The reason behind this is a cultural belief that if a person has an urge to defecate, then it should not be curbed. The patients need to be counselled that holding the urge to defecate until the next day after already defecating is not harmful. Hence, the subsequent urges can be ignored. This counselling works well, and most patients can decrease frequency from several times to once or twice a day.\nIt happens, not infrequently, that a patient with existing hemorrhoids tries to attempt defecation out of their schedule for social reasons. It leads to excessive straining, leading to hemorrhoidal rupture (bleeding) and even thrombosed hemorrhoids[\nThe strength of pelvic floor muscles and anal tone play an important role in delaying and recovering from the hemorrhoidal prolapse. Therefore, recommending Kegel exercises (KE) to patients with prolapsing hemorrhoids (grade II, III, and IV) helps to decrease the prolapse and stop/slow the progression of prolapse when coupled with other components of TONEFACT.\nKE, also known as pelvic floor muscle training or pelvic floor muscle exercises, is a non-invasive and safe behavioral treatment method shown to be effective in increasing pelvic floor muscle strength[\nA visual flowchart depicting the clinical application pathway of TONEFACT is depicted in Figure\nFlow chart to implement TONEFACT.\nMultiple physiological mechanisms explain how behavioral modifications lead to the regression of hemorrhoidal symptoms. Dietary and defecation habit changes help alleviate inflammation in the hemorrhoidal plexus, contributing to the mechanism of symptom development[\nVarious outpatient procedures, such as sclerotherapy, infrared coagulation, rubber band ligation,\nImplementing the TONEFACT regimen successfully requires a level of motivation and self-discipline in the patient and persistent effort from the treating physician to guide his patient. TONEFACT helps to prevent surgery in a good proportion of such patients.\nAn individualized approach is essential as not all causative factors may be present in all patients. This requires the real-world application of TONEFACT, including data collection practices, follow-up strategies, and the role of structured counseling in enhancing compliance. Precisely the same protocol is followed at our institute, the Garg Fistula Research Institute. Though tremendous efforts are required, the results are equally gratifying when the patients who had been recommended for surgery (late grade II, grade III, thrombosed hemorrhoids patients, and early grade patients with excessive bleeding) no longer need surgery. In our experience implementing TONEFACT in early hemorrhoids is even more gratifying, as it prevents progression in most patients. Therefore, the reduction in disease burden (of advanced hemorrhoids) would be huge, as the patient load in early hemorrhoids is even larger. There are a few centers where the conservative management of hemorrhoids (even advanced ones) is done. However, our experience can help to highlight that the TONEFACT concept is effective in preventing surgery in a large number, if not all, patients with hemorrhoids with excessive bleeding and advanced hemorrhoids.\nAs TONEFACT is a lifestyle modification, the associated costs are negligible, primarily limited to purchasing fiber supplements. In contrast, surgical interventions entail substantial expenses due to hospital admission, operative procedures, anesthesia, and postoperative recovery. This stark cost differential underscores the potential cost-effectiveness of TONEFACT in alleviating the global healthcare burden, particularly in patients who can avoid surgery altogether.\nLimitations of this opinion review are a small sample size and studies included in this review are non-comparative and non-randomized as no comparative study is available for advanced-grade hemorrhoids and nonsurgical measures. Though we took the utmost care, selection bias is still possible. We also understand the skepticism about the ability of behavioral modifications alone to reduce prolapsed hemorrhoids. While we do not claim complete reversal in all cases, our experience, supported by long-term follow-up data, indicates significant symptomatic improvement and avoidance of surgery in a substantial proportion of patients. However, we acknowledge the need for future studies to incorporate larger, stratified cohorts and ideally include randomized controlled trials for more definitive conclusions and to validate these findings. The main strength was that it represents the first review of conservative management for advanced-grade hemorrhoids and has promising results.\nTONEFACT is a promising and potentially transformative approach to treat early and advanced hemorrhoids without surgery. However, prospective randomized controlled trials are required to authenticate and validate the findings of TONEFACT treatment in patients with hemorrhoids.", "topic": "Brain"}
{"pmid": "38869373", "pmcid": "12309287", "title": "Biomechanical of bilateral heel rise, and its association with balance, functional mobility, and walking speed in older adults", "publication_year": "N/A", "abstract": "", "full_text": "\n\n\n\n\n\nThe strength, power, and mobility of the ankle-foot complex play a significant role in controlling postural balance, functional capacity, and the risk of falls in older adults (\nThe advancing age leads to a decrease in the strength and mobility of the ankle-foot, as shown, for example, by an annual rate of loss of ankle plantarflexor strength of approximately 2.3% in older adults (\nDuring HR, like walking, sufficient ankle and foot mobility, as well as increased midfoot stability, are required for adequate action of the ankle plantarflexors (\nA cross-sectional study was conducted, inviting all community-dwelling, self-sufficient adults aged 60 and above who participated in a multimodal program offered at various primary care centers across the city. A total of 86 individuals were assessed between December and January 2024, of whom sixty-nine completed all evaluations (\nThe sample size was calculated based on an estimated medium correlation size of 0.3, which is the ratio reported for associations between ankle and foot strength and functional tests in older adults (\nStudy participant flow chart.\nThe anthropometric assessment was carried out according to the standardized protocol of the International Society for the Advancement of Kinanthropometry (ISAK) (\nThen, the functional tests of the TUG, single leg stance (SLS), and walking speed (WS) were assessed. An obstacle-free corridor was used for the TUG test, with a cone placed 3 meters from a chair with a backrest and a height of 40 cm. Each participant was instructed to start from a seated position, stand up without assistance, walk three meters, turn 180°, return to the starting point, and sit down again. The test was performed twice, asking the participant to complete it in the shortest possible time. The time was recorded with a stopwatch, and the shortest time obtained from the two attempts was used for analysis (\nFor the bilateral HR test, a clinician-guided warm-up was previously performed for 5 minutes, which included general mobility and flexibility exercises for the ankle and foot, followed by 30 seated HR tests and familiarization with the standing HR test through three repetitions to ensure adequate test performance. Then, each participant stood barefoot, with feet parallel, on a force platform (Bertec, USA) in front of a wall. They were instructed to rise both heels “as high and as fast as possible”. The lowering of the heels was self-determined without receiving instructions for its control. During the execution of the test, each participant was instructed according to the procedure previously used for the bilateral heel rise (\nThe force platform was configured to obtain the vertical ground reaction force (vGRF) and CoP signal at 200 Hz. The signals were smoothed post hoc with a 2nd-order filter and a 10 Hz low-pass filter, and the vGRF was adjusted to each participant’s Bw. The signal onset was determined when the curve exceeded 3 SD of the pre-task resting signal average (\nGraphical summary of an example of the variables obtained during the Heel rise (HR) test. In the graph, the GRF is plotted on the vertical axis, and time in seconds is plotted on the horizontal axis. At the beginning of the recording, we observe the GRF line (in black) when the participant stands on the platform and registers their Bw. They then initiate the HR (point A: onset) by raising the heel (points ABC: rise phase) to the toe position (point C). They then lower the heel (points CD: drop phase) until the entire foot is in contact with the ground. The red line represents the displacement of the CoM, the blue line represents the CoM velocity, and the green and purple lines represent the displacements of the center of pressure in x and y, respectively. In addition, the impulse (shaded area under the curve at points AB and the projection of Bw) is shown in the GRF record.\nThe biomechanical parameter of peak force was determined from the top of the vGRF signal, and peak time was the difference between the onset and peak force time (\nData were tabulated and presented as means and standard deviations for each HR phase and functional test. Normality assumptions were checked using the Kolmogorov-Smirnov test (p > 0.05). Associations between functional tests and biomechanical parameters were examined using bivariate correlations with Spearman rank correlation coefficients (\nCharacteristics of the study sample participants. Mean, standard deviation (SD), and minimum-maximum values are presented\nAbbreviations: Y = years, HR = Heart rate, SBP = Sistolic blood pressure, DBP = Diastolic blood pressure, bpm = beat per minute, mmHg = millimeter of mercury, kg = Kilograms, cm = centimeters, kg/m\nThe HR correlations with the functional tests (\nOn the other hand, vertical stiffness showed a small to medium correlation in the rise phase (TUG:\nCorrelations of HR biomechanical parameters with balance tests (see also\nVertical stiffness showed a small correlation in the rise phase (SLS:\nThe present study aimed to describe the association between the biomechanical parameters of HR and single-leg stability, functional mobility, and walking speed in community-dwelling older adults. The main results show that greater peak force and shorter peak time in the rise phase during HR are more associated with better performance in walking and speed tests, such as the TUG and WS, than with balance (SLS). Additionally, a greater anteroposterior displacement of the CoP was associated with better performance in the drop phase of the three functional tests but not in the rise phase. Finally, greater vertical stiffness and velocity of the CoM were associated with better functional performance in the tests in both phases.\nPostural balance impairment in older adults has been frequently associated with greater oscillation of the CoP due to impaired postural control, which increases the risk of falls (\nSummary of biomechanical parameters of the heel rise and functional tests in older adults. Values obtained during the rise and drop phases and functional tests in older adults are presented. Values are presented as mean (standard deviation)\nAbbreviations: Fpeak: Force peak (maximum vertical ground reaction force during hell rise); Bw: body weight; ms: milliseconds; RMS: Root Mean Square; CoPx: center of pressure mediolateral direction; CoPx: center of pressure anteroposterior direction; m: meter; CoM: center of mass; m/s: meter/second; n.a.: not applicable; TUG: timed Up & Go test; SLS: single leg stance; WS: walking speed.\nCorrelations between biomechanical parameters of the heel rise and the performance of functional tests. Spearman rank correlation coefficient values and confidence Interval 95% are presented, and significant values are shown in bold\nAbbreviations: Fpeak: Force peak (maximum vertical ground reaction force during hell rise); Bw: body weight; ms: milliseconds; RMS: Root Mean Square; CoPx: center of pressure mediolateral direction; CoPx: center of pressure anteroposterior direction; m: meter; CoM: center of mass; m/s: meter/second; TUG: timed Up & Go test; SLS: single leg stance; WS: walking speed.\nIn functional transfer tasks, such as gait, the ankle-foot complex needs to be able to transition between flexible and stiff conditions in order to adequately act during the push-off and absorption of foot impact in the stance phase (\nRegarding the clinical implications of this study, the fast bilateral heel rise provides a compact window into several capacities—strength, power and balance control—that are important for everyday mobility in late life. Decades of work with the unilateral version already show that the number of pain-free rises an older person can complete mirrors their static and dynamic balance scores (\nMore recently, researchers have begun to connect those same metrics with downstream outcomes that physiotherapists care about. Slower gait speed, shorter strides, and longer double-support phases—kinematic fingerprints of fall-prone elders—co-vary with weaker or slower heel-rise performance, suggesting a shared physiological bottleneck in ankle power generation (\nTechnological tools have also taken on importance in these areas. Pressure mats in footwear, inertial units, and small force sensors attached to the heel can record elevation height, velocity, and even fatigue-induced asymmetries, turning a two-minute field test into a clinical-quality data stream without the need to strap the patient into bulky laboratory equipment (\nThis study is not without limitations. An important aspect was that ankle plantarflexor muscle strength and ankle joint range of motion were not measured, which would limit the interpretation of some functional performance results in older adults, which have been shown to be related to strength and mobility capacity. Studies analyzing biomechanical factors in this type of dynamic testing should consider strength and mobility capacities in older adults, as well as attempt to compare these parameters with younger or middle-aged individuals. Another limitation was that the sample of participants presented a significant difference in the number of women compared to older men. Although it has been reported that sex does not influence both the concentric and eccentric phases of the HR (\nOlder adults present biomechanical parameters of HR that are more closely associated with functional tests, such as the TUG and WS, than with balance tests (SLS). The fast HR showed associations with vGRF, anteroposterior displacement control, vertical stiffness, and vertical speed with tasks of greater dynamic demand, such as TUG and WS. Bilateral rapid heel rise and its biomechanical demands offer interesting insight into functional capabilities important for daily mobility in old age.", "content_for_embedding": "\n\n\n\n\n\nThe strength, power, and mobility of the ankle-foot complex play a significant role in controlling postural balance, functional capacity, and the risk of falls in older adults (\nThe advancing age leads to a decrease in the strength and mobility of the ankle-foot, as shown, for example, by an annual rate of loss of ankle plantarflexor strength of approximately 2.3% in older adults (\nDuring HR, like walking, sufficient ankle and foot mobility, as well as increased midfoot stability, are required for adequate action of the ankle plantarflexors (\nA cross-sectional study was conducted, inviting all community-dwelling, self-sufficient adults aged 60 and above who participated in a multimodal program offered at various primary care centers across the city. A total of 86 individuals were assessed between December and January 2024, of whom sixty-nine completed all evaluations (\nThe sample size was calculated based on an estimated medium correlation size of 0.3, which is the ratio reported for associations between ankle and foot strength and functional tests in older adults (\nStudy participant flow chart.\nThe anthropometric assessment was carried out according to the standardized protocol of the International Society for the Advancement of Kinanthropometry (ISAK) (\nThen, the functional tests of the TUG, single leg stance (SLS), and walking speed (WS) were assessed. An obstacle-free corridor was used for the TUG test, with a cone placed 3 meters from a chair with a backrest and a height of 40 cm. Each participant was instructed to start from a seated position, stand up without assistance, walk three meters, turn 180°, return to the starting point, and sit down again. The test was performed twice, asking the participant to complete it in the shortest possible time. The time was recorded with a stopwatch, and the shortest time obtained from the two attempts was used for analysis (\nFor the bilateral HR test, a clinician-guided warm-up was previously performed for 5 minutes, which included general mobility and flexibility exercises for the ankle and foot, followed by 30 seated HR tests and familiarization with the standing HR test through three repetitions to ensure adequate test performance. Then, each participant stood barefoot, with feet parallel, on a force platform (Bertec, USA) in front of a wall. They were instructed to rise both heels “as high and as fast as possible”. The lowering of the heels was self-determined without receiving instructions for its control. During the execution of the test, each participant was instructed according to the procedure previously used for the bilateral heel rise (\nThe force platform was configured to obtain the vertical ground reaction force (vGRF) and CoP signal at 200 Hz. The signals were smoothed post hoc with a 2nd-order filter and a 10 Hz low-pass filter, and the vGRF was adjusted to each participant’s Bw. The signal onset was determined when the curve exceeded 3 SD of the pre-task resting signal average (\nGraphical summary of an example of the variables obtained during the Heel rise (HR) test. In the graph, the GRF is plotted on the vertical axis, and time in seconds is plotted on the horizontal axis. At the beginning of the recording, we observe the GRF line (in black) when the participant stands on the platform and registers their Bw. They then initiate the HR (point A: onset) by raising the heel (points ABC: rise phase) to the toe position (point C). They then lower the heel (points CD: drop phase) until the entire foot is in contact with the ground. The red line represents the displacement of the CoM, the blue line represents the CoM velocity, and the green and purple lines represent the displacements of the center of pressure in x and y, respectively. In addition, the impulse (shaded area under the curve at points AB and the projection of Bw) is shown in the GRF record.\nThe biomechanical parameter of peak force was determined from the top of the vGRF signal, and peak time was the difference between the onset and peak force time (\nData were tabulated and presented as means and standard deviations for each HR phase and functional test. Normality assumptions were checked using the Kolmogorov-Smirnov test (p > 0.05). Associations between functional tests and biomechanical parameters were examined using bivariate correlations with Spearman rank correlation coefficients (\nCharacteristics of the study sample participants. Mean, standard deviation (SD), and minimum-maximum values are presented\nAbbreviations: Y = years, HR = Heart rate, SBP = Sistolic blood pressure, DBP = Diastolic blood pressure, bpm = beat per minute, mmHg = millimeter of mercury, kg = Kilograms, cm = centimeters, kg/m\nThe HR correlations with the functional tests (\nOn the other hand, vertical stiffness showed a small to medium correlation in the rise phase (TUG:\nCorrelations of HR biomechanical parameters with balance tests (see also\nVertical stiffness showed a small correlation in the rise phase (SLS:\nThe present study aimed to describe the association between the biomechanical parameters of HR and single-leg stability, functional mobility, and walking speed in community-dwelling older adults. The main results show that greater peak force and shorter peak time in the rise phase during HR are more associated with better performance in walking and speed tests, such as the TUG and WS, than with balance (SLS). Additionally, a greater anteroposterior displacement of the CoP was associated with better performance in the drop phase of the three functional tests but not in the rise phase. Finally, greater vertical stiffness and velocity of the CoM were associated with better functional performance in the tests in both phases.\nPostural balance impairment in older adults has been frequently associated with greater oscillation of the CoP due to impaired postural control, which increases the risk of falls (\nSummary of biomechanical parameters of the heel rise and functional tests in older adults. Values obtained during the rise and drop phases and functional tests in older adults are presented. Values are presented as mean (standard deviation)\nAbbreviations: Fpeak: Force peak (maximum vertical ground reaction force during hell rise); Bw: body weight; ms: milliseconds; RMS: Root Mean Square; CoPx: center of pressure mediolateral direction; CoPx: center of pressure anteroposterior direction; m: meter; CoM: center of mass; m/s: meter/second; n.a.: not applicable; TUG: timed Up & Go test; SLS: single leg stance; WS: walking speed.\nCorrelations between biomechanical parameters of the heel rise and the performance of functional tests. Spearman rank correlation coefficient values and confidence Interval 95% are presented, and significant values are shown in bold\nAbbreviations: Fpeak: Force peak (maximum vertical ground reaction force during hell rise); Bw: body weight; ms: milliseconds; RMS: Root Mean Square; CoPx: center of pressure mediolateral direction; CoPx: center of pressure anteroposterior direction; m: meter; CoM: center of mass; m/s: meter/second; TUG: timed Up & Go test; SLS: single leg stance; WS: walking speed.\nIn functional transfer tasks, such as gait, the ankle-foot complex needs to be able to transition between flexible and stiff conditions in order to adequately act during the push-off and absorption of foot impact in the stance phase (\nRegarding the clinical implications of this study, the fast bilateral heel rise provides a compact window into several capacities—strength, power and balance control—that are important for everyday mobility in late life. Decades of work with the unilateral version already show that the number of pain-free rises an older person can complete mirrors their static and dynamic balance scores (\nMore recently, researchers have begun to connect those same metrics with downstream outcomes that physiotherapists care about. Slower gait speed, shorter strides, and longer double-support phases—kinematic fingerprints of fall-prone elders—co-vary with weaker or slower heel-rise performance, suggesting a shared physiological bottleneck in ankle power generation (\nTechnological tools have also taken on importance in these areas. Pressure mats in footwear, inertial units, and small force sensors attached to the heel can record elevation height, velocity, and even fatigue-induced asymmetries, turning a two-minute field test into a clinical-quality data stream without the need to strap the patient into bulky laboratory equipment (\nThis study is not without limitations. An important aspect was that ankle plantarflexor muscle strength and ankle joint range of motion were not measured, which would limit the interpretation of some functional performance results in older adults, which have been shown to be related to strength and mobility capacity. Studies analyzing biomechanical factors in this type of dynamic testing should consider strength and mobility capacities in older adults, as well as attempt to compare these parameters with younger or middle-aged individuals. Another limitation was that the sample of participants presented a significant difference in the number of women compared to older men. Although it has been reported that sex does not influence both the concentric and eccentric phases of the HR (\nOlder adults present biomechanical parameters of HR that are more closely associated with functional tests, such as the TUG and WS, than with balance tests (SLS). The fast HR showed associations with vGRF, anteroposterior displacement control, vertical stiffness, and vertical speed with tasks of greater dynamic demand, such as TUG and WS. Bilateral rapid heel rise and its biomechanical demands offer interesting insight into functional capabilities important for daily mobility in old age.", "topic": "Brain"}
{"pmid": "38820560", "pmcid": "12307874", "title": "Advanced Design for High-Performance and AI Chips", "publication_year": "N/A", "abstract": "\nRecent years have witnessed transformative changes brought about by artificial intelligence (AI) techniques with billions of parameters for the realization of high accuracy, proposing high demand for the advanced and AI chip to solve these AI tasks efficiently and powerfully. Rapid progress has been made in the field of advanced chips recently, such as the development of photonic computing, the advancement of the quantum processors, the boost of the biomimetic chips, and so on. Designs tactics of the advanced chips can be conducted with elaborated consideration of materials, algorithms, models, architectures, and so on. Though a few reviews present the development of the chips from their unique aspects, reviews in the view of the latest design for advanced and AI chips are few. Here, the newest development is systematically reviewed in the field of advanced chips. First, background and mechanisms are summarized, and subsequently most important considerations for co-design of the software and hardware are illustrated. Next, strategies are summed up to obtain advanced and AI chips with high excellent performance by taking the important information processing steps into consideration, after which the design thought for the advanced chips in the future is proposed. Finally, some perspectives are put forward.", "full_text": "The past decade has witnessed the rapid progress of artificial intelligence (AI) techniques, which has revolutionized a wide range of fields, including the way to interpret information, the approach to discovery new materials, the method for creative work, and so on [\nMany endeavors have been made to meet the challenges proposed by the AI tasks, with a lot of achievements and techniques emerging as the most promising approaches to address these issues [\nSignificant progress has been made in both the hardware and the software of the advanced chips recently, which favors the fabrication of the chips. It is proposed that the fabrication of the chip bears some analogy to the construction of buildings. The fabricated chips can then be applied to handle various information to realize complexed and AI tasks, including computer vision, speech recognition and transcription, parallel imaging and all-optical classification, patients’ gaits classification, and other various fields, with Internet of Things (IoT), smart travel, smart robot, and smart home included (Fig.\nOverview of the advanced and AI chip. The design for the software and hardware favors the fabrication of the chips, which bears some analogy to the construction of buildings. The fabricated chips can then be applied to handle various information to realize complexed and AI tasks\nPublication and the citation frequency of the papers concerning about the AI chip. The data are collected from web of science with “AI chip” or “advanced chip” as topic words, and are also filtered according to the actual relevance of the topic\nDesign strategies about the advanced chips. Design strategies carried for\nIn this review, the basic background of AI chips was introduced first, as well as their working mechanisms, after which the design ideas in regard to software and hardware from the aspects of both the technique development for the conventional silicon-based chips, and the adoption of novel modes that extend the information processing from electrons, to photons, quantum, and biological elements, were demonstrated. Key factors which should be under consideration when designing the advanced chips were discussed from the view of the information processing procedures. Last but not least, we put forward some ideas with respect to the outlook of the advanced chips.\nThe chips are applied to deal with various information and data. For instance, data can be collected from multimodal sensors. As for a typical task, the information is first captured by the sensors and is then digitized by a large number of analog-to-digital converters (ADCs) [\nSchematic illustration for the working mechanism of the advanced chips. Schematic illustration for the stage of\nThe neuromorphic hardware learning from the information processing of human brain is a promising candidate for next-generation computer architectures because of its massive parallelism, robust fault tolerance, and high efficiency, which is different to the conventional architecture. The exploiting of the neuromorphic computing systems makes it possible to implement the parallel processing, which enables the execution of separate complex tasks by making use of several processors simultaneously, leading to the enhanced processing efficiency [\nInformation is expected to be processed by the chips as the way of human brain, including learning, reasoning, and memorizing [\nHigh capacity and high-throughput computing architectures are then required to handle the complex multimodality information collected from the environment [\nAI relies on hardware and software to simulate human intelligence, and it is critical to carry out the co-design of both the software and the hardware for the advanced and AI chips. Specifically, software programming is of importance for the construction and training of  NN, while hardware is crucial to process and handle the data for AI operation [\nAI algorithms have been evolved rapidly. The intricate cognitive capabilities achieved by the human brain have sparked extensive research in AI with the promotion of sophisticated brain-inspired algorithms. It is worthwhile mentioning that the device-algorithm co-optimizations need to be carried out for the real-world application. Particularly, the software toolchain with data management, model simulation, and host management included is beneficial to deploy the algorithms and models efficiently for various applications [\nThe design of the software plays a crucial role in achieving various advantages of the advanced chips by working together with the hardware [\nSchematic of how software designs facilitate the development of advanced chips.\nSome challenges brought by the explosive growth of the AI can be met by the design of algorithm, like the issue that multiple types of data are needed to be handled along with the boost development of the artificial intelligence generated content (AIGC) [\nHardware design is imperative for promoting the development of different types of chips, the reason that it can solve the problems of different chips, making full use of these chips in various fields. To be specific, memristor, which can simulate the plasticity of biological synapses, plays a critical role in the brain-inspired computing. Photonic computing is featured with ultra high-speed, while it is also encountered with the problem of poor compatibility with silicon-based electronic chip. The computing power of quantum computing to deal with specific problems far exceeds that of classical computers, but the extremely low-temperature requirement is usually a challenge. Neuromorphic computing is managed to mimic the structure of human brain, and it can realize event-driven computing by means of asynchronous SNN, which is qualified for real-time perception and IoT. Accordingly, new circuit layout or material structure design is carried out to meet these challenges.\nThe development of materials is served as one of the most important supports for the thriving chip industry. For instance, CIM-based hardware systems are designed according to the requirements from AI algorithm to accelerate the extensive computations by means of eliminating frequent data transfers between memory and processing units [\nMuch efforts have been made on mapping the biological behavior in the nervous system to the electrical behavior in various devices, and many techniques have been emerged as the most promising approaches to meet the challenges brought by the AI tasks. It turns out that excessive energy consumption occurs with a significant amount of data moving between memory and processor, which is known as the von Neumann bottleneck [\nAlthough the rapid progress has been made in CIM technology, it is crucial to recognize that the majority of the non-linear computations for the results after linear matrix–vector multiplying relies on conventional complementary metal oxide semiconductor (CMOS) circuits, with ADCs and digital circuits for complex arithmetic included, leading to excessive area and energy costs [\nSchematic of how hardware design promotes the development of different types of chips.\nIn addition to the imitating of the essential synaptic functions, the in-depth study of the underlying learning and memory mechanisms in the biological brain is also vital for the realization of intelligent information processing at the hardware level [\nIn the post-Moore era, greater challenges have been proposed for the continuous demand of higher performance [\nIn addition to the neuromorphic computing and photonic computing, quantum computing has been emerged as another advanced type of computing [\nBesides the new materials and novel modes for the development of the advanced chips, progress has also been made in the aspect of integrating technique [\nThe complexed and comprehensive simulations about the functions of the biological learning and memory are expected to be accomplished by the artificial neuromorphic devices [\nSchematic for the design strategies of AI chips in regard to data memory and transfer.\nThe issue of data transfer limit for high-performance silicon chips has drawn a lot of attention, for which several schemes have been proposed [\nIn an attempt to offer new dimensions of data transfer with the aim of fulfilling the growing need for speed, an integrated multi-dimensional system that integrated wavelength and mode multiplexing on a silicon photonic circuit for the on-chip and chip-to-chip interconnects was put forward [\nDynamic computing is a promising approach in DL, and the dynamic neural networks are managed to adapt the computational graphs to the input in the inference stage, showing the attractive properties in many aspects [\nSchematic for the design strategies of AI chips in regard to computing. Comparison between\nThe energy constraints become a major restriction to deploy traditional AI methods, and therefore high demand for the energy efficiency has also been proposed for the computing. Correspondingly, much efforts have been made to come up with the schemes for energy-efficient computing. For example, better energy efficiency can be offered by analog in-memory computing (analog-AI) as it can perform matrix–vector multiplications (MVM) in parallel on ‘memory tiles’ [\nIn addition to the requirement from dynamic computing and energy constraints, high demand has also been put forward for the weight-reconfigurable capacity of computing for some fields, like the healthcare monitoring, on which occasion it is essential to finely reconfigure the relative intensity of weight from each input. In an attempt to achieve the precise and independent modification of each input, a neuromorphic computing system that was managed to integrate two different environmental information with reconfigurable weights by making use of a simple circuitry based on electrochemical artificial synapses was designed [\nA sharply increased calculations have been brought about with the development of AI technology [\nThe computing speed should be further accelerated to cooperate with the improved performance of various tasks at the algorithmic level [\nDesign considerations of AI chips for high-performance computing. The workflow of\nAnother challenge met by the optical computing is that they are implemented in silico on electronic computers, and therefore both strict modeling and large amounts of training data are essential (Fig.\nIn addition to the method mentioned above, parallel multi-thread processing is also one of the key approaches to achieve high-speed and high-capacity signal processing, which is a promising way to meet the increasing demand for high-capacity datasets processing [\nIn addition to the enhanced computing performance, the high energy efficiency is another important requirement for the advanced chips. For example, in regard to many vision tasks, the ADCs with high throughput and high precision reduce the imaging frame rate on account of limited data bandwidth, causing remarkable energy consumption [\nThe vast amounts of data transferred between memory and processor lead to the unessential energy consumption. Both the time and the energy are expected to be saved by the Analog-AI hardware with the function to apply arrays of non-volatile memory (NVM) to execute the MAC operations. One case in point was that an analog-AI chip was designed to recognize and transcript speech energy efficiently. It was noticeable that not only the fully end-to-end SW\nDesign considerations of AI chips with improved energy efficiency.\nTo reach the goal of energy efficiency, the composition of different power consumption should be taken into considerations. The power that is required to operate an AI system is usually composed of two aspects, resting power which is determined by the hardware design, and running power which relies on the model as the hardware is fixed [\nIn contrast to the most common neuromorphic hardware design which begins with the bottom of the compute stack, elaborated design can be conducted for the customization of the neuromorphic hardware which is to be applied at the edge for the specific purposes with low power consumption taken into consideration. One case in point was that a sensing-computing neuromorphic chip, Speck, was designed with a 128 × 128-pixel DVS integrated onto an asynchronous spike-based AI chip, which is shown in Fig.\nOverall, the recent development, including but not limited to the co-design strategies for the software and hardware, the realization of enhanced overall performance, and the potential for broader application have been reviewed in depth. Great progress has been made in the field of advanced chips due to the high challenges brought by AI, which has revolutionized various aspects, ranging from information industry to material science. To execute the complex algorithmic programs and advanced tasks proposed by these new challenges, the elaborate design of chips covers every aspect, including materials, algorithm, architectures, processing technology, integrating method, and so on. Progress has been made on developing novel materials and models, as well as overcoming the shortcomings of the existing conventional materials and architectures for chips. New fabrication processes for both the production and the package of the devices have been developed, aiming to induce the cost and develop complex chips. The advanced chips are qualified to be applied for video recognition tasks, speech recognition and transcription, visual memory and many other fields, offering fast and efficient information processing functions (Fig.\nOutlook of the advanced chips\nSummary for the state-of-the-art advanced and AI chips is illustrated in Table\nSummary for the performance of the state-of-the-art advanced chips\nSignificant improvements of the advanced chips have happened and accompanied by the discovery of novel modes, the improvement of the package techniques, the accelerating of the efficiency, as well as the enhancement of computing power. This review offers a keen insight into the design strategies for the advanced and AI chips, with some perspectives for the chips applied in the future proposed as follows:\nEndeavors have been made to equip the AI chips with more intelligent performance learning from biology. a) Efforts have been made focused on mapping the biological behavior to the electrical behavior in devices. It is expected for the systems to realize more complex biological performances. The associative learning behavior, which is commonly found in the cranial nerves of insects and is featured with the acquisition, extinction, restoration, and generalization, has been simulated by ZnO QDs‑based optoelectronic memristors, which provide novel scheme for the field of machine self-learning. It is desirable to develop chips learning from more advanced behaviors of the creatures. b) Extensive investigations have been carried out on neuromorphic devices based on the human brain, which is a potential candidate for the next-generation computer architecture. The method of how to learn from the high-level brain dynamic mechanisms to equip neuromorphic computing with more energy advantages is always in high demand. Endeavors have been made from both the software and the hardware aspects to address this issue. Moreover, chips used for dealing with image information are expected to be managed to handle the dynamic, diverse, and unpredictable scenes in real application scenarios, like autonomous driving. It is desirable to design the chips that are efficient in various fields to percept and address even the difficult issues existing in the real world. In particular, the dynamic computing, which is a critical feature of human brain, has been simulated by this system. In the future, more advanced strategies can be adopted for the realization of high-level brain dynamic mechanisms to fully achieve the brain advantages in many aspects.\nEfforts can be made to make full use of the novel modes that extend the information processing from electrons, to photons, quantum, and biological elements, by taking advantages of the strengths and overcoming their weaknesses. a) Photonics-based systems are managed to provide high-speed computing units, and therefore efforts have been made focused on the algorithms design to exploit their unique advantages. For instance, approaches have been developed to realize the high throughput and precision by the successful application of cellular automata [\nThe advanced chips that are qualified for real-world applications are always in high demand. Multi-input signals are usually needed to be processed properly by the advanced processors suitable for diverse external information in the open-world applications. The integrated signals from different input are needed to be handled accurately and timely. The version of GPT-4 has successfully accomplished the processing of multimodal data, like images and audio. A neuromorphic computing system applied for the risk assessment has been developed with several kinds of factors taking into considerations. In the future work, more work focused in the development of algorithms and hardware tailored for open-world applications can be conducted. The overall performances are expected to be enhanced for the chips to meet the high requirement proposed by the real-world applications.\nThe reconfigurable behavior is an important aim for computing hardware. For the chips with reconfigurable capacities, their function can be changed even after the accomplishment of the fabrication, and therefore multi-modal data and different tasks can be dealt with, making the high flexibility in adapting to different tasks feasible. It is especially critical to the chips used for some specific purposes like healthcare monitoring, for which it is imperative to finely reconfigure the relative intensity of weight updates from each input. Explorations have been made to equip different types of chips with strong reconfigurability. The reconfigurability and multimodal capability have been achieved for a TDONN chip by taking advantages of on-chip diffractive optics with massive tunable elements. The reconfigurability has also been available for the diffractive-interference hybrid photonic chiplet, which is acted as the fundamental building block for a diversity of advanced ML tasks, with 1000-category classification and content generation included. An all-analog chip combining electronic and light computing (ACCEL) is also equipped with the reconfigurability for different tasks without changing the OAC module. The integration of two different information with reconfigurable weights has been accomplished by a neuromorphic computing system. In the future, the high degree of adaptability to different assignments empowered by reconfiguration is expected to be accessible for more chiplet when it is necessary.\nMore explorations on large-scale integrations are expected to be made for chips. With the increasing of information, chips are required to be integrated to an ever-growing level to process the booming signals. The large-scale integrations of various chips are indispensable to getting rid of the shortcomings of each chip. For inorganic counterparts, like CMOS chips, an integration level in ultra-large-scale has been realized, while poor mechanical compatibility with organisms exists. It is ideal for the devices to overcome inherent shortcomings and accomplish the large-scale integration. Moreover, the integrations are closely related to the technologies. A diversity of techniques like photolithography, screening, printing, and shadow-mask evaporation has been developed. In the future, the continuous progress of the techniques is expected to be made in order to miniaturize these devices.\nThe application of sustainable materials in AI chips is one of the most important trends in this field with the aim of reducing the environmental impact and improving energy efficiency. Efforts can be made from various aspects, such as selecting degradable substrates, developing environmentally friendly manufacturing process, preparing environmentally friendly heat dissipation materials, and so on. Some bio-elastomers with active-controllable degradation rates have been designed, which can be applied as the bio-electronic substrates and encapsulation layers. In the future, more endeavors can be made to make a balance between meeting the high-performance requirements of AI chips and controlling the costs when using sustainable materials.", "content_for_embedding": "The past decade has witnessed the rapid progress of artificial intelligence (AI) techniques, which has revolutionized a wide range of fields, including the way to interpret information, the approach to discovery new materials, the method for creative work, and so on [\nMany endeavors have been made to meet the challenges proposed by the AI tasks, with a lot of achievements and techniques emerging as the most promising approaches to address these issues [\nSignificant progress has been made in both the hardware and the software of the advanced chips recently, which favors the fabrication of the chips. It is proposed that the fabrication of the chip bears some analogy to the construction of buildings. The fabricated chips can then be applied to handle various information to realize complexed and AI tasks, including computer vision, speech recognition and transcription, parallel imaging and all-optical classification, patients’ gaits classification, and other various fields, with Internet of Things (IoT), smart travel, smart robot, and smart home included (Fig.\nOverview of the advanced and AI chip. The design for the software and hardware favors the fabrication of the chips, which bears some analogy to the construction of buildings. The fabricated chips can then be applied to handle various information to realize complexed and AI tasks\nPublication and the citation frequency of the papers concerning about the AI chip. The data are collected from web of science with “AI chip” or “advanced chip” as topic words, and are also filtered according to the actual relevance of the topic\nDesign strategies about the advanced chips. Design strategies carried for\nIn this review, the basic background of AI chips was introduced first, as well as their working mechanisms, after which the design ideas in regard to software and hardware from the aspects of both the technique development for the conventional silicon-based chips, and the adoption of novel modes that extend the information processing from electrons, to photons, quantum, and biological elements, were demonstrated. Key factors which should be under consideration when designing the advanced chips were discussed from the view of the information processing procedures. Last but not least, we put forward some ideas with respect to the outlook of the advanced chips.\nThe chips are applied to deal with various information and data. For instance, data can be collected from multimodal sensors. As for a typical task, the information is first captured by the sensors and is then digitized by a large number of analog-to-digital converters (ADCs) [\nSchematic illustration for the working mechanism of the advanced chips. Schematic illustration for the stage of\nThe neuromorphic hardware learning from the information processing of human brain is a promising candidate for next-generation computer architectures because of its massive parallelism, robust fault tolerance, and high efficiency, which is different to the conventional architecture. The exploiting of the neuromorphic computing systems makes it possible to implement the parallel processing, which enables the execution of separate complex tasks by making use of several processors simultaneously, leading to the enhanced processing efficiency [\nInformation is expected to be processed by the chips as the way of human brain, including learning, reasoning, and memorizing [\nHigh capacity and high-throughput computing architectures are then required to handle the complex multimodality information collected from the environment [\nAI relies on hardware and software to simulate human intelligence, and it is critical to carry out the co-design of both the software and the hardware for the advanced and AI chips. Specifically, software programming is of importance for the construction and training of  NN, while hardware is crucial to process and handle the data for AI operation [\nAI algorithms have been evolved rapidly. The intricate cognitive capabilities achieved by the human brain have sparked extensive research in AI with the promotion of sophisticated brain-inspired algorithms. It is worthwhile mentioning that the device-algorithm co-optimizations need to be carried out for the real-world application. Particularly, the software toolchain with data management, model simulation, and host management included is beneficial to deploy the algorithms and models efficiently for various applications [\nThe design of the software plays a crucial role in achieving various advantages of the advanced chips by working together with the hardware [\nSchematic of how software designs facilitate the development of advanced chips.\nSome challenges brought by the explosive growth of the AI can be met by the design of algorithm, like the issue that multiple types of data are needed to be handled along with the boost development of the artificial intelligence generated content (AIGC) [\nHardware design is imperative for promoting the development of different types of chips, the reason that it can solve the problems of different chips, making full use of these chips in various fields. To be specific, memristor, which can simulate the plasticity of biological synapses, plays a critical role in the brain-inspired computing. Photonic computing is featured with ultra high-speed, while it is also encountered with the problem of poor compatibility with silicon-based electronic chip. The computing power of quantum computing to deal with specific problems far exceeds that of classical computers, but the extremely low-temperature requirement is usually a challenge. Neuromorphic computing is managed to mimic the structure of human brain, and it can realize event-driven computing by means of asynchronous SNN, which is qualified for real-time perception and IoT. Accordingly, new circuit layout or material structure design is carried out to meet these challenges.\nThe development of materials is served as one of the most important supports for the thriving chip industry. For instance, CIM-based hardware systems are designed according to the requirements from AI algorithm to accelerate the extensive computations by means of eliminating frequent data transfers between memory and processing units [\nMuch efforts have been made on mapping the biological behavior in the nervous system to the electrical behavior in various devices, and many techniques have been emerged as the most promising approaches to meet the challenges brought by the AI tasks. It turns out that excessive energy consumption occurs with a significant amount of data moving between memory and processor, which is known as the von Neumann bottleneck [\nAlthough the rapid progress has been made in CIM technology, it is crucial to recognize that the majority of the non-linear computations for the results after linear matrix–vector multiplying relies on conventional complementary metal oxide semiconductor (CMOS) circuits, with ADCs and digital circuits for complex arithmetic included, leading to excessive area and energy costs [\nSchematic of how hardware design promotes the development of different types of chips.\nIn addition to the imitating of the essential synaptic functions, the in-depth study of the underlying learning and memory mechanisms in the biological brain is also vital for the realization of intelligent information processing at the hardware level [\nIn the post-Moore era, greater challenges have been proposed for the continuous demand of higher performance [\nIn addition to the neuromorphic computing and photonic computing, quantum computing has been emerged as another advanced type of computing [\nBesides the new materials and novel modes for the development of the advanced chips, progress has also been made in the aspect of integrating technique [\nThe complexed and comprehensive simulations about the functions of the biological learning and memory are expected to be accomplished by the artificial neuromorphic devices [\nSchematic for the design strategies of AI chips in regard to data memory and transfer.\nThe issue of data transfer limit for high-performance silicon chips has drawn a lot of attention, for which several schemes have been proposed [\nIn an attempt to offer new dimensions of data transfer with the aim of fulfilling the growing need for speed, an integrated multi-dimensional system that integrated wavelength and mode multiplexing on a silicon photonic circuit for the on-chip and chip-to-chip interconnects was put forward [\nDynamic computing is a promising approach in DL, and the dynamic neural networks are managed to adapt the computational graphs to the input in the inference stage, showing the attractive properties in many aspects [\nSchematic for the design strategies of AI chips in regard to computing. Comparison between\nThe energy constraints become a major restriction to deploy traditional AI methods, and therefore high demand for the energy efficiency has also been proposed for the computing. Correspondingly, much efforts have been made to come up with the schemes for energy-efficient computing. For example, better energy efficiency can be offered by analog in-memory computing (analog-AI) as it can perform matrix–vector multiplications (MVM) in parallel on ‘memory tiles’ [\nIn addition to the requirement from dynamic computing and energy constraints, high demand has also been put forward for the weight-reconfigurable capacity of computing for some fields, like the healthcare monitoring, on which occasion it is essential to finely reconfigure the relative intensity of weight from each input. In an attempt to achieve the precise and independent modification of each input, a neuromorphic computing system that was managed to integrate two different environmental information with reconfigurable weights by making use of a simple circuitry based on electrochemical artificial synapses was designed [\nA sharply increased calculations have been brought about with the development of AI technology [\nThe computing speed should be further accelerated to cooperate with the improved performance of various tasks at the algorithmic level [\nDesign considerations of AI chips for high-performance computing. The workflow of\nAnother challenge met by the optical computing is that they are implemented in silico on electronic computers, and therefore both strict modeling and large amounts of training data are essential (Fig.\nIn addition to the method mentioned above, parallel multi-thread processing is also one of the key approaches to achieve high-speed and high-capacity signal processing, which is a promising way to meet the increasing demand for high-capacity datasets processing [\nIn addition to the enhanced computing performance, the high energy efficiency is another important requirement for the advanced chips. For example, in regard to many vision tasks, the ADCs with high throughput and high precision reduce the imaging frame rate on account of limited data bandwidth, causing remarkable energy consumption [\nThe vast amounts of data transferred between memory and processor lead to the unessential energy consumption. Both the time and the energy are expected to be saved by the Analog-AI hardware with the function to apply arrays of non-volatile memory (NVM) to execute the MAC operations. One case in point was that an analog-AI chip was designed to recognize and transcript speech energy efficiently. It was noticeable that not only the fully end-to-end SW\nDesign considerations of AI chips with improved energy efficiency.\nTo reach the goal of energy efficiency, the composition of different power consumption should be taken into considerations. The power that is required to operate an AI system is usually composed of two aspects, resting power which is determined by the hardware design, and running power which relies on the model as the hardware is fixed [\nIn contrast to the most common neuromorphic hardware design which begins with the bottom of the compute stack, elaborated design can be conducted for the customization of the neuromorphic hardware which is to be applied at the edge for the specific purposes with low power consumption taken into consideration. One case in point was that a sensing-computing neuromorphic chip, Speck, was designed with a 128 × 128-pixel DVS integrated onto an asynchronous spike-based AI chip, which is shown in Fig.\nOverall, the recent development, including but not limited to the co-design strategies for the software and hardware, the realization of enhanced overall performance, and the potential for broader application have been reviewed in depth. Great progress has been made in the field of advanced chips due to the high challenges brought by AI, which has revolutionized various aspects, ranging from information industry to material science. To execute the complex algorithmic programs and advanced tasks proposed by these new challenges, the elaborate design of chips covers every aspect, including materials, algorithm, architectures, processing technology, integrating method, and so on. Progress has been made on developing novel materials and models, as well as overcoming the shortcomings of the existing conventional materials and architectures for chips. New fabrication processes for both the production and the package of the devices have been developed, aiming to induce the cost and develop complex chips. The advanced chips are qualified to be applied for video recognition tasks, speech recognition and transcription, visual memory and many other fields, offering fast and efficient information processing functions (Fig.\nOutlook of the advanced chips\nSummary for the state-of-the-art advanced and AI chips is illustrated in Table\nSummary for the performance of the state-of-the-art advanced chips\nSignificant improvements of the advanced chips have happened and accompanied by the discovery of novel modes, the improvement of the package techniques, the accelerating of the efficiency, as well as the enhancement of computing power. This review offers a keen insight into the design strategies for the advanced and AI chips, with some perspectives for the chips applied in the future proposed as follows:\nEndeavors have been made to equip the AI chips with more intelligent performance learning from biology. a) Efforts have been made focused on mapping the biological behavior to the electrical behavior in devices. It is expected for the systems to realize more complex biological performances. The associative learning behavior, which is commonly found in the cranial nerves of insects and is featured with the acquisition, extinction, restoration, and generalization, has been simulated by ZnO QDs‑based optoelectronic memristors, which provide novel scheme for the field of machine self-learning. It is desirable to develop chips learning from more advanced behaviors of the creatures. b) Extensive investigations have been carried out on neuromorphic devices based on the human brain, which is a potential candidate for the next-generation computer architecture. The method of how to learn from the high-level brain dynamic mechanisms to equip neuromorphic computing with more energy advantages is always in high demand. Endeavors have been made from both the software and the hardware aspects to address this issue. Moreover, chips used for dealing with image information are expected to be managed to handle the dynamic, diverse, and unpredictable scenes in real application scenarios, like autonomous driving. It is desirable to design the chips that are efficient in various fields to percept and address even the difficult issues existing in the real world. In particular, the dynamic computing, which is a critical feature of human brain, has been simulated by this system. In the future, more advanced strategies can be adopted for the realization of high-level brain dynamic mechanisms to fully achieve the brain advantages in many aspects.\nEfforts can be made to make full use of the novel modes that extend the information processing from electrons, to photons, quantum, and biological elements, by taking advantages of the strengths and overcoming their weaknesses. a) Photonics-based systems are managed to provide high-speed computing units, and therefore efforts have been made focused on the algorithms design to exploit their unique advantages. For instance, approaches have been developed to realize the high throughput and precision by the successful application of cellular automata [\nThe advanced chips that are qualified for real-world applications are always in high demand. Multi-input signals are usually needed to be processed properly by the advanced processors suitable for diverse external information in the open-world applications. The integrated signals from different input are needed to be handled accurately and timely. The version of GPT-4 has successfully accomplished the processing of multimodal data, like images and audio. A neuromorphic computing system applied for the risk assessment has been developed with several kinds of factors taking into considerations. In the future work, more work focused in the development of algorithms and hardware tailored for open-world applications can be conducted. The overall performances are expected to be enhanced for the chips to meet the high requirement proposed by the real-world applications.\nThe reconfigurable behavior is an important aim for computing hardware. For the chips with reconfigurable capacities, their function can be changed even after the accomplishment of the fabrication, and therefore multi-modal data and different tasks can be dealt with, making the high flexibility in adapting to different tasks feasible. It is especially critical to the chips used for some specific purposes like healthcare monitoring, for which it is imperative to finely reconfigure the relative intensity of weight updates from each input. Explorations have been made to equip different types of chips with strong reconfigurability. The reconfigurability and multimodal capability have been achieved for a TDONN chip by taking advantages of on-chip diffractive optics with massive tunable elements. The reconfigurability has also been available for the diffractive-interference hybrid photonic chiplet, which is acted as the fundamental building block for a diversity of advanced ML tasks, with 1000-category classification and content generation included. An all-analog chip combining electronic and light computing (ACCEL) is also equipped with the reconfigurability for different tasks without changing the OAC module. The integration of two different information with reconfigurable weights has been accomplished by a neuromorphic computing system. In the future, the high degree of adaptability to different assignments empowered by reconfiguration is expected to be accessible for more chiplet when it is necessary.\nMore explorations on large-scale integrations are expected to be made for chips. With the increasing of information, chips are required to be integrated to an ever-growing level to process the booming signals. The large-scale integrations of various chips are indispensable to getting rid of the shortcomings of each chip. For inorganic counterparts, like CMOS chips, an integration level in ultra-large-scale has been realized, while poor mechanical compatibility with organisms exists. It is ideal for the devices to overcome inherent shortcomings and accomplish the large-scale integration. Moreover, the integrations are closely related to the technologies. A diversity of techniques like photolithography, screening, printing, and shadow-mask evaporation has been developed. In the future, the continuous progress of the techniques is expected to be made in order to miniaturize these devices.\nThe application of sustainable materials in AI chips is one of the most important trends in this field with the aim of reducing the environmental impact and improving energy efficiency. Efforts can be made from various aspects, such as selecting degradable substrates, developing environmentally friendly manufacturing process, preparing environmentally friendly heat dissipation materials, and so on. Some bio-elastomers with active-controllable degradation rates have been designed, which can be applied as the bio-electronic substrates and encapsulation layers. In the future, more endeavors can be made to make a balance between meeting the high-performance requirements of AI chips and controlling the costs when using sustainable materials.", "topic": "Brain"}
{"pmid": "38768971", "pmcid": "12304951", "title": "Artificial intelligence in traumatic brain injury: Brain imaging analysis and outcome prediction: A mini review", "publication_year": "N/A", "abstract": "Integration of artificial intelligence increases in all aspects of human life, particularly in healthcare systems. Traumatic brain injury is a significant cause of mortality and long-term disability, with an important impact on the socio-economic system of healthcare. The role of artificial intelligence in imaging and outcome prediction for traumatic brain injury patients is reviewed with a particular emphasis to the characteristics of machine and deep learning methods. Evidence of potential improvement in the clinical practice in discussed.", "full_text": "", "content_for_embedding": "Integration of artificial intelligence increases in all aspects of human life, particularly in healthcare systems. Traumatic brain injury is a significant cause of mortality and long-term disability, with an important impact on the socio-economic system of healthcare. The role of artificial intelligence in imaging and outcome prediction for traumatic brain injury patients is reviewed with a particular emphasis to the characteristics of machine and deep learning methods. Evidence of potential improvement in the clinical practice in discussed.", "topic": "Brain"}
{"pmid": "38710025", "pmcid": "12307760", "title": "Time to death and its predictors among neonates with seizure in North West Ethiopia", "publication_year": "N/A", "abstract": "Neonatal seizures were associated with significant rates of mortality; in which about one-third of the neonates with seizure ending up with death. Despite this, the time to death and its predictors among neonates with seizure has not been investigated; especially in Ethiopia. To determine the time to death and its predictors among neonates with seizure in public hospitals of Awi zone, Northwest Ethiopia. A multicenter prospective follow-up study was conducted in public hospitals of Awi zone on 263 neonates with seizure. Descriptive statistics, Kaplan–Meier curve, Nelson–Aalen curves, and log-rank tests were employed to describe the time to death and to assess the risk of mortality among different covariates. The Cox proportional hazards model was used to identify the predictors of time to death. AHR with 95% CI was used to identify significant predictor variables, and a statistical significance was declared at", "full_text": "A neonatal seizure can be defined as an emergency medical condition that occurs due to sudden, transient, and excessive neuronal activity in the brain of a neonate; leading to electroencephalographic abnormalities and/or overt clinical features like change in behavior, motor, or autonomic function\nNeonatal seizures are commonly caused by hypoxic-ischemic encephalopathy, infections of the central nervous system, acute metabolic disturbances, and intracranial hemorrhages\nNeonatal seizures are major causes of morbidity in infants. Full recovery from neonatal seizure had been reported only in 25–40% of the affected neonates\nLiteratures had been indicated that most seizure-related neonatal deaths occur early during the acute illness\nThis study was conducted in Awi zone, Amhara region, Ethiopia; which is about 426 km away from Addis Ababa, the capital city of Ethiopia. Based on the 2021 census, this zone had a total population of 1,342,324; of whom 51% were females. The zone has a total of 52 health institutions; of which 47 are health centers, 4 are primary hospitals, and 1 is a general hospital. All of the hospitals (Injibara General Hospital, Agew Gimjabet Primary Hospital, Dangila Primary Hospital, Changi Primary Hospital, and Jawi Primary Hospital) provide neonatal admission services. Therefore, this multicenter prospective follow-up study was conducted in those hospitals from January 1-2023 to December 31-2023.\nAll neonates who were admitted to the NICUs of Awi Zone Public Hospitals with a diagnosed neonatal seizure were the source population, and all neonates who were admitted to those institutions with a diagnosed of neonatal seizure from January 1-2023 to December 31-2023 were the study population. However, neonates with complex malformations and neonates who were referred from Awi zone Public Hospitals were excluded from this study.\nAll neonates who have been admitted in the NICUs of Awi zone public hospitals and diagnosed as having neonatal seizure during the follow-up period and passed the eligibility criteria were samples (census method was used). During the study period (January 1-2023 to December 31-2023), a total of 5352 neonates were admitted to the NICUs of Awi Zone public hospitals. Of these, 281 neonates were diagnosed as having seizure. One neonate was diagnosed as having complex malformations, and 17 neonates were inter-hospital referrals between Awi zone public hospitals. Therefore, the final sample size of this study was 263.\n\n\n\nIs calculated from the time of seizure diagnosis till the death of the neonate.\nNeonates who did not die during the follow-up period, including lost follow-up, referred to other health institution, discharged, or still admitted beyond 28 days of neonatal age.\nNeonates who died during the follow-up time.\nnewborn from birth to 28 days old.\nNeonates were diagnosed as having seizure “clinically” if they have one or more signs of provoked or unprovoked seizure that have been witnessed by experienced physicians. These signs include: (1) focal or generalized sustained muscle stiffening or contractions, (2) regular rhythmic jerking, (3) epileptic spasms, (4) myoclonic jerks, (5) autonomic changes, (6) behavioral arrests, and (7) automatisms such as abnormal oral-buccal-lingual movements (lip smacking, chewing, tongue movements), ocular signs (fluttering, rolling, staring, and/ or deviation of the eyes), progressive movements, and complex purposeless movements\nperinatal asphyxia can be defined as failure to initiate and sustain breathing at birth or having an APGAR score of < 7 at 5 minutes\nA pre-tested and structured questionnaire and data extraction checklist were used to collect the data. Face-to-face interviews were used to collect maternal socio-demographics. Obstetrics and baseline clinical characteristics of the neonates were recorded on the day of diagnosis of neonatal seizure. Then, the neonates were followed for a maximum of 28 days; or until discharged, died, referred, or lost to follow-up. Seizure patterns, episodes of attacks, treatment & outcome-related variables were recorded at the end of follow-up period. One BSc pediatric nurse supervisor and two BSc pediatric nurse data collectors were recruited in each hospital to collect the data. Pre-testing of the data collection tool, training of the data collectors and supervisors, close monitoring and cross-checking, coding, double data entry, and cross-tabulation before analysis were employed to control the quality of the data.\nAfter coding, data were entered into EpiData version 4.6.0.6; and exported to STATA version 17 for analysis. Descriptive statistics were used to describe the socio-demographic characteristics, obstetrics, clinical profiles, and seizure characteristics of the study participants. The hazard of death among neonates admitted with seizure across different categories of the covariates was compared by using the Nelson Aalen curves and log-rank tests. Kaplan–Meier survival estimate curve was used to plot the survival probability. The Cox proportional hazards model was used to identify the predictors of time to death. First, bi-variable cox-regression was employed and variables with a\nThe mean age ± standard deviation (SD) of mothers of the study participants was 27.41 ± 5.85 years. Nearly half 125(47.5%) of the mothers were aged less than or equal to 25 years. More than half (54.4%) of the mothers were rural residents. Majority (79.8%) of the mothers were followers of Orthodox Christianity. About 95.1% and 72.6% of the mothers were married and live in a household with less than 5 family members respectively (Table\n\nSocio demographic characteristics of mothers of neonates with seizure at NICU of Awi zone public hospitals, Northwest Ethiopia, 2023(\n*Private employee, daily laborer\nAmong 263 neonates enrolled, 50.6% were females; and 52.9% were aged less than 1 day at admission. Nearly two-thirds (63.1%) of neonates had normal birth weight; with a mean ± SD birth weight of 2797 ± 646.9 g. About 66.5% of the neonates were cried immediately after delivery; while one-fourth (25.1%) of the neonates were asphyxiated (Table\n\nBase line clinical characteristics of the neonates diagnosed with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023 (\nAPGAR, appearance, pulse, grimace, activity, respiration\n*Might be due to home delivery\nMost of the mothers (92%) had received antenatal care; of these, about 47.1% had received ANC follow-up at least four times. About 24% of mothers had bad obstetrics history; of them, 14.8% of the mothers had newborn death, 10.6% had a history of abortion, and 9.9% had a history of stillbirth. About 17.5% of the mothers had hypertensive disorders of pregnancy during the current pregnancy. Most (95.1%) of the mothers had given birth at health institutions, and more than half (59.3%) had given birth spontaneously (Table\n\nObstetric characteristics of mothers of neonates with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023(\nOut of 5352 NICU admissions over one year period, there were about 263 (4.9%) neonatal seizure cases. The most frequently observed seizure (58.6%) was a subtle seizure; followed by a tonic seizure (21.3%). About 70.7% of the neonates experienced their first seizure within 72 h of birth. Nearly three-fourths (70.3%) of neonates had multiple episodes of seizures. All neonates had received anticonvulsant therapy. Phenobarbitone was prescribed for about (15.2%) of neonates, and 62.0% of the neonates received more than one anticonvulsant drug. Of these, 36.5% used two drug combinations (calcium gluconate and phenobarbital); and 25.5% used three drug combinations (calcium gluconate, phenobarbital, and phenytoin). Well-controlled seizures were seen in 72.6% of the neonates. About two third (66.9%) of the neonates stay in the ward for less than or equal to 7 days. Regarding treatment outcomes, 68.1% of the neonates were discharged with improvement and 11.4% were died (Table\n\nPatterns, time onset, episodes of attacks, treatment & outcome related of neonates with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023(\nA total of 263 neonates admitted with seizure were followed for a median follow-up period of 4 days (IQR: 2–7 days), with a minimum of half a day and a maximum of 23 days follow-up period. Of the total study subjects, 30(11.41% (95% CI = 8.0 – 15.0%)) neonates died during the study period. The incidence rate of death was 22.5 (95% CI = 14.0–29.6) per 1000 person-days of observation, with a total follow-up time of 1334.3 person-days of observation. Regarding the time of death, 7(23.3%), 13(49.4%), 16(60.8%), and 22 (83.7%) of deaths occurred within the 2nd, 3rd, 4th and 7th days of follow up respectively; and the median time to death was found to be 3 days with an IRQ of 2–7days.\nRegarding survival Probability, the cumulative survival among neonates with seizure on the 1st, 3rd, 6th, 12th, and 23rd day of admission were 0.996 (95% CI 0.972, 0.999), 0.94% (95% CI 0.9, 0.97), 0.89% (95% CI 0.83, 0.93), 0.78 (95%CI 0.68,0.85), and 0.52% (95% CI 0.2, 0.57), respectively (Fig.\n\nKaplan–Meier cumulative survival estimate of neonates with seizure at NICUs of public hospitals in Awi zone, northwest Ethiopia, 2023 (\nThe Nelson–Aalen curve shows that the hazard of death among neonates admitted with seizure differs across different baseline clinical characteristics of the neonates. Neonates with birth trauma (\n\nNelson–Aalen cumulative hazard curve showing the hazards of death across different categories of the covariates (birth trauma (\nThe Cox proportional hazard (PH) assumption was fulfilled, as shown both statistically and graphically. Statistically, it was tested by the Schoenfeld residual test (global test, X\n\nCox-Snell residual test for overall adequacy of the model fitted for time to death among neonates with of neonatal seizure in NICUs of Public hospitals in Awi zone, Northwest Ethiopia; 2023.\nTo identify predictors of time to death among neonates with seizure, initially, all the study variables\nOf the 14 variables entered to the multivariable Cox regression, 4 variables (neonates with birth trauma, having sepsis at admission, neonates with admission blood glucose value < 40 mg/dl, and neonates with tonic seizure) were found to be significant predictors of time to death among neonates with seizure.\nThe hazard of neonatal death was four times (AHR = 3.94, 95% CI 1.46, 10.64) higher among neonates with birth trauma as compared to their counterparts. Neonates who had Sepsis at admission had nearly three times (AHR = 3.38, 95% CI 1.06, 10.83) higher risk of death than those who did not had sepsis. The hazard of death among neonates with admission blood glucose value of < 40 mg/dl was nearly three times (AHR = 3.24, 95% CI 1.13, 9.29) as compared to neonates whose admission blood glucose value was 40–125 mg/dl. The hazard of neonatal death was nearly 4.5 times (AHR = 4.49, 95% CI 1.29, 15.61) higher for neonates who have tonic seizure as compared to neonates who have subtle seizure (Table\n\nPredictors of death among neonates diagnosed with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023(\nSignificant values are in bold.\nThis study aimed to assess the time to death and its predictors among neonates with seizure in public hospitals of Awi zone, Ethiopia. Accordingly, the median time to death among neonates with seizure in the study area was found to be 3 days with an IQR of 2–7 days. In this study, among 263 neonates with seizure, 30(11.41%, 95% CI 8–15) neonates were died during the follow-up period. This finding was lower than previous studies done in northern Ethiopia (21.3%)\nIn this study, the incidence rate of death was found to be 22.5 per 1000 person-days of observation (95% CI = 14.0–29.6), with a total follow-up time of 1334.3 person-days. This study also revealed that about 83.7% of deaths among neonates with seizure occurred during the first week of their life. As per the author’s knowledge, there was no prior published evidence to discuss this incidence rate and time of death.\nIn this study, the findings of multivariable Cox regression analysis showed neonates with birth trauma, having sepsis at admission, neonates with admission blood glucose value < 40 mg/dl, and neonates with tonic seizure and were significant predictors of early death among neonates with seizure. The hazard of early mortality was nearly 4 times higher in neonates with birth trauma than in neonates without birth trauma. This finding was supported by a study conducted in the United States\nIn this study, having sepsis at admission was found to be a significant predictor of early death among neonates with seizure; as the hazard of early death was three times higher among neonates with sepsis than neonates without sepsis. This finding was in line with previous studies conducted in the United States\nThis study was a multicenter prospective follow-up study; and had used robust research methods. However, it might not be free from some minor limitations. Firstly, as there were no definitive diagnostic infrastructures (EEG) for diagnosing neonatal seizure in Ethiopia, this study had used clinical-only seizures. Therefore, the reported incidence of neonatal seizure might not indicate the true incidence; and the exhibited time to death might not represent electrographic-only seizures. Secondly, due to resource constraints, the follow-up period was relatively short. Hence, this study did not reported the fate of neonates with seizure who were still on treatment for seizure. Additionally, it has not revealed the neurodevelopmental outcomes of the survivors of neonatal seizure.\nThis study had revealed that the incidence of in-hospital mortality among neonates with seizure is high and the median time to death is short. Despite the advancements in neonatal intensive care units, more than one in ten neonates with seizures are dying in the study area. The median time to death was 3 days, and more than eight in ten neonates with seizure are dying in the first week of their life. Birth trauma, sepsis, hypoglycemia, and tonic type seizure were significant predictors of early mortality among neonates with seizure. Therefore, a special emphasis and close follow should be given to neonates with seizure in the first 7 days of their life. Early detection and appropriate management of neonates having birth trauma, sepsis, and hypoglycemia might also be helpful to reduce seizure-related neonatal mortalities. It is also better if future researches are conducted on this issue by addressing the limitations of this study.\nBelow is the link to the electronic supplementary material.\n\nSupplementary Material 1", "content_for_embedding": "A neonatal seizure can be defined as an emergency medical condition that occurs due to sudden, transient, and excessive neuronal activity in the brain of a neonate; leading to electroencephalographic abnormalities and/or overt clinical features like change in behavior, motor, or autonomic function\nNeonatal seizures are commonly caused by hypoxic-ischemic encephalopathy, infections of the central nervous system, acute metabolic disturbances, and intracranial hemorrhages\nNeonatal seizures are major causes of morbidity in infants. Full recovery from neonatal seizure had been reported only in 25–40% of the affected neonates\nLiteratures had been indicated that most seizure-related neonatal deaths occur early during the acute illness\nThis study was conducted in Awi zone, Amhara region, Ethiopia; which is about 426 km away from Addis Ababa, the capital city of Ethiopia. Based on the 2021 census, this zone had a total population of 1,342,324; of whom 51% were females. The zone has a total of 52 health institutions; of which 47 are health centers, 4 are primary hospitals, and 1 is a general hospital. All of the hospitals (Injibara General Hospital, Agew Gimjabet Primary Hospital, Dangila Primary Hospital, Changi Primary Hospital, and Jawi Primary Hospital) provide neonatal admission services. Therefore, this multicenter prospective follow-up study was conducted in those hospitals from January 1-2023 to December 31-2023.\nAll neonates who were admitted to the NICUs of Awi Zone Public Hospitals with a diagnosed neonatal seizure were the source population, and all neonates who were admitted to those institutions with a diagnosed of neonatal seizure from January 1-2023 to December 31-2023 were the study population. However, neonates with complex malformations and neonates who were referred from Awi zone Public Hospitals were excluded from this study.\nAll neonates who have been admitted in the NICUs of Awi zone public hospitals and diagnosed as having neonatal seizure during the follow-up period and passed the eligibility criteria were samples (census method was used). During the study period (January 1-2023 to December 31-2023), a total of 5352 neonates were admitted to the NICUs of Awi Zone public hospitals. Of these, 281 neonates were diagnosed as having seizure. One neonate was diagnosed as having complex malformations, and 17 neonates were inter-hospital referrals between Awi zone public hospitals. Therefore, the final sample size of this study was 263.\n\n\n\nIs calculated from the time of seizure diagnosis till the death of the neonate.\nNeonates who did not die during the follow-up period, including lost follow-up, referred to other health institution, discharged, or still admitted beyond 28 days of neonatal age.\nNeonates who died during the follow-up time.\nnewborn from birth to 28 days old.\nNeonates were diagnosed as having seizure “clinically” if they have one or more signs of provoked or unprovoked seizure that have been witnessed by experienced physicians. These signs include: (1) focal or generalized sustained muscle stiffening or contractions, (2) regular rhythmic jerking, (3) epileptic spasms, (4) myoclonic jerks, (5) autonomic changes, (6) behavioral arrests, and (7) automatisms such as abnormal oral-buccal-lingual movements (lip smacking, chewing, tongue movements), ocular signs (fluttering, rolling, staring, and/ or deviation of the eyes), progressive movements, and complex purposeless movements\nperinatal asphyxia can be defined as failure to initiate and sustain breathing at birth or having an APGAR score of < 7 at 5 minutes\nA pre-tested and structured questionnaire and data extraction checklist were used to collect the data. Face-to-face interviews were used to collect maternal socio-demographics. Obstetrics and baseline clinical characteristics of the neonates were recorded on the day of diagnosis of neonatal seizure. Then, the neonates were followed for a maximum of 28 days; or until discharged, died, referred, or lost to follow-up. Seizure patterns, episodes of attacks, treatment & outcome-related variables were recorded at the end of follow-up period. One BSc pediatric nurse supervisor and two BSc pediatric nurse data collectors were recruited in each hospital to collect the data. Pre-testing of the data collection tool, training of the data collectors and supervisors, close monitoring and cross-checking, coding, double data entry, and cross-tabulation before analysis were employed to control the quality of the data.\nAfter coding, data were entered into EpiData version 4.6.0.6; and exported to STATA version 17 for analysis. Descriptive statistics were used to describe the socio-demographic characteristics, obstetrics, clinical profiles, and seizure characteristics of the study participants. The hazard of death among neonates admitted with seizure across different categories of the covariates was compared by using the Nelson Aalen curves and log-rank tests. Kaplan–Meier survival estimate curve was used to plot the survival probability. The Cox proportional hazards model was used to identify the predictors of time to death. First, bi-variable cox-regression was employed and variables with a\nThe mean age ± standard deviation (SD) of mothers of the study participants was 27.41 ± 5.85 years. Nearly half 125(47.5%) of the mothers were aged less than or equal to 25 years. More than half (54.4%) of the mothers were rural residents. Majority (79.8%) of the mothers were followers of Orthodox Christianity. About 95.1% and 72.6% of the mothers were married and live in a household with less than 5 family members respectively (Table\n\nSocio demographic characteristics of mothers of neonates with seizure at NICU of Awi zone public hospitals, Northwest Ethiopia, 2023(\n*Private employee, daily laborer\nAmong 263 neonates enrolled, 50.6% were females; and 52.9% were aged less than 1 day at admission. Nearly two-thirds (63.1%) of neonates had normal birth weight; with a mean ± SD birth weight of 2797 ± 646.9 g. About 66.5% of the neonates were cried immediately after delivery; while one-fourth (25.1%) of the neonates were asphyxiated (Table\n\nBase line clinical characteristics of the neonates diagnosed with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023 (\nAPGAR, appearance, pulse, grimace, activity, respiration\n*Might be due to home delivery\nMost of the mothers (92%) had received antenatal care; of these, about 47.1% had received ANC follow-up at least four times. About 24% of mothers had bad obstetrics history; of them, 14.8% of the mothers had newborn death, 10.6% had a history of abortion, and 9.9% had a history of stillbirth. About 17.5% of the mothers had hypertensive disorders of pregnancy during the current pregnancy. Most (95.1%) of the mothers had given birth at health institutions, and more than half (59.3%) had given birth spontaneously (Table\n\nObstetric characteristics of mothers of neonates with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023(\nOut of 5352 NICU admissions over one year period, there were about 263 (4.9%) neonatal seizure cases. The most frequently observed seizure (58.6%) was a subtle seizure; followed by a tonic seizure (21.3%). About 70.7% of the neonates experienced their first seizure within 72 h of birth. Nearly three-fourths (70.3%) of neonates had multiple episodes of seizures. All neonates had received anticonvulsant therapy. Phenobarbitone was prescribed for about (15.2%) of neonates, and 62.0% of the neonates received more than one anticonvulsant drug. Of these, 36.5% used two drug combinations (calcium gluconate and phenobarbital); and 25.5% used three drug combinations (calcium gluconate, phenobarbital, and phenytoin). Well-controlled seizures were seen in 72.6% of the neonates. About two third (66.9%) of the neonates stay in the ward for less than or equal to 7 days. Regarding treatment outcomes, 68.1% of the neonates were discharged with improvement and 11.4% were died (Table\n\nPatterns, time onset, episodes of attacks, treatment & outcome related of neonates with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023(\nA total of 263 neonates admitted with seizure were followed for a median follow-up period of 4 days (IQR: 2–7 days), with a minimum of half a day and a maximum of 23 days follow-up period. Of the total study subjects, 30(11.41% (95% CI = 8.0 – 15.0%)) neonates died during the study period. The incidence rate of death was 22.5 (95% CI = 14.0–29.6) per 1000 person-days of observation, with a total follow-up time of 1334.3 person-days of observation. Regarding the time of death, 7(23.3%), 13(49.4%), 16(60.8%), and 22 (83.7%) of deaths occurred within the 2nd, 3rd, 4th and 7th days of follow up respectively; and the median time to death was found to be 3 days with an IRQ of 2–7days.\nRegarding survival Probability, the cumulative survival among neonates with seizure on the 1st, 3rd, 6th, 12th, and 23rd day of admission were 0.996 (95% CI 0.972, 0.999), 0.94% (95% CI 0.9, 0.97), 0.89% (95% CI 0.83, 0.93), 0.78 (95%CI 0.68,0.85), and 0.52% (95% CI 0.2, 0.57), respectively (Fig.\n\nKaplan–Meier cumulative survival estimate of neonates with seizure at NICUs of public hospitals in Awi zone, northwest Ethiopia, 2023 (\nThe Nelson–Aalen curve shows that the hazard of death among neonates admitted with seizure differs across different baseline clinical characteristics of the neonates. Neonates with birth trauma (\n\nNelson–Aalen cumulative hazard curve showing the hazards of death across different categories of the covariates (birth trauma (\nThe Cox proportional hazard (PH) assumption was fulfilled, as shown both statistically and graphically. Statistically, it was tested by the Schoenfeld residual test (global test, X\n\nCox-Snell residual test for overall adequacy of the model fitted for time to death among neonates with of neonatal seizure in NICUs of Public hospitals in Awi zone, Northwest Ethiopia; 2023.\nTo identify predictors of time to death among neonates with seizure, initially, all the study variables\nOf the 14 variables entered to the multivariable Cox regression, 4 variables (neonates with birth trauma, having sepsis at admission, neonates with admission blood glucose value < 40 mg/dl, and neonates with tonic seizure) were found to be significant predictors of time to death among neonates with seizure.\nThe hazard of neonatal death was four times (AHR = 3.94, 95% CI 1.46, 10.64) higher among neonates with birth trauma as compared to their counterparts. Neonates who had Sepsis at admission had nearly three times (AHR = 3.38, 95% CI 1.06, 10.83) higher risk of death than those who did not had sepsis. The hazard of death among neonates with admission blood glucose value of < 40 mg/dl was nearly three times (AHR = 3.24, 95% CI 1.13, 9.29) as compared to neonates whose admission blood glucose value was 40–125 mg/dl. The hazard of neonatal death was nearly 4.5 times (AHR = 4.49, 95% CI 1.29, 15.61) higher for neonates who have tonic seizure as compared to neonates who have subtle seizure (Table\n\nPredictors of death among neonates diagnosed with seizure at NICU of Awi zone public hospitals, North West Ethiopia, 2023(\nSignificant values are in bold.\nThis study aimed to assess the time to death and its predictors among neonates with seizure in public hospitals of Awi zone, Ethiopia. Accordingly, the median time to death among neonates with seizure in the study area was found to be 3 days with an IQR of 2–7 days. In this study, among 263 neonates with seizure, 30(11.41%, 95% CI 8–15) neonates were died during the follow-up period. This finding was lower than previous studies done in northern Ethiopia (21.3%)\nIn this study, the incidence rate of death was found to be 22.5 per 1000 person-days of observation (95% CI = 14.0–29.6), with a total follow-up time of 1334.3 person-days. This study also revealed that about 83.7% of deaths among neonates with seizure occurred during the first week of their life. As per the author’s knowledge, there was no prior published evidence to discuss this incidence rate and time of death.\nIn this study, the findings of multivariable Cox regression analysis showed neonates with birth trauma, having sepsis at admission, neonates with admission blood glucose value < 40 mg/dl, and neonates with tonic seizure and were significant predictors of early death among neonates with seizure. The hazard of early mortality was nearly 4 times higher in neonates with birth trauma than in neonates without birth trauma. This finding was supported by a study conducted in the United States\nIn this study, having sepsis at admission was found to be a significant predictor of early death among neonates with seizure; as the hazard of early death was three times higher among neonates with sepsis than neonates without sepsis. This finding was in line with previous studies conducted in the United States\nThis study was a multicenter prospective follow-up study; and had used robust research methods. However, it might not be free from some minor limitations. Firstly, as there were no definitive diagnostic infrastructures (EEG) for diagnosing neonatal seizure in Ethiopia, this study had used clinical-only seizures. Therefore, the reported incidence of neonatal seizure might not indicate the true incidence; and the exhibited time to death might not represent electrographic-only seizures. Secondly, due to resource constraints, the follow-up period was relatively short. Hence, this study did not reported the fate of neonates with seizure who were still on treatment for seizure. Additionally, it has not revealed the neurodevelopmental outcomes of the survivors of neonatal seizure.\nThis study had revealed that the incidence of in-hospital mortality among neonates with seizure is high and the median time to death is short. Despite the advancements in neonatal intensive care units, more than one in ten neonates with seizures are dying in the study area. The median time to death was 3 days, and more than eight in ten neonates with seizure are dying in the first week of their life. Birth trauma, sepsis, hypoglycemia, and tonic type seizure were significant predictors of early mortality among neonates with seizure. Therefore, a special emphasis and close follow should be given to neonates with seizure in the first 7 days of their life. Early detection and appropriate management of neonates having birth trauma, sepsis, and hypoglycemia might also be helpful to reduce seizure-related neonatal mortalities. It is also better if future researches are conducted on this issue by addressing the limitations of this study.\nBelow is the link to the electronic supplementary material.\n\nSupplementary Material 1", "topic": "Brain"}
{"pmid": "38659961", "pmcid": "12309442", "title": "Third‐order self‐embedded vocal motifs in wild orangutans, and the selective evolution of recursion", "publication_year": "N/A", "abstract": "Recursion, the neuro‐computational operation of nesting a signal or pattern within itself, lies at the structural basis of language. Classically considered absent in the vocal repertoires of nonhuman animals, whether recursion evolved step‐by‐step or saltationally in humans is among the most fervent debates in cognitive science since Chomsky's seminal work on syntax in the 1950s. The recent discovery of self‐embedded vocal motifs in wild (nonhuman) great apes—Bornean male orangutans’ long calls—lends initial but important support to the notion that recursion, or at least temporal recursion, is not uniquely human among hominids and that its evolution was based on shared ancestry. Building on these findings, we test four necessary predictions for a gradual evolutionary scenario in wild Sumatran female orangutans’ alarm calls, the longest known combinations of consonant‐like and vowel‐like calls among great apes (excepting humans). From the data, we propose third‐order self‐embedded isochrony: three hierarchical levels of nested isochronous combinatoric units, with each level exhibiting unique variation dynamics and information content relative to context. Our findings confirm that recursive operations underpin great ape call combinatorics, operations that likely evolved gradually in the human lineage as vocal sequences became longer and more intricate.", "full_text": "Language defines being human, but its evolution defies scientific explanation. Features once believed to be unique to language have been found to be shared with animal communication,\nTo assess the recursive capacities of (nonhuman) animals, artificial grammar experiments using humanmade tokens and rules have shown that other species can learn to perceive and process recursive structures after receiving dedicated training\nRecently, the first evidence for recursive operations has, however, emerged in great ape call combinations, suggesting that these operations may have been hiding in plain sight. Lameira and colleagues\nRecursive self‐embedded isochrony in Bornean male orangutan loud calls, as originally described by Lameira et al.\nTempo differences across levels for alarm calls given in response to natural (light pink) and non‐natural (pink) predators. From left to right: tempo of combinations, bouts, and series of alarm calls. * Denotes\nThe findings by Lameira et al.\nIf recursive operations were indeed available in the vocal toolkit of ancestral hominids and were evolutionarily relevant for the emergence of language, then four obligatory key predictions would need to be fulfilled. First, recursive operations among nonhuman hominids should not be restricted to one species or one location. Only then would recursive operations be sufficiently resilient to stochastic demographic variations, ecologic vagaries, and/or extinction events,\nSecond, recursive operations should not be restricted to one sex, given that ultimately any individual ought to be able of deploying recursion.\nThird, recursive operations should not be restricted to one vocal behavior. Notably, to support the view that the use of recursive operations in ancient hominids would have been pertinent for language evolution, they ought to underlie different types of consonant‐like and vowel‐like calls, and combinations thereof, given that these eventually became the two elemental building blocks of the primary medium of language.\nFourth, recursive operations should not be restricted to one context. They ought to occur across, be affected by, and help predict, different external objects or events. Recursive operations across contexts would be imperative for functional (i.e., informative) use, given that only then could recursive operations begin to offer fitness benefits, setting selective forces in motion. Environmental responsiveness and context‐sensitivity would, thus, be a requirement if recursive operations were to ultimately operate on elements encoding semantic information (e.g., words).\nNote that these conditions, while essential for a gradual evolutionary pathway, inherently counter possible saltationist arguments. A single mutational event capable of engendering a major transformation in an ancestral hominid would be likely confined to a single species, location, vocal behavior, or context (had individuals hypothetically remained reproductively unaffected after such an event). Such a limitation would constrain mutation persistence, spread, and fixation within and across populations.\nGiven that initial proof for the presence of recursive operations exists for Bornean male orangutans’ long calls used for sexual advertisement,\nWe tested seven wild adult female orangutans, habituated to human presence, who represented all adult female local residents of the Ketambe forest (3°41′N, 97°39′E) in Aceh, Sumatra, Indonesia, between 2010 and 2011, comprising about 450 hectares of lowland and hill dipterocarp rainforest. Subjects were presented with four predator models, consisting of a human experimenter walking on all fours on the forest ground draped over a sheet with one of four different types of print: tiger patterned (orangutans’ natural predator in Sumatra\nSpectrograms were visually inspected in Raven Pro\nTo evaluate the rhythmic structure of orangutan alarm calls, we considered three levels of analysis: (1) combination level: calls separated by 0.2 s or less were considered to represent a combination; (2) bout level: combinations separated by more than 0.2 and less than 2 s were considered to represent a bout; (3) series level: bouts separated by more than 2 and less than 20 s were considered to represent a series. When a silent gap between two calls was over 20 s, calls were considered to belong to different series. From 14 recordings, we obtained 48 different series.\nFor each level of analysis, we calculated the inter‐onset interval duration (aka: IOI, hereafter\nTo investigate the occurrence of rhythmic categories, we divided the ratio distribution into on‐integer and off‐integer ratio ranges, centering the on‐integer ratio range around 1:1 (or 0.50—representing isochrony), 1:2 (or 0.33), 1:3 (or 0.25), 2:1 (or 0.66). Following previous studies,\nThe range of a 1:3 integer ratio was considered when\nWe used nine generalized linear mixed models (GLMMs) using the\nTo examine the potential influence of context on each level's tempo, we used an LMM per level to assess differences in\nFor the combination level, we ran a model using as a response variable the\nWe created two subsets of\nFor the bout level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nFor the series level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nThe average\nRecursive self‐embedded isochrony in Sumatran female alarm calls. Findings represent a case of third‐order self‐embedded isochrony underpinned by applying a recursive operation twice. Dark blue blocks represent call units of eight different possible call types. Light‐blue blocks containing dark squares represent bouts. Light gray blocks containing light‐blue blocks represent series. Orange lines between blocks represent silent intervals. Red lines represent inter‐onset\n\nThe visual inspection of the density distribution suggested the presence of a 1:1 ratio (isochrony) for the combination level (Figure\nWe found that the temporal organization of alarm calls within a combination was isochronous (1:1 off vs. 1:1 on; estimate = −0.345, SE = 0.108, z‐ratio = −3.186,\nThe temporal organization of consecutive bouts of alarm calls was overall isochronous (1:1 off vs. 1:1 on; estimate = −0.816, SE = 0.227, z‐ratio = −3.599,\nThe density distribution showed a bimodal distribution (Figure\nWe found that consecutive bouts of alarm calls were isochronous (1:1 on vs. 1:1 off; estimate = 0.429, SE = 0.156, z‐ratio = 2.753,\nOur results demonstrate the presence of recursive self‐embedded vocal motifs organized across three hierarchical strata in the alarm calls of female Sumatran orangutans. These results expand on previous findings in the long calls of male Bornean orangutans, in direct agreement with the predictions defined at the start of the study for the gradual evolution of recursion. Together, these two converging lines of evidence show that recursive vocal operations were likely present in ancient hominids across taxa and geography, likely deployed by males and females, and dynamically used to encode information about external objects and events in various call types. These conditions would have been necessary and sufficient to allow natural selection processes to target recursive vocal combinatorics based on their proximate functions, information content, and communication benefits.\nOur findings provide, to our knowledge, the first empirical support to the view that ever more powerful recursive capacities could have been selected for and evolved incrementally. In one context, individuals invoked a recursive\nOur results revealed differences in temporal organization across levels in natural versus non‐natural predator models. Notably, the predator model type affected a combination isochrony and bout tempo. At the bout level, this result is in line with other instances in the animal world where alarm calls\nBoth instances of predator information encoding based on on/off‐isochrony and variation in isochrony tempo support the view that this recursive system has evolved for, and facilitates, effective communication, which ought to be expected to be at a selective premium among dispersed social organizations, as is characteristic for wild orangutans.\nOur findings reveal for the first time, to our knowledge, the presence of an isochronous temporal organization at a large timescale, a regular tempo with >10 s between each sound onset. Previous work on primate vocal rhythms has focused primarily on temporal intervals with durations of <5 seconds.\nOur study was purely observational, but we can offer two possible proximate reasons. Using recurring silence gaps that are at least 10 s long during an alarm call response is likely to allow individuals to systematically monitor and gather information from their environment, being it about the presence or movement of the predator they have just encountered or possible nearby conspecifics. Using isochrony at this scale would then allow individuals to adjust their ensuing antipredator and general behaviors accordingly, something difficult to achieve with recurring gaps shorter by one or two orders of magnitude (i.e., at first‐ and second‐order isochrony). Second, third‐order isochrony with >10 s gaps could also represent a byproduct of optimal resting times between series of alarm calls.\nThere are multiple examples of patterns in music with intervals equally as lengthy as orangutans’ alarm call series, including Bolero's ostinatos, “cycles” in African and Indian music, isorhythm in medieval music, rondo in classical music, and passacaglia and chaconne in Baroque music. Because rhythm tempos progress in these instances over cycles of >10 s, they can only emerge and be detected as extended compositions that surpass the length of typical ordinary speech exchanges and typical laboratory stimuli. This could begin to explain why this temporal scale of organization has not, and cannot often be, investigated. Our results invite a “zoom out” of the empirical window for the study of acoustic communication and vocal rhythms in long sequences across taxa, including humans (e.g., speeches). Nonetheless, the parallel with music is limited. Long‐cycle musical motifs unfold over several bars and (therein) beats, thus, the ratios between these levels must be necessarily even integers. This differs from our observation in wild orangutans, where the three layers of isochrony were related by the odd ratio 2:9:67.\nEncoding information in the temporal structure across hierarchical layers supports the view that, once present, recursive assembly operations could provide proximate benefits, multiplying the amount of information that a message may encode and diversifying how the same message can be encoded. However, most signal systems in nature are, and have been, also shaped by similar benefits and selective forces, and yet, the emergence of different hierarchical layers has not ensued in most cases. Why recursive operations have only been detected in a hominid remains, thus, unclear. A possible answer could rest on the size and scale of the hominid brain. Human\nClassically, primate vocal output is seen as a sonic cast of its vocal anatomy.\nC.D.G. and A.R.L.: Conceptualization. C.D.G.: Methodology. C.D.G.: Investigation. A.R.L. and M.G.: Supervision. C.D.G. and A.R.L.: Writing—original draft. C.D.G., A.R.L., and M.G.: Writing—review and editing. C.D.G. and A.R.L.: Visualization. All authors contributed to the article and approved the submitted version.\nThe authors declare no competing interests.\nThe peer review history for this article is available at:", "content_for_embedding": "Language defines being human, but its evolution defies scientific explanation. Features once believed to be unique to language have been found to be shared with animal communication,\nTo assess the recursive capacities of (nonhuman) animals, artificial grammar experiments using humanmade tokens and rules have shown that other species can learn to perceive and process recursive structures after receiving dedicated training\nRecently, the first evidence for recursive operations has, however, emerged in great ape call combinations, suggesting that these operations may have been hiding in plain sight. Lameira and colleagues\nRecursive self‐embedded isochrony in Bornean male orangutan loud calls, as originally described by Lameira et al.\nTempo differences across levels for alarm calls given in response to natural (light pink) and non‐natural (pink) predators. From left to right: tempo of combinations, bouts, and series of alarm calls. * Denotes\nThe findings by Lameira et al.\nIf recursive operations were indeed available in the vocal toolkit of ancestral hominids and were evolutionarily relevant for the emergence of language, then four obligatory key predictions would need to be fulfilled. First, recursive operations among nonhuman hominids should not be restricted to one species or one location. Only then would recursive operations be sufficiently resilient to stochastic demographic variations, ecologic vagaries, and/or extinction events,\nSecond, recursive operations should not be restricted to one sex, given that ultimately any individual ought to be able of deploying recursion.\nThird, recursive operations should not be restricted to one vocal behavior. Notably, to support the view that the use of recursive operations in ancient hominids would have been pertinent for language evolution, they ought to underlie different types of consonant‐like and vowel‐like calls, and combinations thereof, given that these eventually became the two elemental building blocks of the primary medium of language.\nFourth, recursive operations should not be restricted to one context. They ought to occur across, be affected by, and help predict, different external objects or events. Recursive operations across contexts would be imperative for functional (i.e., informative) use, given that only then could recursive operations begin to offer fitness benefits, setting selective forces in motion. Environmental responsiveness and context‐sensitivity would, thus, be a requirement if recursive operations were to ultimately operate on elements encoding semantic information (e.g., words).\nNote that these conditions, while essential for a gradual evolutionary pathway, inherently counter possible saltationist arguments. A single mutational event capable of engendering a major transformation in an ancestral hominid would be likely confined to a single species, location, vocal behavior, or context (had individuals hypothetically remained reproductively unaffected after such an event). Such a limitation would constrain mutation persistence, spread, and fixation within and across populations.\nGiven that initial proof for the presence of recursive operations exists for Bornean male orangutans’ long calls used for sexual advertisement,\nWe tested seven wild adult female orangutans, habituated to human presence, who represented all adult female local residents of the Ketambe forest (3°41′N, 97°39′E) in Aceh, Sumatra, Indonesia, between 2010 and 2011, comprising about 450 hectares of lowland and hill dipterocarp rainforest. Subjects were presented with four predator models, consisting of a human experimenter walking on all fours on the forest ground draped over a sheet with one of four different types of print: tiger patterned (orangutans’ natural predator in Sumatra\nSpectrograms were visually inspected in Raven Pro\nTo evaluate the rhythmic structure of orangutan alarm calls, we considered three levels of analysis: (1) combination level: calls separated by 0.2 s or less were considered to represent a combination; (2) bout level: combinations separated by more than 0.2 and less than 2 s were considered to represent a bout; (3) series level: bouts separated by more than 2 and less than 20 s were considered to represent a series. When a silent gap between two calls was over 20 s, calls were considered to belong to different series. From 14 recordings, we obtained 48 different series.\nFor each level of analysis, we calculated the inter‐onset interval duration (aka: IOI, hereafter\nTo investigate the occurrence of rhythmic categories, we divided the ratio distribution into on‐integer and off‐integer ratio ranges, centering the on‐integer ratio range around 1:1 (or 0.50—representing isochrony), 1:2 (or 0.33), 1:3 (or 0.25), 2:1 (or 0.66). Following previous studies,\nThe range of a 1:3 integer ratio was considered when\nWe used nine generalized linear mixed models (GLMMs) using the\nTo examine the potential influence of context on each level's tempo, we used an LMM per level to assess differences in\nFor the combination level, we ran a model using as a response variable the\nWe created two subsets of\nFor the bout level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nFor the series level, we used as a response variable the\nWe then created two subsets of data. In the first case, selecting only the\nThe average\nRecursive self‐embedded isochrony in Sumatran female alarm calls. Findings represent a case of third‐order self‐embedded isochrony underpinned by applying a recursive operation twice. Dark blue blocks represent call units of eight different possible call types. Light‐blue blocks containing dark squares represent bouts. Light gray blocks containing light‐blue blocks represent series. Orange lines between blocks represent silent intervals. Red lines represent inter‐onset\n\nThe visual inspection of the density distribution suggested the presence of a 1:1 ratio (isochrony) for the combination level (Figure\nWe found that the temporal organization of alarm calls within a combination was isochronous (1:1 off vs. 1:1 on; estimate = −0.345, SE = 0.108, z‐ratio = −3.186,\nThe temporal organization of consecutive bouts of alarm calls was overall isochronous (1:1 off vs. 1:1 on; estimate = −0.816, SE = 0.227, z‐ratio = −3.599,\nThe density distribution showed a bimodal distribution (Figure\nWe found that consecutive bouts of alarm calls were isochronous (1:1 on vs. 1:1 off; estimate = 0.429, SE = 0.156, z‐ratio = 2.753,\nOur results demonstrate the presence of recursive self‐embedded vocal motifs organized across three hierarchical strata in the alarm calls of female Sumatran orangutans. These results expand on previous findings in the long calls of male Bornean orangutans, in direct agreement with the predictions defined at the start of the study for the gradual evolution of recursion. Together, these two converging lines of evidence show that recursive vocal operations were likely present in ancient hominids across taxa and geography, likely deployed by males and females, and dynamically used to encode information about external objects and events in various call types. These conditions would have been necessary and sufficient to allow natural selection processes to target recursive vocal combinatorics based on their proximate functions, information content, and communication benefits.\nOur findings provide, to our knowledge, the first empirical support to the view that ever more powerful recursive capacities could have been selected for and evolved incrementally. In one context, individuals invoked a recursive\nOur results revealed differences in temporal organization across levels in natural versus non‐natural predator models. Notably, the predator model type affected a combination isochrony and bout tempo. At the bout level, this result is in line with other instances in the animal world where alarm calls\nBoth instances of predator information encoding based on on/off‐isochrony and variation in isochrony tempo support the view that this recursive system has evolved for, and facilitates, effective communication, which ought to be expected to be at a selective premium among dispersed social organizations, as is characteristic for wild orangutans.\nOur findings reveal for the first time, to our knowledge, the presence of an isochronous temporal organization at a large timescale, a regular tempo with >10 s between each sound onset. Previous work on primate vocal rhythms has focused primarily on temporal intervals with durations of <5 seconds.\nOur study was purely observational, but we can offer two possible proximate reasons. Using recurring silence gaps that are at least 10 s long during an alarm call response is likely to allow individuals to systematically monitor and gather information from their environment, being it about the presence or movement of the predator they have just encountered or possible nearby conspecifics. Using isochrony at this scale would then allow individuals to adjust their ensuing antipredator and general behaviors accordingly, something difficult to achieve with recurring gaps shorter by one or two orders of magnitude (i.e., at first‐ and second‐order isochrony). Second, third‐order isochrony with >10 s gaps could also represent a byproduct of optimal resting times between series of alarm calls.\nThere are multiple examples of patterns in music with intervals equally as lengthy as orangutans’ alarm call series, including Bolero's ostinatos, “cycles” in African and Indian music, isorhythm in medieval music, rondo in classical music, and passacaglia and chaconne in Baroque music. Because rhythm tempos progress in these instances over cycles of >10 s, they can only emerge and be detected as extended compositions that surpass the length of typical ordinary speech exchanges and typical laboratory stimuli. This could begin to explain why this temporal scale of organization has not, and cannot often be, investigated. Our results invite a “zoom out” of the empirical window for the study of acoustic communication and vocal rhythms in long sequences across taxa, including humans (e.g., speeches). Nonetheless, the parallel with music is limited. Long‐cycle musical motifs unfold over several bars and (therein) beats, thus, the ratios between these levels must be necessarily even integers. This differs from our observation in wild orangutans, where the three layers of isochrony were related by the odd ratio 2:9:67.\nEncoding information in the temporal structure across hierarchical layers supports the view that, once present, recursive assembly operations could provide proximate benefits, multiplying the amount of information that a message may encode and diversifying how the same message can be encoded. However, most signal systems in nature are, and have been, also shaped by similar benefits and selective forces, and yet, the emergence of different hierarchical layers has not ensued in most cases. Why recursive operations have only been detected in a hominid remains, thus, unclear. A possible answer could rest on the size and scale of the hominid brain. Human\nClassically, primate vocal output is seen as a sonic cast of its vocal anatomy.\nC.D.G. and A.R.L.: Conceptualization. C.D.G.: Methodology. C.D.G.: Investigation. A.R.L. and M.G.: Supervision. C.D.G. and A.R.L.: Writing—original draft. C.D.G., A.R.L., and M.G.: Writing—review and editing. C.D.G. and A.R.L.: Visualization. All authors contributed to the article and approved the submitted version.\nThe authors declare no competing interests.\nThe peer review history for this article is available at:", "topic": "Brain"}
{"pmid": "38608489", "pmcid": "12305819", "title": "Neuro-insights: a systematic review of neuromarketing perspectives across consumer buying stages", "publication_year": "N/A", "abstract": "The application of neurophysiological techniques in marketing and consumer research has seen substantial growth in recent years. This review provides a comprehensive overview of how neuroscience has been integrated into consumer behavior research commonly referred to as “neuromarketing.” While prior reviews have addressed methods, tools, and theoretical foundations, they have largely concentrated on the pre-purchase stage of decision-making. Expanding on this, the current review examines the stage specific affective behavioral and cognitive components neural responses across the full consumer journey. Using the PRISMA framework, the authors systematically analyze stage specific existing neuromarketing literature to present a well-rounded perspective. Moreover, it introduces an integrated framework that aligns neuromarketing insights with each stage of the consumer decision-making process. To support future research, the paper proposes a novel 3 × 3 typology, identifying cross modal interactiona and underexplored areas and gaps in the literature. Overall, this review advances neuromarketing as a rigorous and credible research approach, offering valuable direction for scholars and contributing to its establishment as a recognized discipline within marketing.", "full_text": "Neuromarketing, or consumer neuroscience, is an interdisciplinary field that combines neuroscience, psychology, and economics to explore and influence consumer behavior. Neuromarketing entails analyzing physiological and brain signals to examine the mind, brain, and behavior, aiming to understand, predict, and shape consumer behavior and decision-making (Harrell,\nNeuromarketing has gained prominence as a research domain since the early 2000s, marked notably by Montague et al.'s (\nThe objective of the present study is to systematically review the literature of neuromarketing based on the cognitive and affective dimensions mapped with stage specific neural correlations, based on the Preferred Reporting Items for Systematic Literature Reviews and Meta-Analyses (PRISMA) framework (Page et al.,\nRQ1: What are the stage-specific key theories, variables, methodologies, cross modal interactions and neuro-tools commonly used in neuromarketing research?\nRQ2: To what extent has neuromarketing research and neural correlates have been explored across the different stages of consumer decision-making?\nRQ3: What has been well-studied vs. under-studied at each stage, suggesting future directions, tools, and methods for a more holistic neuromarketing approach.\nThis review makes five significant contributions to the field of neuromarketing. First, our systematic analysis and synthesis of the literature according to the buying stages of decision making, incorporating stage specific neural correlates. Second, it focusses on empirical studies, commercial applications, and theoretical development, aiming to establish a standardized definition of neuromarketing and resolve its definitional ambiguity. Third, we explore how consumer neuroscience integrates established theories and frameworks to provide insights into cross modal interactions of consumer behavior and synthesize how cognitive vs. emotional processing dominates buying stages (pre-purchase, purchase, and post-purchase). Fourth, the review maps specific neural mechanisms (e.g., reward processing, attention, and emotional arousal) to individual decision stages like problem recognition, information search, evaluation, choice, and post-purchase. Finally, the study proposes a 3 × 3 typology, encompassing decision-making (conscious vs. unconscious vs. both) and buying stages (pre-purchase, purchase, and post-purchase). This typology serves as a roadmap for researchers to explore what is well-studied vs. under-studied under-explored areas in the existing literature (Li et al.,\nThis review (\nKey characteristics of past and present literature reviews on neuromarketing and consumer neuroscience.\nThe roots of neuromarketing, which emerged in the early 2000s, can be traced back to foundational psychological theories of perception, learning, and motivation (Watson,\nFurthermore, existing neuromarketing research has largely concentrated on the pre-purchase phase, examining how the brain reacts to marketing stimuli prior to a buying decision. In contrast, there has been limited exploration of its application during the purchase and post-purchase phases—stages where consumer satisfaction, brand loyalty, and advocacy may be significantly influenced by neuropsychological factors (Lin et al.,\nAnalyzing all five stages mapped to the affective, behavioral and cognitive components provides a comprehensive view of decision-making, especially since these phases do not always follow a linear progression (Lemon and Verhoef,\nIn this section, the methodology adopted for carrying out the systematic literature review process is elaborated.\nTo collect data, we utilized the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework (Page et al.,\nDue to the interdisciplinary nature of the research, we explored EBSCOhost Business Source Complete, Scopus, and Web of Science to ensure broad coverage across relevant studies. Specifically, for this review, a search was conducted on April 14, 2025, focusing on neuromarketing, consumer neuroscience and consumer decision making. This initial query yielded 3,203 potentially relevant articles.\nThe identified articles were imported into the citation management software EndNote X9, where the initial pool of 3,203 studies was refined to 338 using the Find Duplicates function. Next, exclusion criteria were applied to filter out publications unrelated to “business,” “management,” “accounting,” “engineering,” or “social science.” Articles not published in academic journals, those outside the 2020–2025 timeframe, or written in languages other than English were also excluded. As part of the inclusion criteria, only documents classified as articles or reviews were retained, with a particular emphasis on source titles relevant to the business and management fields.\nThe field of neuromarketing and consumer neuroscience is highly diverse and fragmented—not only in terms of the topics and areas explored, but also in the methodologies employed, contexts studied, and application approaches—resulting in a heterogeneous body of literature varying widely in both subject matter and quality (Ramsøy,\nPRISMA flowchart.\nThis section presents the conceptual framework guiding our review. As illustrated in\nProposed conceptual framework.\nThe pre-purchase stage encompasses all activities a consumer undertakes prior to making a purchase, including need recognition, information gathering, and evaluating available alternatives (Lemon and Verhoef,\nIn the pre-purchase phase, consumers actively seek information and evaluate various options, which may involve comparing multiple products or concentrating on a particular one (Lee et al.,\nIn the purchase decision phase, consumers make several critical choices, including whether to proceed with the purchase, which product(s) to select, how much to buy, which retailer to choose, and the best time to make the purchase. Other factors, such as delivery times for online orders, also influence decisions in this stage (Lee et al.,\nAfter purchasing a product, consumers often assess their experience in comparison to their initial expectations (Lee et al.,\nIn neuromarketing, post-purchase behavior is examined by analyzing how consumers evaluate their purchase afterward, with a focus on emotional and cognitive responses. For example, Hamelin et al. (\nWe suggest that actual consumer behavior across decision-making stages (\nGiven the definitional ambiguity surrounding neuromarketing and consumer neuroscience (Khamitov et al.,\nSeminal theories and their mapping with purchase stages and attitudinal responses.\nA comprehensive evaluation of the relative strengths and limitations of neuroscience research methods is often missing from the neuromarketing literature. Most studies tend to offer only brief descriptions of the specific neuroscience techniques they employ, without delving into their broader functional capabilities. As noted by Harris et al. (\nNeuroscientific tools, their applications and measurement variables.\nConsumer research represents one of the most prominent areas where neuroscientific methods have been applied extensively (Lim,\nMeasurement indicators.\nMarketing focuses on influencing consumer perceptions and decisions, while neuroscientific tools enable us to gain insights into the brain's responses to marketing stimuli. Three methods are typically employed to gauge customers' reactions: behavioral measures, self-reports, and psychophysiological tests (Hsu and Chen,\nFrom the consumer's standpoint, marketing and consumer research have increasingly employed both neurometric (e.g., fMRI, EEG, MEG, SST, TMS, fNIRS, and PET) and non-neurometric (e.g., Eye Tracking, Galvanic Skin Response, Facial Action Coding, facial EMG, Heart Rate, and Infrared Thermography) techniques to better understand behavior—particularly decision-making processes. These tools enable researchers to uncover cognitive, emotional, and behavioral responses, helping to interpret, explain, and address barriers to acceptance and action regarding various issues. The emergence of interdisciplinary fields such as neurophilosophy, neuroeconomics (Sanfey et al.,\nInstruments and techniques utilized in reviewed studies.\nThe study introduces a 3 × 3 typology (\nProposed 3 × 3 typology.\nGiven the critical role of unconscious decision-making in consumer behavior—an area that has gained growing recognition in marketing literature—contemporary research increasingly suggests that a substantial portion of purchasing behavior is influenced by unconscious processes. The proposed typology (\nLiterature coverage using 3 × 3 typology.\nApplications of consumer neuroscience can be broadly categorized into two types: fundamental and applied. Fundamental applications aim to advance theoretical understanding by developing and refining models based on specific phenomena and variables within controlled environments. These studies explore the relationships between dependent and independent variables, often with potential commercial implications. In contrast, applied applications address real-world business challenges, involving real-time data collection and experimentation to inform managerial decision-making. While fundamental research emphasizes theoretical contributions, applied research is focused on generating actionable insights for business practice.\nUse cases in neuromarketing.\nAs neuromarketing continues to advance, there are several promising avenues for future research to deepen our understanding of consumer behavior. First, integrating multiple neuroimaging techniques—such as fMRI, EEG, and eye-tracking—can offer a more holistic view of the decision-making process. By combining these methods, researchers can simultaneously capture neural, physiological, and attentional data, providing richer insights into how consumers respond to various stimuli in different contexts. Second, the use of advanced machine learning algorithms and artificial intelligence can help decode complex patterns underlying consumer behavior. By applying deep learning models to multimodal data—including brain signals, eye-tracking, facial expressions, and other physiological indicators—researchers can more accurately predict purchasing decisions, preferences, and emotional reactions (Venkatraman et al.,\nThere are several underexplored areas that could emerge as important research topics in neuromarketing (\nPotential future areas.\nAs consumer behavior rapidly evolves and technology advances, the digital landscape becomes increasingly complex, presenting a significant challenge for marketers to understand the emotional and cognitive drivers behind consumer decisions. Traditional marketing methods, such as surveys and focus groups, often fail to capture the subconscious factors that influence purchasing choices. In today's fast-paced market, where attention spans are shorter and consumer options are plentiful, neuromarketing provides a valuable approach for creating more targeted, personalized, and effective marketing strategies. A systematic review of the neuromarketing literature reveals a multifaceted landscape. Previous research has employed various methodologies to explore the role of neuromarketing in marketing studies. These studies have also highlighted gaps in the literature, such as the need to clarify the definition of neuromarketing, investigate emerging subfields within the discipline, and assess the effectiveness of various neuromarketing tools (Lin et al.,\nThe findings of this review offer both theoretical and practical implications. First, our systematic analysis and synthesis of the literature contribute to empirical research, commercial applications, and theory development, helping to establish a standardized definition for neuromarketing and addressing its definitional ambiguity. Second, we explore how consumer neuroscience integrates existing theories and frameworks to better understand both intentional and actual consumer behaviors. This study provides a thorough review of how actual behavior is assessed and measured across the five stages of consumer decision-making: (1) need recognition, (2) information search, (3) evaluation of alternatives, (4) choice, and (5) post-purchase. Third, the study introduces a 2 × 3 typology that combines decision-making (conscious vs. unconscious) with the buying stages (pre-purchase, purchase, and post-purchase). This typology serves as a framework for researchers to explore underexplored areas in neuromarketing and identify gaps in the existing literature (Oliveira Í. A. et al.,\nWhile this study addresses several important aspects of the neuromarketing literature, it does have certain limitations. First, many neuromarketing studies tend to oversimplify consumer behavior by focusing mainly on neural or emotional responses, often neglecting the complex nature of decision-making, including cultural, social, and cognitive factors. Future research exploring the impact of socio-cultural influences would enrich these findings. Second, expanding neuromarketing research to more diverse and naturalistic settings is crucial. Most current studies are conducted in controlled laboratory environments, which may not fully capture real-world consumer behavior. The use of portable neuroimaging tools, such as mobile EEG, could allow researchers to study consumer responses in more dynamic, real-world contexts like retail stores, leading to more ecologically valid data. Additionally, this review used a domain-based systematic literature review approach, which primarily focuses on publication volume and theoretical perspectives. Future research could benefit from employing alternative review methods, such as theory-based reviews, method-based reviews, bibliometric analysis, or content analysis, to gain more in-depth insights into the topic. This review also included only Q1-ranked journals to ensure a consistent level of academic rigor and theoretical contribution. While this enhances the reliability of insights drawn, it may exclude emerging or interdisciplinary work published in lower-ranked journals. Future reviews could expand this scope to include Q2 and field-specific journals to capture the full breadth of evolving contributions in neuromarketing.", "content_for_embedding": "Neuromarketing, or consumer neuroscience, is an interdisciplinary field that combines neuroscience, psychology, and economics to explore and influence consumer behavior. Neuromarketing entails analyzing physiological and brain signals to examine the mind, brain, and behavior, aiming to understand, predict, and shape consumer behavior and decision-making (Harrell,\nNeuromarketing has gained prominence as a research domain since the early 2000s, marked notably by Montague et al.'s (\nThe objective of the present study is to systematically review the literature of neuromarketing based on the cognitive and affective dimensions mapped with stage specific neural correlations, based on the Preferred Reporting Items for Systematic Literature Reviews and Meta-Analyses (PRISMA) framework (Page et al.,\nRQ1: What are the stage-specific key theories, variables, methodologies, cross modal interactions and neuro-tools commonly used in neuromarketing research?\nRQ2: To what extent has neuromarketing research and neural correlates have been explored across the different stages of consumer decision-making?\nRQ3: What has been well-studied vs. under-studied at each stage, suggesting future directions, tools, and methods for a more holistic neuromarketing approach.\nThis review makes five significant contributions to the field of neuromarketing. First, our systematic analysis and synthesis of the literature according to the buying stages of decision making, incorporating stage specific neural correlates. Second, it focusses on empirical studies, commercial applications, and theoretical development, aiming to establish a standardized definition of neuromarketing and resolve its definitional ambiguity. Third, we explore how consumer neuroscience integrates established theories and frameworks to provide insights into cross modal interactions of consumer behavior and synthesize how cognitive vs. emotional processing dominates buying stages (pre-purchase, purchase, and post-purchase). Fourth, the review maps specific neural mechanisms (e.g., reward processing, attention, and emotional arousal) to individual decision stages like problem recognition, information search, evaluation, choice, and post-purchase. Finally, the study proposes a 3 × 3 typology, encompassing decision-making (conscious vs. unconscious vs. both) and buying stages (pre-purchase, purchase, and post-purchase). This typology serves as a roadmap for researchers to explore what is well-studied vs. under-studied under-explored areas in the existing literature (Li et al.,\nThis review (\nKey characteristics of past and present literature reviews on neuromarketing and consumer neuroscience.\nThe roots of neuromarketing, which emerged in the early 2000s, can be traced back to foundational psychological theories of perception, learning, and motivation (Watson,\nFurthermore, existing neuromarketing research has largely concentrated on the pre-purchase phase, examining how the brain reacts to marketing stimuli prior to a buying decision. In contrast, there has been limited exploration of its application during the purchase and post-purchase phases—stages where consumer satisfaction, brand loyalty, and advocacy may be significantly influenced by neuropsychological factors (Lin et al.,\nAnalyzing all five stages mapped to the affective, behavioral and cognitive components provides a comprehensive view of decision-making, especially since these phases do not always follow a linear progression (Lemon and Verhoef,\nIn this section, the methodology adopted for carrying out the systematic literature review process is elaborated.\nTo collect data, we utilized the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework (Page et al.,\nDue to the interdisciplinary nature of the research, we explored EBSCOhost Business Source Complete, Scopus, and Web of Science to ensure broad coverage across relevant studies. Specifically, for this review, a search was conducted on April 14, 2025, focusing on neuromarketing, consumer neuroscience and consumer decision making. This initial query yielded 3,203 potentially relevant articles.\nThe identified articles were imported into the citation management software EndNote X9, where the initial pool of 3,203 studies was refined to 338 using the Find Duplicates function. Next, exclusion criteria were applied to filter out publications unrelated to “business,” “management,” “accounting,” “engineering,” or “social science.” Articles not published in academic journals, those outside the 2020–2025 timeframe, or written in languages other than English were also excluded. As part of the inclusion criteria, only documents classified as articles or reviews were retained, with a particular emphasis on source titles relevant to the business and management fields.\nThe field of neuromarketing and consumer neuroscience is highly diverse and fragmented—not only in terms of the topics and areas explored, but also in the methodologies employed, contexts studied, and application approaches—resulting in a heterogeneous body of literature varying widely in both subject matter and quality (Ramsøy,\nPRISMA flowchart.\nThis section presents the conceptual framework guiding our review. As illustrated in\nProposed conceptual framework.\nThe pre-purchase stage encompasses all activities a consumer undertakes prior to making a purchase, including need recognition, information gathering, and evaluating available alternatives (Lemon and Verhoef,\nIn the pre-purchase phase, consumers actively seek information and evaluate various options, which may involve comparing multiple products or concentrating on a particular one (Lee et al.,\nIn the purchase decision phase, consumers make several critical choices, including whether to proceed with the purchase, which product(s) to select, how much to buy, which retailer to choose, and the best time to make the purchase. Other factors, such as delivery times for online orders, also influence decisions in this stage (Lee et al.,\nAfter purchasing a product, consumers often assess their experience in comparison to their initial expectations (Lee et al.,\nIn neuromarketing, post-purchase behavior is examined by analyzing how consumers evaluate their purchase afterward, with a focus on emotional and cognitive responses. For example, Hamelin et al. (\nWe suggest that actual consumer behavior across decision-making stages (\nGiven the definitional ambiguity surrounding neuromarketing and consumer neuroscience (Khamitov et al.,\nSeminal theories and their mapping with purchase stages and attitudinal responses.\nA comprehensive evaluation of the relative strengths and limitations of neuroscience research methods is often missing from the neuromarketing literature. Most studies tend to offer only brief descriptions of the specific neuroscience techniques they employ, without delving into their broader functional capabilities. As noted by Harris et al. (\nNeuroscientific tools, their applications and measurement variables.\nConsumer research represents one of the most prominent areas where neuroscientific methods have been applied extensively (Lim,\nMeasurement indicators.\nMarketing focuses on influencing consumer perceptions and decisions, while neuroscientific tools enable us to gain insights into the brain's responses to marketing stimuli. Three methods are typically employed to gauge customers' reactions: behavioral measures, self-reports, and psychophysiological tests (Hsu and Chen,\nFrom the consumer's standpoint, marketing and consumer research have increasingly employed both neurometric (e.g., fMRI, EEG, MEG, SST, TMS, fNIRS, and PET) and non-neurometric (e.g., Eye Tracking, Galvanic Skin Response, Facial Action Coding, facial EMG, Heart Rate, and Infrared Thermography) techniques to better understand behavior—particularly decision-making processes. These tools enable researchers to uncover cognitive, emotional, and behavioral responses, helping to interpret, explain, and address barriers to acceptance and action regarding various issues. The emergence of interdisciplinary fields such as neurophilosophy, neuroeconomics (Sanfey et al.,\nInstruments and techniques utilized in reviewed studies.\nThe study introduces a 3 × 3 typology (\nProposed 3 × 3 typology.\nGiven the critical role of unconscious decision-making in consumer behavior—an area that has gained growing recognition in marketing literature—contemporary research increasingly suggests that a substantial portion of purchasing behavior is influenced by unconscious processes. The proposed typology (\nLiterature coverage using 3 × 3 typology.\nApplications of consumer neuroscience can be broadly categorized into two types: fundamental and applied. Fundamental applications aim to advance theoretical understanding by developing and refining models based on specific phenomena and variables within controlled environments. These studies explore the relationships between dependent and independent variables, often with potential commercial implications. In contrast, applied applications address real-world business challenges, involving real-time data collection and experimentation to inform managerial decision-making. While fundamental research emphasizes theoretical contributions, applied research is focused on generating actionable insights for business practice.\nUse cases in neuromarketing.\nAs neuromarketing continues to advance, there are several promising avenues for future research to deepen our understanding of consumer behavior. First, integrating multiple neuroimaging techniques—such as fMRI, EEG, and eye-tracking—can offer a more holistic view of the decision-making process. By combining these methods, researchers can simultaneously capture neural, physiological, and attentional data, providing richer insights into how consumers respond to various stimuli in different contexts. Second, the use of advanced machine learning algorithms and artificial intelligence can help decode complex patterns underlying consumer behavior. By applying deep learning models to multimodal data—including brain signals, eye-tracking, facial expressions, and other physiological indicators—researchers can more accurately predict purchasing decisions, preferences, and emotional reactions (Venkatraman et al.,\nThere are several underexplored areas that could emerge as important research topics in neuromarketing (\nPotential future areas.\nAs consumer behavior rapidly evolves and technology advances, the digital landscape becomes increasingly complex, presenting a significant challenge for marketers to understand the emotional and cognitive drivers behind consumer decisions. Traditional marketing methods, such as surveys and focus groups, often fail to capture the subconscious factors that influence purchasing choices. In today's fast-paced market, where attention spans are shorter and consumer options are plentiful, neuromarketing provides a valuable approach for creating more targeted, personalized, and effective marketing strategies. A systematic review of the neuromarketing literature reveals a multifaceted landscape. Previous research has employed various methodologies to explore the role of neuromarketing in marketing studies. These studies have also highlighted gaps in the literature, such as the need to clarify the definition of neuromarketing, investigate emerging subfields within the discipline, and assess the effectiveness of various neuromarketing tools (Lin et al.,\nThe findings of this review offer both theoretical and practical implications. First, our systematic analysis and synthesis of the literature contribute to empirical research, commercial applications, and theory development, helping to establish a standardized definition for neuromarketing and addressing its definitional ambiguity. Second, we explore how consumer neuroscience integrates existing theories and frameworks to better understand both intentional and actual consumer behaviors. This study provides a thorough review of how actual behavior is assessed and measured across the five stages of consumer decision-making: (1) need recognition, (2) information search, (3) evaluation of alternatives, (4) choice, and (5) post-purchase. Third, the study introduces a 2 × 3 typology that combines decision-making (conscious vs. unconscious) with the buying stages (pre-purchase, purchase, and post-purchase). This typology serves as a framework for researchers to explore underexplored areas in neuromarketing and identify gaps in the existing literature (Oliveira Í. A. et al.,\nWhile this study addresses several important aspects of the neuromarketing literature, it does have certain limitations. First, many neuromarketing studies tend to oversimplify consumer behavior by focusing mainly on neural or emotional responses, often neglecting the complex nature of decision-making, including cultural, social, and cognitive factors. Future research exploring the impact of socio-cultural influences would enrich these findings. Second, expanding neuromarketing research to more diverse and naturalistic settings is crucial. Most current studies are conducted in controlled laboratory environments, which may not fully capture real-world consumer behavior. The use of portable neuroimaging tools, such as mobile EEG, could allow researchers to study consumer responses in more dynamic, real-world contexts like retail stores, leading to more ecologically valid data. Additionally, this review used a domain-based systematic literature review approach, which primarily focuses on publication volume and theoretical perspectives. Future research could benefit from employing alternative review methods, such as theory-based reviews, method-based reviews, bibliometric analysis, or content analysis, to gain more in-depth insights into the topic. This review also included only Q1-ranked journals to ensure a consistent level of academic rigor and theoretical contribution. While this enhances the reliability of insights drawn, it may exclude emerging or interdisciplinary work published in lower-ranked journals. Future reviews could expand this scope to include Q2 and field-specific journals to capture the full breadth of evolving contributions in neuromarketing.", "topic": "Brain"}
{"pmid": "38555987", "pmcid": "12307428", "title": "Classification of finger movements through optimal EEG channel and feature selection", "publication_year": "N/A", "abstract": "", "full_text": "Brain-computer interface (BCI) systems, which are roughly a hardware and software interaction system, convert brain signals into commands to operate external equipment (Nicolas-Alonso & Gomez-Gil,\nBCI applications can be improved using several control signals (Nicolas-Alonso & Gomez-Gil,\nEEG signals include temporal, spectral (frequency domain-based and time-frequency domain-based features), spatial, and nonlinear features that can be utilized for the classification of finger movements. In BCI design, the feature extraction step, especially the selection of the feature domain, is one of the profound steps in research because the extracted features directly affect the classification performance (Riaz et al.,\nSeveral feature domains and feature types can be extracted from EEG signals to be utilized for finger movement classification in the literature (Degirmenci et al.,\nNumerous classifiers such as decision tree (DT) (Degirmenci et al.,\nDespite the difficulties mentioned above, a few BCI system design studies have been reported in the literature to classify finger movements. There is a need to develop these systems by overcoming these difficulties due to the general structure of finger movements in processing EEG signals through the formulation of new approaches. To date, researchers have concentrated on certain EEG signal analysis methods for finger movement classification. These methods can be listed as follows: (i) some definite temporal features as mentioned above, Fourier transform-based spectral features, and common spatial pattern-based spatial features have been mostly computed through respective methods for the feature extraction step, (ii) several feature selection methods that are complex and are not easy to use have been preferred for feature selection step, and (iii) SVM-based and deep learning-based classification methods have been mostly preferred for classification step. Thus, there is a growing need to analyze different feature domains and different feature types from these domains. Considering the computational load on the BCI systems, it would be better to choose a feature selection method that is effective and easy to use for the feature selection step. From another perspective, in the last decade, deep learning methods, which constantly gain popularity in EEG analysis, have been used to classify finger movements. However, these models have some important drawbacks: (i) deep learning methods need a vast amount of data, (ii) they are not as simple and interpretable as machine learning algorithms, (iii) these systems are identified as “closed black boxes” and are deprivation of transparency, and (iv) training process for deep learning models causes high computational load in real-time BCI system design (Rashid et al.,\nThis study aims to implement the detailed feature selection and analysis of channel activity in the field of finger movement classification research. Toward this aim, several feature domains, including time domain, frequency domain, time-frequency domain, and nonlinear domain, are used to extract features from raw EEG signals. Four different feature sets are created by extracting different types of features from the relevant feature domains. The effects of these feature sets are investigated separately. The statistical significance-based feature selection method is used to select the most important features. The effect of the feature selection method on each extracted feature set is investigated individually. Feature and channel analysis are performed using statistically significant feature distribution maps, which represent the selected statistically significant feature distribution among 19 EEG channels for each feature set (time domain, frequency domain, time-frequency domain, and nonlinear domain). Several machine learning algorithms are employed for the classification of finger movements. Hence, this study stands out because it pioneers the combined use of detailed features and channel activity analysis. This approach offers a novel perspective in the field of finger movement classification.\nThe following are the main contributions of this study:\nMultiple effective feature domains such as time, frequency, time-frequency, and nonlinear domains were evaluated for the feature extraction process and the effect of these feature sets was analyzed for finger movement classification separately, one by one. In addition, various combinations of these features were provided and investigated for the classification of finger movement.\nThe impact of the statistical significance-based feature selection method was analyzed individually for each feature set in relation to the classification of finger movement.\nFeature analysis and channel analysis were conducted by examining the distribution of statistically significant features across 19 EEG channels for both subject-dependent and subject-independent finger movement classifications.\nVarious classifier algorithms were comparatively used to evaluate the performance of the sets of extracted features.\nTo observe the effect of the brain's resting state, i.e., no mental task (NoMT) case in EEG signals, it was implemented as a class to finger movements' EEG signals in the experimental analysis.\nFinally, it is important to highlight that this is the first study to implement, for each feature set, a detailed feature analysis and a channel analysis collectively, alongside investigating the effects of different feature domains on finger movement classification. Additionally, Poincaré plot-based nonlinear features are used for the first time to classify finger movements, to the best of the authors' knowledge.\nHere is the organization of the remaining sections of this paper: In the “Material and methods” section, details of the dataset in use, feature extraction step, statistically significance-based feature selection process, classification process, and performance evaluation metrics are given. Experimental results of the methods used and discussion of these results are reported in the “Results and Discussion” section. Finally, the main findings of the study are outlined and described in the “Conclusion” section.\nThe aim of this study is threefold: to show the advantages of (i) multidirectional EEG analysis which is performed using different feature domains, statistically significance-based feature selection, and various machine learning algorithms, (ii) detailed feature analysis performed using statistically significant feature distribution maps, and (iii) effective channel analysis. Hence, this study was designed and conducted in 4 main sequential stages. These are (i) EEG dataset acquisition, (ii) extraction of features from different domains such as time domain, frequency domain, time-frequency domain, and nonlinear domain, (iii) statistical significance-based feature selection process, (iv) classification, and performance evaluation. The flowchart illustrating the methodology employed in the proposed finger movement classification study is given in\nThe schematic representation of the proposed methodology for classifying finger movements. EEG signals are segmented into 1-s intervals for the feature extraction process, including time, frequency, time-frequency, and nonlinear domain features. Various well-known classifiers are evaluated to distinguish BCI commands based on the extracted features. The dashed path in the “Feature Selection” block denotes an alternative analysis, where ANOVA is used to select statistically significant features that replace the full set as inputs to the classifiers.\nIn this study, EEG signals are provided from a large electroencephalographic MI dataset for EEG-based BCIs which are recorded and presented by Kaya et al. (\nTo obtain relevant and significant information about EEG signals, the types of EEG features, and the domain of these features are crucial in the feature extraction step. In the literature, spatial features, especially those that are extracted through CSP, have been the most adopted and computed features for finger movement classification. Although, by researchers, spatial features are being utilized the most, we used temporal, spectral, and nonlinear features and investigated their effectiveness for finger movement classification.\nAt first, using time-domain characteristics of EEG signals, 24 different temporal features were computed separately for all EEG segments. These features present information related to the statistical variations of the EEG signals and their amplitude. Section 3 includes information on the pertinent and distinctive MI time-domain features that we adopted to classify finger movements. “\nFrequency information embedded in EEG signals was used and spectral features were evaluated during the feature extraction process based on the frequency domain for classification of finger movement. In this direction, the frequency characteristics of the MI EEG signals were obtained on the basis of a fast Fourier Transform. Using these frequency representations, several oscillations that are included within EEG signals such as delta (δ), theta (θ), alpha (α), beta (β), and gamma (γ) waves were obtained. The frequency ranges of these bands is as follows: (i) δ (0.5–4 Hz), (ii) θ (4–8 Hz), (iii), α (8–13 Hz), (iv) β (13–30 Hz), and (v) γ (30–100 Hz). The spectral features were obtained by computing the energy, variance, and spectral entropy values of the extracted frequency bands. These features offer insights into how energy, variance, and irregularity (entropy) vary across the respective frequency bands. These spectral features are evaluated as following\nwhere, f denotes the related frequency band that is used to calculate energy, variance, and spectral entropy values. \"M\" represents the maximum frequency. “y” denotes the Fourier transform of the related EEG signal and the average value of “y” is denoted as “\nSection 3 shows the information about the relevant and distinctive MI frequency-domain features that we adopted for the classification of finger movements. ”\nWithin the scope of time-frequency-domain feature extraction, the WT is utilized as a time-frequency representation technique to effectively capture and evaluate the underlying time-frequency characteristics of the EEG signals. WT provides both time and frequency information about the EEG signals. Multi-resolution analysis is carried out using its several filters and bandwidths (Sayilgan et al.,\nWavelet packet decomposition is used to evaluate five distinct EEG sub-bands (δ, θ, α, β, and γ) of each EEG segment. “Haar” mother wavelet and 9-level sub-band decomposition\nUsing the following mathematical formulations (\nHere, the detail (\nThe variance value for each decomposition level is computed as follows:\nHere, μ\nThe entropy at each decomposition level is calculated as follows\nSection 3 provides information on the pertinent and distinctive MI time-frequency-domain features that we used for the classification of finger movements. The abbreviation\nIn the nonlinear domain feature extraction process, Poincare plot measures are used to capture nonlinear characteristics present in EEG signals. A Poincare plot, a technique derived from nonlinear dynamics, is constructed by plotting each data on the x-axis against subsequent data on the y-axis (Isler,\nHere, the standard deviation of successive differences is denoted as\nSection 3 supplies the information about the relevant and distinctive MI nonlinear domain features that we used for finger movement classification. The abbreviation\nIn the feature selection process, the most discriminative and relevant EEG features were selected using the feature selection method based on statistical significance. Among the statistical significance-based feature selection methods, One-way variance analysis (ANOVA test) was used (Bulut et al.,\nFollowing the extraction and selection of the different MI EEG features, various machine learning algorithms, including Decision Tree (DT) (Tzallas et al.,\nHere, the number of correctly predicted EEG segments of finger movements is denoted as TP and TN. On the other hand, the number of EEG segments of finger movements that are incorrectly predicted is denoted as FP and FN.\nIn this study, the classification performance was evaluated for both subject-dependent and subject-independent scenarios across six different feature sets. Feature selection based on statistical significance was applied to enhance model performance. A comparative analysis was conducted using eight machine learning algorithms. Using 19-channel EEG signals, four distinct feature sets were extracted. These feature sets encompass information from four distinct feature domains: time-domain, frequency-domain, time-frequency-domain, and nonlinear domain. In addition to the four primary feature sets, combinations of feature sets derived from different feature domains were also constructed and examined to enhance the classification of finger movements. The impact of each feature set was examined individually. Furthermore, the impact of the ANOVA-based feature selection method was evaluated independently for each feature set. The relevant and discriminative features of all feature sets were selected using ANOVA and the reduced versions of all feature sets were generated to analyze the effectiveness of the ANOVA-based feature selection method. For six different feature sets, all EEG features in their original form, without undergoing any feature selection procedure and the relevant and discriminative EEG features selected from all features were evaluated using different classification algorithms.\nThe number of all extracted features and ANOVA-selected features for both subject-dependent and subject-independent finger movement classifications is presented in\nThe sizes of all feature sets, along with the number of features selected using the statistical significance-based feature selection method for finger movement classification, are presented.\nThe feature sets under consideration include TD (time-domain feature set), FD (frequency-domain feature set), TF (time-frequency-domain feature set), and ND (nonlinear domain feature set).\nAll classifiers' performance was tested using both the time-domain features and the ANOVA-selected time-domain features considered in this study, with the results provided in\nAll classifiers' performances were evaluated in this study using time-domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the time-domain feature set.\nAbbreviations of time-domain features are TD1 (Minumum value), TD2 (Maximum value), TD3 (Mean), TD4 (Standard deviation value), TD5 (Integrated EEG value), TD6 (Mean absolute value), TD7 (Simple square integral), TD8 (Variance), TD9 (Root mean square), TD10 (Waveform length), TD11 (Average amplitude change value), TD12 (Absolute difference in standard deviation), TD13 (Kurtosis), TD14 (Skewness), TD15 (Hjorth parameters (Activity)), TD16 (Hjorth parameters (Mobility)), TD17 (Hjorth parameters (Complexity)), TD18 (Signal range), TD19 (First inter-quartile value (Q1)), TD20 (Second inter-quartile value (Q2)), TD21 (Third inter-quartile value (Q3)), TD22 (Mode value), TD23 (Zero-crossing value), and TD24 (Slope-change value).\nThe distribution of statistically significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the time-domain feature set.\nAbbreviations of time-domain features are TD1 (Minumum value), TD2 (Maximum value), TD3 (Mean), TD4 (Standard deviation), TD5 (Integrated EEG value), TD6 (Mean absolute value), TD7 (Simple square integral), TD8 (Variance), TD9 (Root mean square), TD10 (Waveform length), TD11 (Average amplitude change value), TD12 (Absolute difference in standard deviation), TD13 (Kurtosis), TD14 (Skewness), TD15 (Hjorth parameters (Activity)), TD16 (Hjorth parameters (Mobility)), TD17 (Hjorth parameters (Complexity)), TD18 (Signal range), TD19 (First inter-quartile value (Q1)), TD20 (Second inter-quartile value (Q2)), TD21 (Third inter-quartile value (Q3)), TD22 (Mode value), TD23 (Zero-crossing value), and TD24 (Slope-change value).\nAll classification results obtained using frequency-domain analysis are presented in\nAll classifiers' performances were evaluated in this study using frequency-domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the frequency-domain feature set.\nAbbreviations of frequency-domain features are FD1 (Energy of delta band), FD2 (Variance of delta band), FD3 (Entropy of delta band), FD4 (Energy of theta band), FD5 (Variance of theta band), FD6 (Entropy of theta band), FD7 (Energy of alpha band), FD8 (Variance of alpha band), FD9 (Entropy of alpha band), FD10 (Energy of beta band), FD11 (Variance of beta band), FD12 (Entropy of beta band), FD13 (Energy of gamma band), FD14 (Variance of gamma band), and FD15 (Entropy of gamma band).\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the frequency-domain feature set.\nAbbreviations of frequency-domain features are FD1 (Energy of delta band), FD2 (Variance of delta band), FD3 (Entropy of delta band), FD4 (Energy of theta band), FD5 (Variance of theta band), FD6 (Entropy of theta band), FD7 (Energy of alpha band), FD8 (Variance of alpha band), FD9 (Entropy of alpha band), FD10 (Energy of beta band), FD11 (Variance of beta band), FD12 (Entropy of beta band), FD13 (Energy of gamma band), FD14 (Variance of gamma band), and FD15 (Entropy of gamma band).\nClassification results performed using time-frequency-domain features are presented in\nAll classifiers' performances were evaluated in this study using time-frequency-domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the time-frequency-domain feature set.\nAbbreviations of time-frequency-domain features are TF1 (Energy of delta band), TF2 (Variance of delta band), TF3 (Entropy of delta band), TF4 (Energy of theta band), TF5 (Variance of theta band), TF6 (Entropy of theta band), TF7 (Energy of alpha band), TF8 (Variance of alpha band), TF9 (Entropy of alpha band), TF10 (Energy of beta band), TF11 (Variance of beta band), TF12 (Entropy of beta band), TF13 (Energy of gamma band), TF14 (Variance of gamma band), and TF15 (Entropy of gamma band).\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the time-frequency-domain feature set.\nAbbreviations of time-frequency-domain features are TF1 (Energy of delta band), TF2 (Variance of delta band), TF3 (Entropy of delta band), TF4 (Energy of theta band), TF5 (Variance of theta band), TF6 (Entropy of theta band), TF7 (Energy of alpha band), TF8 (Variance of alpha band), TF9 (Entropy of alpha band), TF10 (Energy of beta band), TF11 (Variance of beta band), TF12 (Entropy of beta band), TF13 (Energy of gamma band), TF14 (Variance of gamma band), and TF15 (Entropy of gamma band).\nThe test classification accuracies obtained based on the nonlinear feature set are presented in\nAll classifiers' performances were evaluated in this study using nonlinear domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the nonlinear domain feature set.\nAbbreviations of nonlinear domain features are ND1 (\nThe distribution of statistically significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the nonlinear domain feature set.\nAbbreviations of nonlinear domain features are ND1 (\nAll classifiers' performances were evaluated in this study using the first combination (TD+FD+TF) feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nAll classifiers' performances were evaluated in this study using the second combination (TD+FD+TF+ND) feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe primary findings and distinctional aspects of our study on finger movement classification are summarized below:\nWe extracted and investigated the different MI EEG features using time-domain, frequency-domain, time-frequency domain, and nonlinear domain of EEG signals.\nThis work represents, to the authors' knowledge, the first attempt to apply nonlinear features extracted from Poincare plots in the context of finger movement classification.\nOf all the feature sets analyzed, the second combination (TD+FD+TF+ND), when used with ANOVA, yielded the highest classification performance across both subject-independent and subject-dependent evaluations. Also, the lowest classification performances were mostly provided using Wavelet Transform-based time-frequency features in both cases (\nSeveral machine learning algorithms were deployed to classify EEG features. Among all algorithms, the highest classification accuracies were generally obtained with SVM algorithm in all feature sets.\nAccording to experimental results in all feature sets, the ANOVA-based feature selection method mostly improved prediction performance in the majority of machine learning algorithms (\nContrary to previous studies in the literature that neglect the brain's no mental task condition, we carried out a 6-class classification of finger movements by including the NoMT case instead of excluding the brain's idle state because we aim to propose a more realastic BCI system design. The classification performance of our 6-class finger movement classification study, which we conducted especially for the subject-dependent condition, is superior to the 5-class finger movement classification studies in the literature that eliminate the NoMT condition.\nA comprehensive evaluation of features and EEG channels was performed based on distribution maps highlighting statistically significant features - this approach, proposed for the first time in the context of finger movement classification - is employed to determine efficient and distinctive features and EEG channels.\nOur EEG feature and channel investigation has shown that the ANOVA test selected statistically significant features from some certain EEG frequency bands, and feature types whose effectiveness has been demonstrated in the BCI research field. These feature selections improved the classification performances in some of the analyzed feature sets (especially in frequency domain feature set). Also, in some feature sets (especially in time-domain, time-frequency domain, and nonlinear domain feature sets, used for subject-independent analysis), ANOVA did not prioritize specific EEG channels, frequency bands, and feature types during the feature reduction process. It was concluded that classification performance could be increased with this balanced distribution in some feature sets (especially in nonlinear feature set for subject-dependent analysis). Further analysis of the results revealed that classification performance can improve when using EEG channels and features that have not yet been established as effective in the literature. These findings highlight the critical role of effective feature and channel analysis in enhancing classifier performance.\nComparison of classification accuracies obtained using all features vs. ANOVA-selected features across all feature sets with the Support Vector Machine (SVM) classifier for Subject E (S4).\nComparison of classification accuracies obtained using all features vs. ANOVA-selected features across all feature sets with the Support Vector Machine (SVM) classifier in subject-independent analysis.\nBeyond these findings, it is important to consider the broader context in which finger movement classification operates, particularly in relation to functional brain networks and neurological disorders. Recent advances in EEG-based BCIs have demonstrated that motor imagery can be effectively used to decode intended movements without any physical action (Pfurtscheller & Neuper,\nWe compare the performance of the proposed finger movement classification with that of recent studies that use same EEG dataset with different feature extraction and classification algorithms. The details of these studies are shown in\nA comparison of the accuracy of this method with several state-of-the-art studies, for both subject-independent and subject-dependent cases, within this field.\nThe abbreviation “N” represents the “number of subjects,” “n” denotes the “number of EEG channels,” “c” refers to the “number of classes,” and “CV” stands for the “Cross-Validation Method”. The classifiers used are CNN (Convolutional Neural Network), RF (Random Forest), ADL (Autonomous Deep Learning), SVM (Support Vector Machine), EEGNet (EEGNet Deep Learning Model), BLS (Bi-layered Long-Short Classifier), and EL (Ensemble Learning). Features are PSD (power spectral density), FT (Fourier transfrom), ITD (intrinsic time-scale decomposition), EMD (empirical mode decomposition), and CSP (common spatial pattern).\nIn this study, the effects of different feature domains and a statistical significance-based feature selection method are investigated to classify finger movements. First, several EEG features of EEG segments are obtained from four different feature domains, including time-domain, frequency-domain, time-frequency, and nonlinear domain. In addition to these feature sets, two different combinations of features from multiple domains were investigated. Hence, a total of 1102 EEG features are calculated from four different feature domains, and six feature sets are generated for our finger movement analysis. By applying the statistical significance-based feature selection method, relevant and significant – hence fewer – EEG features are determined and selected from each feature set separately. All features obtained in six different feature sets and the statistically significant reduced feature sets were tested with various machine learning algorithms. The applied methods were tested on two different classification cases (subject-dependent and subject-independent classification). The results showed that the highest accuracy rates of 39.30% and 59.17% were obtained using the second combination feature set (TD + FD + TF + ND) and the SVM classifier in subject-independent and subject-dependent classifications, respectively. The selected EEG features (which are (i) energy and variance of five frequency bands in frequency-domain feature set, (ii) all feature types in time domain, time-frequency-domain, and nonlinear domain feature sets) and all EEG channels resulted in maximum accuracy of 59.17% with the SVM classifier for subject-dependent analysis.\nFor subject-independent analysis, the selected EEG features included (i) all feature types except waveform length, average amplitude change value, absolute difference in standard deviation, and slope-change value in the time-domain feature set; (ii) energy and variance values of all frequency bands except the gamma band in the frequency-domain feature set; (iii) entropy values of five frequency bands in the time-frequency-domain feature set; and (iv)\nOur main goal in this study is not to provide the best classifier performance but to investigate and indicate the discriminative EEG channels and features for the classification of finger movements. Toward this aim, the EEG channel and feature analysis was performed using channel-based statistical feature distribution maps. Particularly, the detailed feature and EEG channel analysis was implemented for the first time to classify finger movements along with various feature domains and their different combinations to the best of our knowledge. The EEG channel and feature analysis in finger movement classification indicates that it works well and supports literature in choosing specific frequency bands, and features from some feature domains. These specific selections have also been demonstrated to be effective in previous studies of prediction of motor imagery tasks based on EEG. It has been observed that selections are performed from features and channels that are not proven in the literature and that this improves the classification performance. Conversely, performance was also improved by balanced selections across all channels and features. Therefore, our experimental analysis suggests that classifier performance may be improved by implementing detailed feature and EEG channel analysis using a feature selection method based on statistical significance. Nonetheless, in this study, we only used an EEG dataset recorded from 13 healthy subjects, and each EEG data consists of 19 EEG channels sampled at 1000 Hz. To draw a more general conclusion, a dataset covering more subjects is necessary.", "content_for_embedding": "Brain-computer interface (BCI) systems, which are roughly a hardware and software interaction system, convert brain signals into commands to operate external equipment (Nicolas-Alonso & Gomez-Gil,\nBCI applications can be improved using several control signals (Nicolas-Alonso & Gomez-Gil,\nEEG signals include temporal, spectral (frequency domain-based and time-frequency domain-based features), spatial, and nonlinear features that can be utilized for the classification of finger movements. In BCI design, the feature extraction step, especially the selection of the feature domain, is one of the profound steps in research because the extracted features directly affect the classification performance (Riaz et al.,\nSeveral feature domains and feature types can be extracted from EEG signals to be utilized for finger movement classification in the literature (Degirmenci et al.,\nNumerous classifiers such as decision tree (DT) (Degirmenci et al.,\nDespite the difficulties mentioned above, a few BCI system design studies have been reported in the literature to classify finger movements. There is a need to develop these systems by overcoming these difficulties due to the general structure of finger movements in processing EEG signals through the formulation of new approaches. To date, researchers have concentrated on certain EEG signal analysis methods for finger movement classification. These methods can be listed as follows: (i) some definite temporal features as mentioned above, Fourier transform-based spectral features, and common spatial pattern-based spatial features have been mostly computed through respective methods for the feature extraction step, (ii) several feature selection methods that are complex and are not easy to use have been preferred for feature selection step, and (iii) SVM-based and deep learning-based classification methods have been mostly preferred for classification step. Thus, there is a growing need to analyze different feature domains and different feature types from these domains. Considering the computational load on the BCI systems, it would be better to choose a feature selection method that is effective and easy to use for the feature selection step. From another perspective, in the last decade, deep learning methods, which constantly gain popularity in EEG analysis, have been used to classify finger movements. However, these models have some important drawbacks: (i) deep learning methods need a vast amount of data, (ii) they are not as simple and interpretable as machine learning algorithms, (iii) these systems are identified as “closed black boxes” and are deprivation of transparency, and (iv) training process for deep learning models causes high computational load in real-time BCI system design (Rashid et al.,\nThis study aims to implement the detailed feature selection and analysis of channel activity in the field of finger movement classification research. Toward this aim, several feature domains, including time domain, frequency domain, time-frequency domain, and nonlinear domain, are used to extract features from raw EEG signals. Four different feature sets are created by extracting different types of features from the relevant feature domains. The effects of these feature sets are investigated separately. The statistical significance-based feature selection method is used to select the most important features. The effect of the feature selection method on each extracted feature set is investigated individually. Feature and channel analysis are performed using statistically significant feature distribution maps, which represent the selected statistically significant feature distribution among 19 EEG channels for each feature set (time domain, frequency domain, time-frequency domain, and nonlinear domain). Several machine learning algorithms are employed for the classification of finger movements. Hence, this study stands out because it pioneers the combined use of detailed features and channel activity analysis. This approach offers a novel perspective in the field of finger movement classification.\nThe following are the main contributions of this study:\nMultiple effective feature domains such as time, frequency, time-frequency, and nonlinear domains were evaluated for the feature extraction process and the effect of these feature sets was analyzed for finger movement classification separately, one by one. In addition, various combinations of these features were provided and investigated for the classification of finger movement.\nThe impact of the statistical significance-based feature selection method was analyzed individually for each feature set in relation to the classification of finger movement.\nFeature analysis and channel analysis were conducted by examining the distribution of statistically significant features across 19 EEG channels for both subject-dependent and subject-independent finger movement classifications.\nVarious classifier algorithms were comparatively used to evaluate the performance of the sets of extracted features.\nTo observe the effect of the brain's resting state, i.e., no mental task (NoMT) case in EEG signals, it was implemented as a class to finger movements' EEG signals in the experimental analysis.\nFinally, it is important to highlight that this is the first study to implement, for each feature set, a detailed feature analysis and a channel analysis collectively, alongside investigating the effects of different feature domains on finger movement classification. Additionally, Poincaré plot-based nonlinear features are used for the first time to classify finger movements, to the best of the authors' knowledge.\nHere is the organization of the remaining sections of this paper: In the “Material and methods” section, details of the dataset in use, feature extraction step, statistically significance-based feature selection process, classification process, and performance evaluation metrics are given. Experimental results of the methods used and discussion of these results are reported in the “Results and Discussion” section. Finally, the main findings of the study are outlined and described in the “Conclusion” section.\nThe aim of this study is threefold: to show the advantages of (i) multidirectional EEG analysis which is performed using different feature domains, statistically significance-based feature selection, and various machine learning algorithms, (ii) detailed feature analysis performed using statistically significant feature distribution maps, and (iii) effective channel analysis. Hence, this study was designed and conducted in 4 main sequential stages. These are (i) EEG dataset acquisition, (ii) extraction of features from different domains such as time domain, frequency domain, time-frequency domain, and nonlinear domain, (iii) statistical significance-based feature selection process, (iv) classification, and performance evaluation. The flowchart illustrating the methodology employed in the proposed finger movement classification study is given in\nThe schematic representation of the proposed methodology for classifying finger movements. EEG signals are segmented into 1-s intervals for the feature extraction process, including time, frequency, time-frequency, and nonlinear domain features. Various well-known classifiers are evaluated to distinguish BCI commands based on the extracted features. The dashed path in the “Feature Selection” block denotes an alternative analysis, where ANOVA is used to select statistically significant features that replace the full set as inputs to the classifiers.\nIn this study, EEG signals are provided from a large electroencephalographic MI dataset for EEG-based BCIs which are recorded and presented by Kaya et al. (\nTo obtain relevant and significant information about EEG signals, the types of EEG features, and the domain of these features are crucial in the feature extraction step. In the literature, spatial features, especially those that are extracted through CSP, have been the most adopted and computed features for finger movement classification. Although, by researchers, spatial features are being utilized the most, we used temporal, spectral, and nonlinear features and investigated their effectiveness for finger movement classification.\nAt first, using time-domain characteristics of EEG signals, 24 different temporal features were computed separately for all EEG segments. These features present information related to the statistical variations of the EEG signals and their amplitude. Section 3 includes information on the pertinent and distinctive MI time-domain features that we adopted to classify finger movements. “\nFrequency information embedded in EEG signals was used and spectral features were evaluated during the feature extraction process based on the frequency domain for classification of finger movement. In this direction, the frequency characteristics of the MI EEG signals were obtained on the basis of a fast Fourier Transform. Using these frequency representations, several oscillations that are included within EEG signals such as delta (δ), theta (θ), alpha (α), beta (β), and gamma (γ) waves were obtained. The frequency ranges of these bands is as follows: (i) δ (0.5–4 Hz), (ii) θ (4–8 Hz), (iii), α (8–13 Hz), (iv) β (13–30 Hz), and (v) γ (30–100 Hz). The spectral features were obtained by computing the energy, variance, and spectral entropy values of the extracted frequency bands. These features offer insights into how energy, variance, and irregularity (entropy) vary across the respective frequency bands. These spectral features are evaluated as following\nwhere, f denotes the related frequency band that is used to calculate energy, variance, and spectral entropy values. \"M\" represents the maximum frequency. “y” denotes the Fourier transform of the related EEG signal and the average value of “y” is denoted as “\nSection 3 shows the information about the relevant and distinctive MI frequency-domain features that we adopted for the classification of finger movements. ”\nWithin the scope of time-frequency-domain feature extraction, the WT is utilized as a time-frequency representation technique to effectively capture and evaluate the underlying time-frequency characteristics of the EEG signals. WT provides both time and frequency information about the EEG signals. Multi-resolution analysis is carried out using its several filters and bandwidths (Sayilgan et al.,\nWavelet packet decomposition is used to evaluate five distinct EEG sub-bands (δ, θ, α, β, and γ) of each EEG segment. “Haar” mother wavelet and 9-level sub-band decomposition\nUsing the following mathematical formulations (\nHere, the detail (\nThe variance value for each decomposition level is computed as follows:\nHere, μ\nThe entropy at each decomposition level is calculated as follows\nSection 3 provides information on the pertinent and distinctive MI time-frequency-domain features that we used for the classification of finger movements. The abbreviation\nIn the nonlinear domain feature extraction process, Poincare plot measures are used to capture nonlinear characteristics present in EEG signals. A Poincare plot, a technique derived from nonlinear dynamics, is constructed by plotting each data on the x-axis against subsequent data on the y-axis (Isler,\nHere, the standard deviation of successive differences is denoted as\nSection 3 supplies the information about the relevant and distinctive MI nonlinear domain features that we used for finger movement classification. The abbreviation\nIn the feature selection process, the most discriminative and relevant EEG features were selected using the feature selection method based on statistical significance. Among the statistical significance-based feature selection methods, One-way variance analysis (ANOVA test) was used (Bulut et al.,\nFollowing the extraction and selection of the different MI EEG features, various machine learning algorithms, including Decision Tree (DT) (Tzallas et al.,\nHere, the number of correctly predicted EEG segments of finger movements is denoted as TP and TN. On the other hand, the number of EEG segments of finger movements that are incorrectly predicted is denoted as FP and FN.\nIn this study, the classification performance was evaluated for both subject-dependent and subject-independent scenarios across six different feature sets. Feature selection based on statistical significance was applied to enhance model performance. A comparative analysis was conducted using eight machine learning algorithms. Using 19-channel EEG signals, four distinct feature sets were extracted. These feature sets encompass information from four distinct feature domains: time-domain, frequency-domain, time-frequency-domain, and nonlinear domain. In addition to the four primary feature sets, combinations of feature sets derived from different feature domains were also constructed and examined to enhance the classification of finger movements. The impact of each feature set was examined individually. Furthermore, the impact of the ANOVA-based feature selection method was evaluated independently for each feature set. The relevant and discriminative features of all feature sets were selected using ANOVA and the reduced versions of all feature sets were generated to analyze the effectiveness of the ANOVA-based feature selection method. For six different feature sets, all EEG features in their original form, without undergoing any feature selection procedure and the relevant and discriminative EEG features selected from all features were evaluated using different classification algorithms.\nThe number of all extracted features and ANOVA-selected features for both subject-dependent and subject-independent finger movement classifications is presented in\nThe sizes of all feature sets, along with the number of features selected using the statistical significance-based feature selection method for finger movement classification, are presented.\nThe feature sets under consideration include TD (time-domain feature set), FD (frequency-domain feature set), TF (time-frequency-domain feature set), and ND (nonlinear domain feature set).\nAll classifiers' performance was tested using both the time-domain features and the ANOVA-selected time-domain features considered in this study, with the results provided in\nAll classifiers' performances were evaluated in this study using time-domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the time-domain feature set.\nAbbreviations of time-domain features are TD1 (Minumum value), TD2 (Maximum value), TD3 (Mean), TD4 (Standard deviation value), TD5 (Integrated EEG value), TD6 (Mean absolute value), TD7 (Simple square integral), TD8 (Variance), TD9 (Root mean square), TD10 (Waveform length), TD11 (Average amplitude change value), TD12 (Absolute difference in standard deviation), TD13 (Kurtosis), TD14 (Skewness), TD15 (Hjorth parameters (Activity)), TD16 (Hjorth parameters (Mobility)), TD17 (Hjorth parameters (Complexity)), TD18 (Signal range), TD19 (First inter-quartile value (Q1)), TD20 (Second inter-quartile value (Q2)), TD21 (Third inter-quartile value (Q3)), TD22 (Mode value), TD23 (Zero-crossing value), and TD24 (Slope-change value).\nThe distribution of statistically significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the time-domain feature set.\nAbbreviations of time-domain features are TD1 (Minumum value), TD2 (Maximum value), TD3 (Mean), TD4 (Standard deviation), TD5 (Integrated EEG value), TD6 (Mean absolute value), TD7 (Simple square integral), TD8 (Variance), TD9 (Root mean square), TD10 (Waveform length), TD11 (Average amplitude change value), TD12 (Absolute difference in standard deviation), TD13 (Kurtosis), TD14 (Skewness), TD15 (Hjorth parameters (Activity)), TD16 (Hjorth parameters (Mobility)), TD17 (Hjorth parameters (Complexity)), TD18 (Signal range), TD19 (First inter-quartile value (Q1)), TD20 (Second inter-quartile value (Q2)), TD21 (Third inter-quartile value (Q3)), TD22 (Mode value), TD23 (Zero-crossing value), and TD24 (Slope-change value).\nAll classification results obtained using frequency-domain analysis are presented in\nAll classifiers' performances were evaluated in this study using frequency-domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the frequency-domain feature set.\nAbbreviations of frequency-domain features are FD1 (Energy of delta band), FD2 (Variance of delta band), FD3 (Entropy of delta band), FD4 (Energy of theta band), FD5 (Variance of theta band), FD6 (Entropy of theta band), FD7 (Energy of alpha band), FD8 (Variance of alpha band), FD9 (Entropy of alpha band), FD10 (Energy of beta band), FD11 (Variance of beta band), FD12 (Entropy of beta band), FD13 (Energy of gamma band), FD14 (Variance of gamma band), and FD15 (Entropy of gamma band).\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the frequency-domain feature set.\nAbbreviations of frequency-domain features are FD1 (Energy of delta band), FD2 (Variance of delta band), FD3 (Entropy of delta band), FD4 (Energy of theta band), FD5 (Variance of theta band), FD6 (Entropy of theta band), FD7 (Energy of alpha band), FD8 (Variance of alpha band), FD9 (Entropy of alpha band), FD10 (Energy of beta band), FD11 (Variance of beta band), FD12 (Entropy of beta band), FD13 (Energy of gamma band), FD14 (Variance of gamma band), and FD15 (Entropy of gamma band).\nClassification results performed using time-frequency-domain features are presented in\nAll classifiers' performances were evaluated in this study using time-frequency-domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the time-frequency-domain feature set.\nAbbreviations of time-frequency-domain features are TF1 (Energy of delta band), TF2 (Variance of delta band), TF3 (Entropy of delta band), TF4 (Energy of theta band), TF5 (Variance of theta band), TF6 (Entropy of theta band), TF7 (Energy of alpha band), TF8 (Variance of alpha band), TF9 (Entropy of alpha band), TF10 (Energy of beta band), TF11 (Variance of beta band), TF12 (Entropy of beta band), TF13 (Energy of gamma band), TF14 (Variance of gamma band), and TF15 (Entropy of gamma band).\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the time-frequency-domain feature set.\nAbbreviations of time-frequency-domain features are TF1 (Energy of delta band), TF2 (Variance of delta band), TF3 (Entropy of delta band), TF4 (Energy of theta band), TF5 (Variance of theta band), TF6 (Entropy of theta band), TF7 (Energy of alpha band), TF8 (Variance of alpha band), TF9 (Entropy of alpha band), TF10 (Energy of beta band), TF11 (Variance of beta band), TF12 (Entropy of beta band), TF13 (Energy of gamma band), TF14 (Variance of gamma band), and TF15 (Entropy of gamma band).\nThe test classification accuracies obtained based on the nonlinear feature set are presented in\nAll classifiers' performances were evaluated in this study using nonlinear domain feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe distribution of statistical significant features, selected via ANOVA, across 19 EEG channels for subject-independent finger movement classification using the nonlinear domain feature set.\nAbbreviations of nonlinear domain features are ND1 (\nThe distribution of statistically significant features, selected via ANOVA, across 19 EEG channels for subject-dependent finger movement classification using the nonlinear domain feature set.\nAbbreviations of nonlinear domain features are ND1 (\nAll classifiers' performances were evaluated in this study using the first combination (TD+FD+TF) feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nAll classifiers' performances were evaluated in this study using the second combination (TD+FD+TF+ND) feature set.\nThe maximum accuracy values for each subject-dependent and subject-independent case are highlighted in bold. SI is “Subject-independent”.\nThe primary findings and distinctional aspects of our study on finger movement classification are summarized below:\nWe extracted and investigated the different MI EEG features using time-domain, frequency-domain, time-frequency domain, and nonlinear domain of EEG signals.\nThis work represents, to the authors' knowledge, the first attempt to apply nonlinear features extracted from Poincare plots in the context of finger movement classification.\nOf all the feature sets analyzed, the second combination (TD+FD+TF+ND), when used with ANOVA, yielded the highest classification performance across both subject-independent and subject-dependent evaluations. Also, the lowest classification performances were mostly provided using Wavelet Transform-based time-frequency features in both cases (\nSeveral machine learning algorithms were deployed to classify EEG features. Among all algorithms, the highest classification accuracies were generally obtained with SVM algorithm in all feature sets.\nAccording to experimental results in all feature sets, the ANOVA-based feature selection method mostly improved prediction performance in the majority of machine learning algorithms (\nContrary to previous studies in the literature that neglect the brain's no mental task condition, we carried out a 6-class classification of finger movements by including the NoMT case instead of excluding the brain's idle state because we aim to propose a more realastic BCI system design. The classification performance of our 6-class finger movement classification study, which we conducted especially for the subject-dependent condition, is superior to the 5-class finger movement classification studies in the literature that eliminate the NoMT condition.\nA comprehensive evaluation of features and EEG channels was performed based on distribution maps highlighting statistically significant features - this approach, proposed for the first time in the context of finger movement classification - is employed to determine efficient and distinctive features and EEG channels.\nOur EEG feature and channel investigation has shown that the ANOVA test selected statistically significant features from some certain EEG frequency bands, and feature types whose effectiveness has been demonstrated in the BCI research field. These feature selections improved the classification performances in some of the analyzed feature sets (especially in frequency domain feature set). Also, in some feature sets (especially in time-domain, time-frequency domain, and nonlinear domain feature sets, used for subject-independent analysis), ANOVA did not prioritize specific EEG channels, frequency bands, and feature types during the feature reduction process. It was concluded that classification performance could be increased with this balanced distribution in some feature sets (especially in nonlinear feature set for subject-dependent analysis). Further analysis of the results revealed that classification performance can improve when using EEG channels and features that have not yet been established as effective in the literature. These findings highlight the critical role of effective feature and channel analysis in enhancing classifier performance.\nComparison of classification accuracies obtained using all features vs. ANOVA-selected features across all feature sets with the Support Vector Machine (SVM) classifier for Subject E (S4).\nComparison of classification accuracies obtained using all features vs. ANOVA-selected features across all feature sets with the Support Vector Machine (SVM) classifier in subject-independent analysis.\nBeyond these findings, it is important to consider the broader context in which finger movement classification operates, particularly in relation to functional brain networks and neurological disorders. Recent advances in EEG-based BCIs have demonstrated that motor imagery can be effectively used to decode intended movements without any physical action (Pfurtscheller & Neuper,\nWe compare the performance of the proposed finger movement classification with that of recent studies that use same EEG dataset with different feature extraction and classification algorithms. The details of these studies are shown in\nA comparison of the accuracy of this method with several state-of-the-art studies, for both subject-independent and subject-dependent cases, within this field.\nThe abbreviation “N” represents the “number of subjects,” “n” denotes the “number of EEG channels,” “c” refers to the “number of classes,” and “CV” stands for the “Cross-Validation Method”. The classifiers used are CNN (Convolutional Neural Network), RF (Random Forest), ADL (Autonomous Deep Learning), SVM (Support Vector Machine), EEGNet (EEGNet Deep Learning Model), BLS (Bi-layered Long-Short Classifier), and EL (Ensemble Learning). Features are PSD (power spectral density), FT (Fourier transfrom), ITD (intrinsic time-scale decomposition), EMD (empirical mode decomposition), and CSP (common spatial pattern).\nIn this study, the effects of different feature domains and a statistical significance-based feature selection method are investigated to classify finger movements. First, several EEG features of EEG segments are obtained from four different feature domains, including time-domain, frequency-domain, time-frequency, and nonlinear domain. In addition to these feature sets, two different combinations of features from multiple domains were investigated. Hence, a total of 1102 EEG features are calculated from four different feature domains, and six feature sets are generated for our finger movement analysis. By applying the statistical significance-based feature selection method, relevant and significant – hence fewer – EEG features are determined and selected from each feature set separately. All features obtained in six different feature sets and the statistically significant reduced feature sets were tested with various machine learning algorithms. The applied methods were tested on two different classification cases (subject-dependent and subject-independent classification). The results showed that the highest accuracy rates of 39.30% and 59.17% were obtained using the second combination feature set (TD + FD + TF + ND) and the SVM classifier in subject-independent and subject-dependent classifications, respectively. The selected EEG features (which are (i) energy and variance of five frequency bands in frequency-domain feature set, (ii) all feature types in time domain, time-frequency-domain, and nonlinear domain feature sets) and all EEG channels resulted in maximum accuracy of 59.17% with the SVM classifier for subject-dependent analysis.\nFor subject-independent analysis, the selected EEG features included (i) all feature types except waveform length, average amplitude change value, absolute difference in standard deviation, and slope-change value in the time-domain feature set; (ii) energy and variance values of all frequency bands except the gamma band in the frequency-domain feature set; (iii) entropy values of five frequency bands in the time-frequency-domain feature set; and (iv)\nOur main goal in this study is not to provide the best classifier performance but to investigate and indicate the discriminative EEG channels and features for the classification of finger movements. Toward this aim, the EEG channel and feature analysis was performed using channel-based statistical feature distribution maps. Particularly, the detailed feature and EEG channel analysis was implemented for the first time to classify finger movements along with various feature domains and their different combinations to the best of our knowledge. The EEG channel and feature analysis in finger movement classification indicates that it works well and supports literature in choosing specific frequency bands, and features from some feature domains. These specific selections have also been demonstrated to be effective in previous studies of prediction of motor imagery tasks based on EEG. It has been observed that selections are performed from features and channels that are not proven in the literature and that this improves the classification performance. Conversely, performance was also improved by balanced selections across all channels and features. Therefore, our experimental analysis suggests that classifier performance may be improved by implementing detailed feature and EEG channel analysis using a feature selection method based on statistical significance. Nonetheless, in this study, we only used an EEG dataset recorded from 13 healthy subjects, and each EEG data consists of 19 EEG channels sampled at 1000 Hz. To draw a more general conclusion, a dataset covering more subjects is necessary.", "topic": "Brain"}
{"pmid": "38504626", "pmcid": "12309336", "title": "The Impact of Dexamethasone Administration on Labor Progression in Pregnant Women: A Randomized, Double-Blind Clinical Trial", "publication_year": "N/A", "abstract": "", "full_text": "Dexamethasone is recognized for its role in cervical ripening and labor induction due to its anti-inflammatory effects. Previous research suggests it may enhance uterine contractility and cervical softening, but evidence on its optimal timing, effects on labor progression, and delivery outcomes is limited and inconclusive\nThis study shows that dexamethasone administration in the latent phase improves Bishop Scores and demonstrates that the latent phase intervention group had the lowest cesarean section rate, which warrants further investigation\nInduction of labor refers to the process of stimulating uterine contractions before the onset of spontaneous labor, with or without rupture of membranes. It was considered an intervention when the benefits of a rapid delivery for the mother or fetus outweighed the risks of continuing the pregnancy (\nThis was a randomized, controlled clinical trial designed to assess the effects of dexamethasone on labor induction in pregnant women. The trial was conducted at Besat Hospital in Sanandaj, Iran. It adhered to the Consolidated Standards of Reporting Trials (CONSORT) guidelines for reporting randomized controlled trials (\nThe study included pregnant women who were ≥18 years, with a singleton pregnancy, gestational age of 37 weeks, primigravida status, and cephalic presentation of the fetus. Women were excluded from participation if they had multiple pregnancies, non-cephalic fetal position, cephalopelvic disproportion, premature rupture of membranes, macrosomia, history of uterine surgery or anomalies, placenta previa, active genital herpes, intrauterine growth restriction, oligohydramnios, or fetal distress, which was determined based on abnormal non-stress test (NST) findings, Doppler velocimetry abnormalities, or persistent fetal heart rate decelerations. Other complications that could interfere with the trial outcomes were also considered exclusion criteria.\nThe sample size calculation was based on a clinical superiority trial design. Based on a meta-analysis by Salman et al (2017) (\n-Intervention group in the active phase receiving dexamethasone (First group)\n-Intervention group in the latent phase receiving dexamethasone (Second group)\n-Control group in the active phase (Third group)\n-Control group in the latent phase (Fourth group)\nParticipants were randomly assigned to 1 of 4 groups using a balanced block randomization method to ensure equal distribution and minimize baseline differences in maternal age, BMI, gestational age, and fetal sex. Randomization was carried out using randomization.com, with a fixed block size of 8 to maintain group balance and minimize selection bias. This ensured an equal distribution of participants across the 4 groups in a predictable but concealed manner. An independent statistician generated the sequence to eliminate selection bias. To ensure allocation concealment, the sequence was securely stored in sealed, opaque, and sequentially numbered envelopes. These envelopes were only accessible to the study coordinator at the time of participant enrollment, ensuring that group assignments remained unpredictable until allocation.\nParticipants in the control group received normal saline as a single 8 mL intravenous bolus along with standard labor induction medications. The standard medications included misoprostol (50 µg every 3 to 6 hours orally) to facilitate cervical ripening and oxytocin infusion, which was administered as needed to stimulate uterine contractions. These treatments ensured the initiation and progression of labor in the control group.\nIn contrast, participants in the intervention groups received dexamethasone (8 mg IV) along with the standard labor induction medications. Dexamethasone was administered to evaluate its potential role in enhancing the labor induction process. The combination of dexamethasone with misoprostol and oxytocin was tested to assess any additional benefits in reducing the time to delivery and improving maternal and neonatal outcomes.\nThe primary outcome was the time to delivery, measured from the initiation of labor induction to the birth of the infant. This outcome was chosen to evaluate the efficiency of labor induction in response to dexamethasone treatment compared to the control group, providing key insights into whether dexamethasone could reduce the duration of labor.\nSecondary outcomes included maternal side effects, such as hypertension, tachycardia, and nausea, which were monitored throughout the study to assess any adverse reactions to the treatments. Neonatal outcomes were evaluated, including APGAR scores at 1 and 5 minutes, birth weight, and the need for neonatal resuscitation. These secondary outcomes aimed to assess the impact of dexamethasone on neonatal health, including whether it influenced the immediate well-being of the newborn after delivery.\nAt the time of enrollment, baseline demographic and clinical data were collected, including maternal age, BMI, gestational age, and fetal sex. Clinical assessments were carried out to monitor FHR through a nonstress test, uterine contractions, and regular vaginal examinations to assess cervical dilation. There were no changes to the original data collection plan during the trial.\nData analysis was performed using Stata Version 18. Descriptive statistics, including mean, standard deviation (SD), and frequency, were used to summarize the baseline characteristics and outcomes of the study. Continuous variables, such as age, BMI, gestational age, birth weight, and labor phase durations, were assessed for normality using the Shapiro-Wilk test. Variables that followed a normal distribution were compared between groups using 1-way analysis of variance, while those that did not follow a normal distribution were analyzed using the Kruskal-Wallis test. Categorical variables, such as sex distribution and delivery type, were compared using the chi-square test. The Fisher exact test was applied where expected frequencies were small. For comparison of the baseline Bishop score, which differed significantly between groups, post-hoc pairwise comparisons were conducted using the Bonferroni correction to control for multiple comparisons. The effect of dexamethasone on labor progression and neonatal outcomes was assessed by comparing mean differences between groups using independent t tests for normally distributed data and Mann-Whitney U tests for non-normally distributed data. Effect sizes for significant differences were reported using Cohen's d for continuous variables. For comparing labor durations, cervical dilation rates, and secondary pregnancy times between groups, 95% confidence intervals (CIs) were calculated to assess the precision of the mean differences. The significance level for all statistical tests in this study was set at 0.05.\nThe study included a total of 120 participants (\nThe active phase groups—intervention group in the active phase receiving dexamethasone and control group in the active phase—demonstrated efficient labor progression, with some notable differences. The intervention group in the active phase, receiving dexamethasone group had a normal vaginal delivery (NVD) rate of 96.67% (29/30) and a caesarian section (C/S) rate of 3.33% (1/30), while the control group in the active phase achieved a perfect 100% NVD rate (30/30) and no C/S deliveries (0/30). Both groups showed a high proportion of the rupture of membranes (ROM) in the active phase, with the intervention group in the active phase receiving dexamethasone at 93.23% (28/30) and the control group in the active phase at 93.33% (28/30), indicating similar timing of ROM. The intervention group in the active phase receiving dexamethasone, however, benefited from dexamethasone, exhibiting a significantly shorter latent-to-active phase duration (mean, 4.85 hours), active phase duration (mean 2.20 hours), and total time to delivery (mean 7.21 hours), along with a rapid cervical dilation rate (2.57 cm/hour). Specific duration and dilation data for the control group in the active phase group are not provided, but its 100% NVD rate suggests robust labor progression without intervention, potentially comparable to or slightly slower than the intervention group in the active phase receiving dexamethasone, given the absence of dexamethasone acceleration. No neonates from either group required neonatal intensive care unit (NICU) admission, reflecting favorable outcomes in both (\nNext, comparing the latent phase groups, the intervention group in the latent phase receiving dexamethasone had a lower NVD rate of 90% (27/30) and a higher C/S rate of 10% (3/30) compared to the control group in the latent phase group’s 93.33% NVD (28/30) and 6.67% C/S (2/30). ROM in the active phase was less frequent in both latent phase groups, with the intervention group in the latent phase receiving dexamethasone at 66.67% (20/30) and the control group in the latent phase at 70% (21/30) compared to the active phase groups. The intervention group in the latent phase receiving dexamethasone showed significantly longer labor durations, with a latent-to-active phase duration of 12.51 hours, active phase duration of 4.11 hours, and the total time to delivery of 17.20 hours, alongside a slower cervical dilation rate of 1.13 cm/hour. Specific duration and dilation data for the intervention group in the latent phase receiving dexamethasone are not provided, but its higher C/S rate and lower active phase ROM suggest slower progression than the control group in the latent phase, despite dexamethasone administration. No NICU admissions were reported in either latent phase group (\nWhen analyzing all 4 groups together, the intervention group in the active phase, receiving dexamethasone, stood out with the fastest labor progression. The control group in the active phase group followed with a 100% NVD rate and 93.33% active phase ROM, indicating strong performance without dexamethasone, though likely slower than the intervention group in the active phase receiving dexamethasone in duration metrics. The intervention group in the latent phase receiving dexamethasone, with a 90% NVD rate, 10% C/S rate, and 66.67% active phase ROM, appeared less efficient than both active phase groups and potentially the control group in the latent phase, despite intervention. The control group in the latent phase lagged furthest behind, with the longest durations (12.51 hours latent-to-active, 4.11 hours active phase, 17.20 hours total), slowest dilation rate (1.13 cm/hour), and a 93.33% NVD rate, with 70% active phase ROM. Statistical significance was evident for ROM timing across groups (P = 0.027) and the intervention group in the active phase receiving dexamethasone group versus the control group in the latent phase comparisons (P < 0.0001 for most metrics), highlighting dexamethasone’s pronounced effect in the active phase. No NICU admissions occurred across the 4 groups, suggesting safe outcomes regardless of labor duration (\nThe active phase groups—intervention group in the active phase receiving dexamethasone and control group in the active phase—showed consistent neonatal outcomes. The intervention group in the active phase receiving dexamethasone had APGAR scores of 9 at 1 minute and 10 at 5 minutes, identical to the control group in the active phase, which also recorded 9 at 1 minute and 10 at 5 minutes. This uniformity suggests that dexamethasone administration in the active phase did not adversely affect neonatal well-being compared to the control group (\nAmong the latent phase groups—intervention group in the latent phase receiving dexamethasone and control group in the latent phase—neonatal outcomes were similarly stable. The intervention group in the latent phase receiving dexamethasone had APGAR scores of 9 at 1 minute and 10 at 5 minutes, matching the control group in the latent phase, which also showed 9 at 1 minute and 10 at 5 minutes. This indicates no differential impact of dexamethasone in the latent phase on neonatal health relative to the control (\nAcross all 4 groups—intervention group in the active phase receiving dexamethasone, intervention group in the latent phase receiving dexamethasone, control group in the active phase, and control group in the latent phase—APGAR scores were consistently 9 at 1 minute and 10 at 5 minutes (\nThe active phase groups differed in their use of labor-inducing medications. The intervention group in the active phase receiving dexamethasone required less oxytocin (mean, 10.66 units; SD, 6.39) compared to the control group in the active phase (mean, 12.33 units; SD, 5.68), and substantially fewer misoprostol doses (mean, 23.33; SD, 36.51) than the control group in the active phase (mean, 60; SD, 72.39). This suggests that dexamethasone in the active phase may reduce the need for additional induction agents compared to the control (\nAmong the latent phase groups, the intervention group in the latent phase receiving dexamethasone used less oxytocin (mean, 12.33 units; SD, 7.27) than the control group in the latent phase (mean, 15 units; SD, 6.82), and fewer misoprostol doses (mean, 41.66; SD, 41.69) compared to the control group in the latent phase (mean, 90.03; SD, 83.42). This indicates a similar trend of reduced medication reliance with dexamethasone in the latent phase versus the control (\nAcross all 4 groups, the control group in the latent phase had the highest oxytocin use (mean, 15 units; SD, 6.82) and misoprostol use (mean, 90.03; SD, 83.42), followed by the control group in the active phase (oxytocin: 12.33 units; SD, 5.68; misoprostol: 60; SD, 72.39). The intervention group in the latent phase receiving dexamethasone showed moderate use (oxytocin: 12.33 units; SD, 7.27; misoprostol: 41.66; SD, 41.69), while the intervention group in the active phase receiving dexamethasone had the lowest (oxytocin: 10.66 units; SD, 6.39; misoprostol: 23.33; SD, 36.51). The difference in misoprostol use was statistically significant (\nThe active phase groups showed minimal adverse complications. The intervention group in the active phase receiving dexamethasone reported 1 case of vaginal bleeding (1/30; 3.33%), while the control group in the active phase had no instances of vaginal bleeding (0/30; 0%). Neither group experienced FHR deceleration (0/30; 0% for both), indicating that dexamethasone in the active phase may be associated with a low incidence of complications, limited to a single bleeding event, compared to none in the control group (\nAmong the latent phase groups, the intervention group in the latent phase receiving dexamethasone had a higher incidence of adverse complications, with 3 cases of FHR deceleration (3/30; 10%) and no vaginal bleeding (0/30; 0%). The control group in the latent phase reported 2 cases of FHR deceleration (2/30; 6.67%) and no vaginal bleeding (0/30; 0%). This suggests that the latent phase, particularly with dexamethasone, may be associated with a higher risk of FHR deceleration compared to the control, although vaginal bleeding was absent in both (\nIn terms of adverse events, the incidence of vaginal bleeding was low across all groups. Only 1 participant (0.83%) experienced vaginal bleeding during the study, and the highest incidence was seen in the intervention group in the active phase (3.33%). However, this was not statistically significant (\nThe analysis revealed significant differences between the groups in several key outcomes. The latency to active phase duration was significantly reduced in both intervention groups compared to the control group, with the active phase dexamethasone group showing a difference of –7.66 hours (95% CI: –11.13 to –4.19,\nCONSORT flow diagram\nMean differences in the duration of labor phases across groups with 95% confidence intervals\nC/S: Cesarean, NVD: Natural vaginal delivery, ROM: Rupture of Membranes, NICU: Neonatal intensive care unit\nFHR: Fetal heart rate\n*\nThis study was designed to evaluate the impact of different interventions on the progression of labor, as well as on maternal and neonatal outcomes. The findings indicate significant variations between the groups, with the intervention group in the active phase receiving dexamethasone demonstrating the most advantageous results in terms of labor duration, cervical dilation, and time to delivery. These outcomes carry substantial clinical implications for the optimization of labor induction protocols and the enhancement of maternal and neonatal care. By shortening the duration of labor, we can decrease the risk of maternal complications such as chorioamnionitis, which is more common with prolonged labor. This is particularly relevant in settings where there's a higher baseline risk of infection due to ruptured membranes. Additionally, the risk of uterine rupture, especially in women with previous cesarean sections, can be mitigated by the accelerated labor observed in the intervention group in the active phase, receiving dexamethasone. This not only enhances safety in labor management but also improves the overall birthing experience by reducing fatigue, pain, and psychological stress, which can have long-term benefits on maternal mental health after delivery. From a neonatal perspective, quicker progression to delivery might reduce fetal stress, potentially lowering the incidence of fetal distress and the need for operative interventions like cesarean sections (\nMohaghegh et al (2021) conducted a systematic review and meta-analysis of 17 randomized controlled trials involving 1879 patients to evaluate the effects of dexamethasone on labor progression. Their findings indicated that dexamethasone administration significantly reduced the interval from labor induction to the active phase by approximately 70 minutes (MD: –1.17, 95% CI –1.37 to –1, P < 0.00001) (\nKashanian et al (2011) conducted a randomized study comparing intravenous dexamethasone (8 mg) to a control group in 100 women undergoing labor induction. Their findings demonstrated a significantly shorter interval from induction to the active phase in the dexamethasone group (2.87 ± 0.93 hours vs 3.80 ± 0.93 hours,\nThe notably reduced duration from the latent to the active phase of labor, along with a shorter active phase in the intervention group in the active phase receiving dexamethasone, suggests that the intervention effectively expedited labor. This observation aligns with previous research, which has emphasized the advantages of integrating corticosteroids with conventional labor induction methods to enhance cervical ripening and augment uterine contractility. The faster cervical dilation observed in the intervention group in the active phase receiving dexamethasone supports the use of corticosteroids to prepare the cervix for labor, potentially reducing the need for mechanical or other pharmacological interventions that carry their own risks. Moreover, the reduced time to delivery can be critical in scenarios where there are concerns about fetal well-being, allowing for prompt intervention if necessary (\nInterestingly, while the control group in the latent phase also demonstrated positive outcomes in some parameters, differences between this group and the intervention group receiving dexamethasone in the active phase were not statistically significant for several measures. This implies that the intervention in the control group in the latent phase might provide similar benefits in specific scenarios, although additional studies are needed to thoroughly assess its effectiveness and safety compared to the approach taken with the intervention group in the active phase, receiving dexamethasone. Importantly, the comparable maternal and neonatal safety outcomes, including APGAR scores and the absence of significant adverse effects, reinforce the safety profile of dexamethasone when used appropriately. Previous literature supports that corticosteroids do not significantly increase adverse outcomes when used judiciously (\nShabanian et al (2024) conducted a double-blind randomized controlled trial comparing vaginal dexamethasone tablets with hyoscine in primigravid women undergoing labor induction. Their findings demonstrated that dexamethasone significantly improved the Bishop score and shortened the induction-to-delivery interval, supporting the role of corticosteroids in enhancing cervical ripening. Although their administration route (vaginal) differs from systemic approaches, the observed benefits align with previous findings (\nFrom a clinical standpoint, these findings suggest that dexamethasone could improve labor management by accelerating the transition from the latent to the active phase and reducing the duration of active labor. This may lead to more efficient hospital resource utilization and decreased maternal discomfort, fatigue, and stress associated with prolonged labor. In addition, dexamethasone appears to enhance maternal and neonatal safety by potentially reducing complications such as infection and uterine rupture, particularly in high-risk cases. Given these benefits, dexamethasone could be integrated into labor induction protocols, especially when rapid delivery is needed to ensure fetal well-being. However, larger multicenter trials are necessary to confirm these findings across diverse populations and to investigate any long-term maternal and neonatal effects. Until then, clinicians should consider dexamethasone as an adjunct in specific labor management scenarios, carefully weighing its benefits against the known corticosteroid profile.\nDespite its strengths, this study has limitations. While the sample size was adequate to detect significant differences in primary outcomes, it may not have been large enough to capture subtle variations in secondary outcomes. In addition, the single-center design may limit the generalizability of the findings to broader populations. Future research should involve larger, multicenter trials to validate these results and explore potential variations across different settings and demographics.\nThe findings from this study advocate for the potential inclusion of corticosteroids in labor induction protocols, particularly where acceleration of labor is beneficial. Clinically, this could mean fewer complications, better maternal comfort, and improved neonatal outcomes. Nevertheless, the necessity for broader, more comprehensive studies to address the limitations and explore long-term outcomes remains a priority. This will help in establishing more definitive guidelines for the use of such interventions in clinical practice.\nThe study adhered to ethical guidelines set by the Ethics Committee of Kurdistan University of Medical Sciences, Sanandaj, Iran (IR.MUK.REC.1402.037). Written informed consent was obtained from all participants. Personal information was kept confidential, and participants were informed of their right to withdraw from the study at any time without any consequence to their care. The study was conducted in accordance with the principles of the Declaration of Helsinki.\nThe trial was registered at the Iranian Registry of Clinical Trials before participant enrollment (IRCT20231002059592N1).\nThe authors declare that they have no competing interests.\nThe trial was funded by Kurdistan University of Medical Sciences, Sanandaj, Iran.", "content_for_embedding": "Dexamethasone is recognized for its role in cervical ripening and labor induction due to its anti-inflammatory effects. Previous research suggests it may enhance uterine contractility and cervical softening, but evidence on its optimal timing, effects on labor progression, and delivery outcomes is limited and inconclusive\nThis study shows that dexamethasone administration in the latent phase improves Bishop Scores and demonstrates that the latent phase intervention group had the lowest cesarean section rate, which warrants further investigation\nInduction of labor refers to the process of stimulating uterine contractions before the onset of spontaneous labor, with or without rupture of membranes. It was considered an intervention when the benefits of a rapid delivery for the mother or fetus outweighed the risks of continuing the pregnancy (\nThis was a randomized, controlled clinical trial designed to assess the effects of dexamethasone on labor induction in pregnant women. The trial was conducted at Besat Hospital in Sanandaj, Iran. It adhered to the Consolidated Standards of Reporting Trials (CONSORT) guidelines for reporting randomized controlled trials (\nThe study included pregnant women who were ≥18 years, with a singleton pregnancy, gestational age of 37 weeks, primigravida status, and cephalic presentation of the fetus. Women were excluded from participation if they had multiple pregnancies, non-cephalic fetal position, cephalopelvic disproportion, premature rupture of membranes, macrosomia, history of uterine surgery or anomalies, placenta previa, active genital herpes, intrauterine growth restriction, oligohydramnios, or fetal distress, which was determined based on abnormal non-stress test (NST) findings, Doppler velocimetry abnormalities, or persistent fetal heart rate decelerations. Other complications that could interfere with the trial outcomes were also considered exclusion criteria.\nThe sample size calculation was based on a clinical superiority trial design. Based on a meta-analysis by Salman et al (2017) (\n-Intervention group in the active phase receiving dexamethasone (First group)\n-Intervention group in the latent phase receiving dexamethasone (Second group)\n-Control group in the active phase (Third group)\n-Control group in the latent phase (Fourth group)\nParticipants were randomly assigned to 1 of 4 groups using a balanced block randomization method to ensure equal distribution and minimize baseline differences in maternal age, BMI, gestational age, and fetal sex. Randomization was carried out using randomization.com, with a fixed block size of 8 to maintain group balance and minimize selection bias. This ensured an equal distribution of participants across the 4 groups in a predictable but concealed manner. An independent statistician generated the sequence to eliminate selection bias. To ensure allocation concealment, the sequence was securely stored in sealed, opaque, and sequentially numbered envelopes. These envelopes were only accessible to the study coordinator at the time of participant enrollment, ensuring that group assignments remained unpredictable until allocation.\nParticipants in the control group received normal saline as a single 8 mL intravenous bolus along with standard labor induction medications. The standard medications included misoprostol (50 µg every 3 to 6 hours orally) to facilitate cervical ripening and oxytocin infusion, which was administered as needed to stimulate uterine contractions. These treatments ensured the initiation and progression of labor in the control group.\nIn contrast, participants in the intervention groups received dexamethasone (8 mg IV) along with the standard labor induction medications. Dexamethasone was administered to evaluate its potential role in enhancing the labor induction process. The combination of dexamethasone with misoprostol and oxytocin was tested to assess any additional benefits in reducing the time to delivery and improving maternal and neonatal outcomes.\nThe primary outcome was the time to delivery, measured from the initiation of labor induction to the birth of the infant. This outcome was chosen to evaluate the efficiency of labor induction in response to dexamethasone treatment compared to the control group, providing key insights into whether dexamethasone could reduce the duration of labor.\nSecondary outcomes included maternal side effects, such as hypertension, tachycardia, and nausea, which were monitored throughout the study to assess any adverse reactions to the treatments. Neonatal outcomes were evaluated, including APGAR scores at 1 and 5 minutes, birth weight, and the need for neonatal resuscitation. These secondary outcomes aimed to assess the impact of dexamethasone on neonatal health, including whether it influenced the immediate well-being of the newborn after delivery.\nAt the time of enrollment, baseline demographic and clinical data were collected, including maternal age, BMI, gestational age, and fetal sex. Clinical assessments were carried out to monitor FHR through a nonstress test, uterine contractions, and regular vaginal examinations to assess cervical dilation. There were no changes to the original data collection plan during the trial.\nData analysis was performed using Stata Version 18. Descriptive statistics, including mean, standard deviation (SD), and frequency, were used to summarize the baseline characteristics and outcomes of the study. Continuous variables, such as age, BMI, gestational age, birth weight, and labor phase durations, were assessed for normality using the Shapiro-Wilk test. Variables that followed a normal distribution were compared between groups using 1-way analysis of variance, while those that did not follow a normal distribution were analyzed using the Kruskal-Wallis test. Categorical variables, such as sex distribution and delivery type, were compared using the chi-square test. The Fisher exact test was applied where expected frequencies were small. For comparison of the baseline Bishop score, which differed significantly between groups, post-hoc pairwise comparisons were conducted using the Bonferroni correction to control for multiple comparisons. The effect of dexamethasone on labor progression and neonatal outcomes was assessed by comparing mean differences between groups using independent t tests for normally distributed data and Mann-Whitney U tests for non-normally distributed data. Effect sizes for significant differences were reported using Cohen's d for continuous variables. For comparing labor durations, cervical dilation rates, and secondary pregnancy times between groups, 95% confidence intervals (CIs) were calculated to assess the precision of the mean differences. The significance level for all statistical tests in this study was set at 0.05.\nThe study included a total of 120 participants (\nThe active phase groups—intervention group in the active phase receiving dexamethasone and control group in the active phase—demonstrated efficient labor progression, with some notable differences. The intervention group in the active phase, receiving dexamethasone group had a normal vaginal delivery (NVD) rate of 96.67% (29/30) and a caesarian section (C/S) rate of 3.33% (1/30), while the control group in the active phase achieved a perfect 100% NVD rate (30/30) and no C/S deliveries (0/30). Both groups showed a high proportion of the rupture of membranes (ROM) in the active phase, with the intervention group in the active phase receiving dexamethasone at 93.23% (28/30) and the control group in the active phase at 93.33% (28/30), indicating similar timing of ROM. The intervention group in the active phase receiving dexamethasone, however, benefited from dexamethasone, exhibiting a significantly shorter latent-to-active phase duration (mean, 4.85 hours), active phase duration (mean 2.20 hours), and total time to delivery (mean 7.21 hours), along with a rapid cervical dilation rate (2.57 cm/hour). Specific duration and dilation data for the control group in the active phase group are not provided, but its 100% NVD rate suggests robust labor progression without intervention, potentially comparable to or slightly slower than the intervention group in the active phase receiving dexamethasone, given the absence of dexamethasone acceleration. No neonates from either group required neonatal intensive care unit (NICU) admission, reflecting favorable outcomes in both (\nNext, comparing the latent phase groups, the intervention group in the latent phase receiving dexamethasone had a lower NVD rate of 90% (27/30) and a higher C/S rate of 10% (3/30) compared to the control group in the latent phase group’s 93.33% NVD (28/30) and 6.67% C/S (2/30). ROM in the active phase was less frequent in both latent phase groups, with the intervention group in the latent phase receiving dexamethasone at 66.67% (20/30) and the control group in the latent phase at 70% (21/30) compared to the active phase groups. The intervention group in the latent phase receiving dexamethasone showed significantly longer labor durations, with a latent-to-active phase duration of 12.51 hours, active phase duration of 4.11 hours, and the total time to delivery of 17.20 hours, alongside a slower cervical dilation rate of 1.13 cm/hour. Specific duration and dilation data for the intervention group in the latent phase receiving dexamethasone are not provided, but its higher C/S rate and lower active phase ROM suggest slower progression than the control group in the latent phase, despite dexamethasone administration. No NICU admissions were reported in either latent phase group (\nWhen analyzing all 4 groups together, the intervention group in the active phase, receiving dexamethasone, stood out with the fastest labor progression. The control group in the active phase group followed with a 100% NVD rate and 93.33% active phase ROM, indicating strong performance without dexamethasone, though likely slower than the intervention group in the active phase receiving dexamethasone in duration metrics. The intervention group in the latent phase receiving dexamethasone, with a 90% NVD rate, 10% C/S rate, and 66.67% active phase ROM, appeared less efficient than both active phase groups and potentially the control group in the latent phase, despite intervention. The control group in the latent phase lagged furthest behind, with the longest durations (12.51 hours latent-to-active, 4.11 hours active phase, 17.20 hours total), slowest dilation rate (1.13 cm/hour), and a 93.33% NVD rate, with 70% active phase ROM. Statistical significance was evident for ROM timing across groups (P = 0.027) and the intervention group in the active phase receiving dexamethasone group versus the control group in the latent phase comparisons (P < 0.0001 for most metrics), highlighting dexamethasone’s pronounced effect in the active phase. No NICU admissions occurred across the 4 groups, suggesting safe outcomes regardless of labor duration (\nThe active phase groups—intervention group in the active phase receiving dexamethasone and control group in the active phase—showed consistent neonatal outcomes. The intervention group in the active phase receiving dexamethasone had APGAR scores of 9 at 1 minute and 10 at 5 minutes, identical to the control group in the active phase, which also recorded 9 at 1 minute and 10 at 5 minutes. This uniformity suggests that dexamethasone administration in the active phase did not adversely affect neonatal well-being compared to the control group (\nAmong the latent phase groups—intervention group in the latent phase receiving dexamethasone and control group in the latent phase—neonatal outcomes were similarly stable. The intervention group in the latent phase receiving dexamethasone had APGAR scores of 9 at 1 minute and 10 at 5 minutes, matching the control group in the latent phase, which also showed 9 at 1 minute and 10 at 5 minutes. This indicates no differential impact of dexamethasone in the latent phase on neonatal health relative to the control (\nAcross all 4 groups—intervention group in the active phase receiving dexamethasone, intervention group in the latent phase receiving dexamethasone, control group in the active phase, and control group in the latent phase—APGAR scores were consistently 9 at 1 minute and 10 at 5 minutes (\nThe active phase groups differed in their use of labor-inducing medications. The intervention group in the active phase receiving dexamethasone required less oxytocin (mean, 10.66 units; SD, 6.39) compared to the control group in the active phase (mean, 12.33 units; SD, 5.68), and substantially fewer misoprostol doses (mean, 23.33; SD, 36.51) than the control group in the active phase (mean, 60; SD, 72.39). This suggests that dexamethasone in the active phase may reduce the need for additional induction agents compared to the control (\nAmong the latent phase groups, the intervention group in the latent phase receiving dexamethasone used less oxytocin (mean, 12.33 units; SD, 7.27) than the control group in the latent phase (mean, 15 units; SD, 6.82), and fewer misoprostol doses (mean, 41.66; SD, 41.69) compared to the control group in the latent phase (mean, 90.03; SD, 83.42). This indicates a similar trend of reduced medication reliance with dexamethasone in the latent phase versus the control (\nAcross all 4 groups, the control group in the latent phase had the highest oxytocin use (mean, 15 units; SD, 6.82) and misoprostol use (mean, 90.03; SD, 83.42), followed by the control group in the active phase (oxytocin: 12.33 units; SD, 5.68; misoprostol: 60; SD, 72.39). The intervention group in the latent phase receiving dexamethasone showed moderate use (oxytocin: 12.33 units; SD, 7.27; misoprostol: 41.66; SD, 41.69), while the intervention group in the active phase receiving dexamethasone had the lowest (oxytocin: 10.66 units; SD, 6.39; misoprostol: 23.33; SD, 36.51). The difference in misoprostol use was statistically significant (\nThe active phase groups showed minimal adverse complications. The intervention group in the active phase receiving dexamethasone reported 1 case of vaginal bleeding (1/30; 3.33%), while the control group in the active phase had no instances of vaginal bleeding (0/30; 0%). Neither group experienced FHR deceleration (0/30; 0% for both), indicating that dexamethasone in the active phase may be associated with a low incidence of complications, limited to a single bleeding event, compared to none in the control group (\nAmong the latent phase groups, the intervention group in the latent phase receiving dexamethasone had a higher incidence of adverse complications, with 3 cases of FHR deceleration (3/30; 10%) and no vaginal bleeding (0/30; 0%). The control group in the latent phase reported 2 cases of FHR deceleration (2/30; 6.67%) and no vaginal bleeding (0/30; 0%). This suggests that the latent phase, particularly with dexamethasone, may be associated with a higher risk of FHR deceleration compared to the control, although vaginal bleeding was absent in both (\nIn terms of adverse events, the incidence of vaginal bleeding was low across all groups. Only 1 participant (0.83%) experienced vaginal bleeding during the study, and the highest incidence was seen in the intervention group in the active phase (3.33%). However, this was not statistically significant (\nThe analysis revealed significant differences between the groups in several key outcomes. The latency to active phase duration was significantly reduced in both intervention groups compared to the control group, with the active phase dexamethasone group showing a difference of –7.66 hours (95% CI: –11.13 to –4.19,\nCONSORT flow diagram\nMean differences in the duration of labor phases across groups with 95% confidence intervals\nC/S: Cesarean, NVD: Natural vaginal delivery, ROM: Rupture of Membranes, NICU: Neonatal intensive care unit\nFHR: Fetal heart rate\n*\nThis study was designed to evaluate the impact of different interventions on the progression of labor, as well as on maternal and neonatal outcomes. The findings indicate significant variations between the groups, with the intervention group in the active phase receiving dexamethasone demonstrating the most advantageous results in terms of labor duration, cervical dilation, and time to delivery. These outcomes carry substantial clinical implications for the optimization of labor induction protocols and the enhancement of maternal and neonatal care. By shortening the duration of labor, we can decrease the risk of maternal complications such as chorioamnionitis, which is more common with prolonged labor. This is particularly relevant in settings where there's a higher baseline risk of infection due to ruptured membranes. Additionally, the risk of uterine rupture, especially in women with previous cesarean sections, can be mitigated by the accelerated labor observed in the intervention group in the active phase, receiving dexamethasone. This not only enhances safety in labor management but also improves the overall birthing experience by reducing fatigue, pain, and psychological stress, which can have long-term benefits on maternal mental health after delivery. From a neonatal perspective, quicker progression to delivery might reduce fetal stress, potentially lowering the incidence of fetal distress and the need for operative interventions like cesarean sections (\nMohaghegh et al (2021) conducted a systematic review and meta-analysis of 17 randomized controlled trials involving 1879 patients to evaluate the effects of dexamethasone on labor progression. Their findings indicated that dexamethasone administration significantly reduced the interval from labor induction to the active phase by approximately 70 minutes (MD: –1.17, 95% CI –1.37 to –1, P < 0.00001) (\nKashanian et al (2011) conducted a randomized study comparing intravenous dexamethasone (8 mg) to a control group in 100 women undergoing labor induction. Their findings demonstrated a significantly shorter interval from induction to the active phase in the dexamethasone group (2.87 ± 0.93 hours vs 3.80 ± 0.93 hours,\nThe notably reduced duration from the latent to the active phase of labor, along with a shorter active phase in the intervention group in the active phase receiving dexamethasone, suggests that the intervention effectively expedited labor. This observation aligns with previous research, which has emphasized the advantages of integrating corticosteroids with conventional labor induction methods to enhance cervical ripening and augment uterine contractility. The faster cervical dilation observed in the intervention group in the active phase receiving dexamethasone supports the use of corticosteroids to prepare the cervix for labor, potentially reducing the need for mechanical or other pharmacological interventions that carry their own risks. Moreover, the reduced time to delivery can be critical in scenarios where there are concerns about fetal well-being, allowing for prompt intervention if necessary (\nInterestingly, while the control group in the latent phase also demonstrated positive outcomes in some parameters, differences between this group and the intervention group receiving dexamethasone in the active phase were not statistically significant for several measures. This implies that the intervention in the control group in the latent phase might provide similar benefits in specific scenarios, although additional studies are needed to thoroughly assess its effectiveness and safety compared to the approach taken with the intervention group in the active phase, receiving dexamethasone. Importantly, the comparable maternal and neonatal safety outcomes, including APGAR scores and the absence of significant adverse effects, reinforce the safety profile of dexamethasone when used appropriately. Previous literature supports that corticosteroids do not significantly increase adverse outcomes when used judiciously (\nShabanian et al (2024) conducted a double-blind randomized controlled trial comparing vaginal dexamethasone tablets with hyoscine in primigravid women undergoing labor induction. Their findings demonstrated that dexamethasone significantly improved the Bishop score and shortened the induction-to-delivery interval, supporting the role of corticosteroids in enhancing cervical ripening. Although their administration route (vaginal) differs from systemic approaches, the observed benefits align with previous findings (\nFrom a clinical standpoint, these findings suggest that dexamethasone could improve labor management by accelerating the transition from the latent to the active phase and reducing the duration of active labor. This may lead to more efficient hospital resource utilization and decreased maternal discomfort, fatigue, and stress associated with prolonged labor. In addition, dexamethasone appears to enhance maternal and neonatal safety by potentially reducing complications such as infection and uterine rupture, particularly in high-risk cases. Given these benefits, dexamethasone could be integrated into labor induction protocols, especially when rapid delivery is needed to ensure fetal well-being. However, larger multicenter trials are necessary to confirm these findings across diverse populations and to investigate any long-term maternal and neonatal effects. Until then, clinicians should consider dexamethasone as an adjunct in specific labor management scenarios, carefully weighing its benefits against the known corticosteroid profile.\nDespite its strengths, this study has limitations. While the sample size was adequate to detect significant differences in primary outcomes, it may not have been large enough to capture subtle variations in secondary outcomes. In addition, the single-center design may limit the generalizability of the findings to broader populations. Future research should involve larger, multicenter trials to validate these results and explore potential variations across different settings and demographics.\nThe findings from this study advocate for the potential inclusion of corticosteroids in labor induction protocols, particularly where acceleration of labor is beneficial. Clinically, this could mean fewer complications, better maternal comfort, and improved neonatal outcomes. Nevertheless, the necessity for broader, more comprehensive studies to address the limitations and explore long-term outcomes remains a priority. This will help in establishing more definitive guidelines for the use of such interventions in clinical practice.\nThe study adhered to ethical guidelines set by the Ethics Committee of Kurdistan University of Medical Sciences, Sanandaj, Iran (IR.MUK.REC.1402.037). Written informed consent was obtained from all participants. Personal information was kept confidential, and participants were informed of their right to withdraw from the study at any time without any consequence to their care. The study was conducted in accordance with the principles of the Declaration of Helsinki.\nThe trial was registered at the Iranian Registry of Clinical Trials before participant enrollment (IRCT20231002059592N1).\nThe authors declare that they have no competing interests.\nThe trial was funded by Kurdistan University of Medical Sciences, Sanandaj, Iran.", "topic": "Brain"}
{"pmid": "38458383", "pmcid": "12309953", "title": "Boosting your mood: How exercise and the amygdala dance together", "publication_year": "N/A", "abstract": "Accumulative evidence has shown that functional heterogeneity exists in subregions of amygdala. Recently, exercise serving as automatic emotion regulation has been observed to induce the altered activation of amygdala associated with mood change. However, the specific role of subregions of amygdala underlying these effects are not fully understood. By using resting-state functional magnetic resonance imaging (rs-fMRI), this study examined whether the subregions of amygdala play distinct roles in mood improvement induced by acute exercise.\nParticipants (\nResults revealed that exercise-induced mood improvements were correlated with significant Group × Time interaction effects on FC, showing a notable right-hemispheric predominance. Specifically, enhanced connectivity of the right mAmyg with orbitofrontal cortex, parietal, and cerebellar regions was associated with reduced negative affect and increased self-esteem. Concurrently, enhanced connectivity of the right lAmyg with the orbitofrontal cortex and striatum was linked to a broad spectrum of improvements, including reduced tension and anger, and increased vigor.\nThese findings suggest that acute exercise improves mood via distinct, lateralized neural pathways centered on different amygdala subregions. The mAmyg and lAmyg play complementary roles in automatic emotion regulation, with the right mAmyg modulating affective valence and self-evaluation, while the right lAmyg appears to regulate a broad spectrum of mood states and enhance positive arousal. This work provides a more nuanced neurobiological model for the therapeutic effects of exercise.", "full_text": "Emotion regulation refers to an individual’s ability to influence emotions in oneself by activating a goal to modify the emotion-generating process (\nAcute aerobic exercise has immediate and short-term effects on mood, by reducing negative mood such as anxiety, depression, tension, and anger (\nAn increasing number of studies have focused on exploring the neural mechanisms underlying the mood change associated with acute aerobic exercise. Existing literature have partly identified several brain regions and functional networks influenced by acute exercise. Key brain regions involved include the ventrolateral prefrontal cortex (\nThe amygdala, an almond-shaped structure deep within the temporal lobe, assumes responsibility for emotional regulation, implicit emotional learning, emotions-related attention and emotion perception (\nWhile current research has explored the neural mechanisms underlying the altered mood induced by acute aerobic exercise, the specific role of lateral and medial amygdala in this process has not yet been investigated. Furthermore, existing studies have some limitations such as relatively small sample sizes and gender homogeneity in some cases. Therefore, this study aims to utilize resting-state functional magnetic resonance imaging (rs-fMRI) to investigate the functional connectivity (FC) patterns of the amygdala with other brain regions in a larger and more diverse sample following a moderate-intensity acute aerobic exercise intervention. By comparing FC differences between the basolateral and medial amygdala and other brain regions, this study seeks to elucidate the distinct contributions of these amygdala subregions to the neural mechanisms underlying exercise-induced mood changes.\nEighty-three college students aged 18–22 years were recruited for the present study. The minimum sample size was determined using G*Power (Version 3.1) for a repeated-measures ANOVA (2 groups × 2 time points, within-between interaction), with α = 0.05, power = 0.95, and an expected moderate effect size of\nEligible participants were randomized into an acute aerobic exercise intervention group (AG,\nDemographic information of aerobic exercise group and control group.\nBMI, body mass index; PARS-3, Physical activity rating scale.\nThe study protocol was approved by the ethical committee of the Institute of Psychology, Chinese Academy of Sciences (approval number:\n\nAll baseline demographic variables, including gender, age, academic year, and BMI (calculated from height and weight), as well as physical activity level as measured by the PARS-3, were included as covariates in all subsequent analyses.\n\n\nParticipants were scheduled for a single laboratory visit, during which all experimental procedures were administered over the course of approximately half a day(\nThe exercise interventions were administered employing a MONARK 834 cycle ergometer (MONARK, Varberg, Sweden). The exercise protocol commenced with a 5-minute warm-up phase, succeeded by 20 min of moderate-intensity aerobic exercise at a controlled work rate and 5-minute cool-down phase. During 20-min exercise phase, the average pedaling rate was maintained at 77 ± 4 rpm. Following the exercise period, a five-minute cool-down phase was implemented, during which participants gradually reduced their pedaling rate and workload to slow down their heart rate and breathing.\nThe warm-up phase was initiated at a workload corresponding to 60 % of participants' APMHR, approximately yielding a power output of 25 W and a pedaling rate of 50 rpm. The workload was progressively augmented until the exercise intensity reached 60–69 % of the APMHR, as reflected by an average pedaling rate of 77 ± 4 rpm. Throughout the intervention, HR, RPE scales, power output in watts, and revolutions per minute were monitored at 2-minute intervals. After the exercise intervention, participants were instructed to rest until their HR reverted to baseline levels. Only once this criterion was met, the post-test session MRI scan was conducted, ensuring completion within a 10-minute timeframe.\nIn the control group, participants were tasked with engaging in a 30-minute session of reading neutral textual material without causing mood swings, conducted within a noise-free environment. Throughout this duration, their heart rate was monitored. Notably, the use of cell phones was strictly prohibited during this period.\nAll neuroimaging data were acquired using the Siemens 3T Trio system (Siemens, Erlangen, Germany). The functional images of resting states were obtained using an echo-planar imaging (EPI) sequence, employing the following scanning parameters: Repetition Time (TR) = 2000 ms, Echo Time (TE) = 30 ms, flip angle (FA) = 90°, slice thickness = 3.0 mm, field of view (FOV) = 200×200 mm\nFor data preprocessing and analysis, the Data Processing & Analysis for Brain Imaging (DPABI) toolbox was employed (\nROIs were selected from the Brainnetome Atlas, a template that divides the entire brain into 246 subregions based on brain structure and connectivity (\nTo specifically assess the impact of the acute exercise intervention on the change in resting-state functional connectivity (rs-FC), a whole-brain, voxel-wise mixed-effect analysis was performed for each of the four predefined amygdala subregion ROIs. This analysis was conducted using the mixed-effect analysis module within the DPABI toolbox. For each seed ROI, a model was constructed with FC as the dependent variable. The model included a between-subject factor of Group (Aerobic Exercise vs. Control), a within-subject factor of Time (Pre- vs. Post-intervention), and crucially, the Group × Time interaction term. Our primary analysis focused on the statistical map of this interaction, which identifies brain regions where the change in FC from pre- to post-intervention differed significantly between the two groups. To control for potential artifacts arising from head motion, a critical source of variance in rs-fMRI, the mean Framewise Displacement (FD) value from each scan was included as a time-varying covariate of no interest in the model. The resulting statistical maps for the Group × Time interaction were corrected for multiple comparisons using Gaussian Random Field (GRF) theory. The significance thresholds were set at a voxel-level\nStatistical analyses for demographic and behavioral data were performed using SPSS (Social Science Statistical Package) version 26.0 (IBM, Armonk, NY, USA). A two-tailed p value < 0.05 was considered statistically significant for all analyses unless otherwise specified.\nGroup differences in baseline demographics (age, BMI, PARS-3) were tested with independent-sample\nGeneralized Estimating Equations (GEE) assessed Group × Time interactions on mood scores, including age, gender, BMI, and PARS-3 scores as covariates. The GEE model parameters were determined based on model estimation, employing a non-structured working correlation matrix to account for within-subject dependencies. As a sensitivity analysis to further examine the robustness of significant intervention effects on mood, post-intervention mood scores were also analyzed using ANCOVA. In these models, the respective baseline mood score for each dimension, along with age, gender, BMI, and PARS-3 scores, were included as covariates.\nWithin the exercise group (AG), Pearson correlation coefficients were calculated to explore the relationship between significant changes in functional connectivity (ΔFC values from brain regions showing significant group differences in the fMRI analysis) and changes in mood scores (ΔMood from PANAS and A-POMS, calculated as post-intervention minus pre-intervention scores).\nBaseline data analysis on mood revealed no significant intergroup differences in other emotions except for the positive affect dimension of PANAS and the vigor dimension of A-POMS, where the AG group scored significantly higher than the CG group (see Table S1).\nGEE analysis revealed significant time main effects in scores for the NA of PANAS, and all dimensions of A-POMS (\nMood questionnaire scores before and after acute aerobic exercise.\nAbbreviations: PA, positive affect; NA, negative affect; TMD, Total Mood Disturbance, the total score of Abbreviated Profile of Mood State scale.\nGEE analysis results of the influence of exercise on emotion.\nTo further validate the robustness of our findings and to account for potential baseline differences that might confound the intervention effects, we conducted a sensitivity analysis using a covariance analysis (ANCOVA). In addition to age, gender, BMI, and physical activity level, we included baseline mood scores as covariates in the model to examine the group differences in post-intervention mood scores. The results revealed that the group difference for esteem (\nThe whole-brain, ROI-to-voxel analysis revealed significant Group × Time interaction effects on functional connectivity for all four amygdala subregion seeds (See\nSpecifically, acute exercise enhanced the left medial amygdala's (mAmyg) connectivity with right superior parietal lobule (SPL) and Heschl's gyrus. For the right mAmyg, the intervention strengthened its FC with multiple large clusters, including the bilateral middle and inferior frontal gyrus, left OFC, left cerebellum (Crus II), right SPL and middle occipital gyrus (MOG). For the left lateral amygdala (lAmyg), we observed significantly increased ∆FC with clusters in the cerebellum (lobules VI, VII, VIII, IX). Finally, the right lateral amygdala (R lAmyg) showed greater increases in ∆FC with the right medial OFC and the left caudate/nucleus accumbens (NAc).\nTo explore the clinical significance of these neural changes, we correlated the ∆FC values with changes in mood scores within the acute aerobic exercise group (\nCorrelation analysis of emotional changes and functional connectivity changes in acute aerobic exercise group. Each panel displays a significant correlation found within the aerobic exercise group (\nThe enhanced connectivity of the left mAmyg with the right SPL was associated with a greater reduction in Negative Affect (\nSimilarly, enhanced connectivity from the right mAmyg to several regions was also linked to greater reductions in Negative Affect. These regions included the left OFC (\nThe right lAmyg showed associations with a particularly diverse range of mood dimensions. Enhanced connectivity between the right lAmyg and the right OFC was related to decreases across four negative indices: Negative Affect (\nTo our knowledge, this is one of the first studies to investigate the distinct roles of amygdala subregions in mood improvements following a single bout of acute aerobic exercise in a large sample of young adults (the published database: doi.org/10.57760/sciencedb.14222 (\nFirstly, this study has uncovered the critical role of right amygdala-OFC circuits in mediating the mood-enhancing effects of exercise, revealing distinct contributions from medial and lateral subregions. The observed enhancement of amygdala-OFC connectivity is broadly in line with previous work (\nA notable finding of this study was the elucidation of the neural mechanisms underlying the increase in exercise-induced vigor. Vigor is conceptualized not merely as a simple positive mood, but as a multifaceted state of positive arousal encompassing feelings of physical strength, emotional energy, and cognitive liveliness (\nThis study also revealed that acute aerobic exercise led to enhanced FC between the bilateral medial amygdala and the right SPL. And this enhancement was associated with decreases in negative emotion. The SPL are generally associated with functions such as selective attention and working memory, and the control network and dorsal attention network containing it play roles in reallocating attention during emotion regulation and emotion control (\nOur study found that acute exercise significantly enhanced self-esteem, a key behavioral outcome that aligns with a large body of research showing the capacity of exercise to bolster positive affective states (\nA striking pattern that emerged from our findings is the pronounced right-hemisphere lateralization of exercise-induced changes in amygdala connectivity. While we observed significant effects originating from both left and right subregions, the most extensive and diverse associations with mood improvement were linked to the right medial and right lateral amygdala. This observation aligns with a body of literature suggesting a specialized role for the right amygdala in the processing of emotionally salient stimuli and the automatic generation of affective states (\nIt is important to acknowledge several limitations in this study. Firstly, the current investigation only examined the effects of moderate-intensity aerobic exercise on emotions. Previous research has indicated that high-intensity exercise might have a more pronounced impact on emotion improvement, particularly in individuals with high levels of anxiety (\nIn conclusion, this study provides compelling evidence that acute aerobic exercise improves mood by engaging distinct and lateralized neural circuits centered on different amygdala subregions. Our findings converge on a unifying theme: acute aerobic exercise appears to function as a potent form of automatic emotion regulation. This form of regulation, as delineated by\nSpecifically, we identified several potential pathways underlying these benefits. These include: (1) enhanced top-down regulation of negative affect via OFC-amygdala circuits; (2) improved attentional resource allocation through parietal-amygdala pathways; (3) modulation of self-evaluative emotions like self-esteem via cerebellar-amygdala connections; and (4) a boost in motivation and reward processing related to feelings of vigor through striatal-amygdala pathways.\nFuture research should further validate the roles of these distinct processes in exercise-induced mood improvement. It is essential to explore the interplay between top-down automatic emotion regulation, attentional shifting, self-evaluation, and the motivational effects of engaging the brain's reward circuitry. Understanding how these specific amygdala-based pathways contribute to well-being will be crucial for developing more targeted and effective exercise-based interventions for emotional disorders.\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.", "content_for_embedding": "Emotion regulation refers to an individual’s ability to influence emotions in oneself by activating a goal to modify the emotion-generating process (\nAcute aerobic exercise has immediate and short-term effects on mood, by reducing negative mood such as anxiety, depression, tension, and anger (\nAn increasing number of studies have focused on exploring the neural mechanisms underlying the mood change associated with acute aerobic exercise. Existing literature have partly identified several brain regions and functional networks influenced by acute exercise. Key brain regions involved include the ventrolateral prefrontal cortex (\nThe amygdala, an almond-shaped structure deep within the temporal lobe, assumes responsibility for emotional regulation, implicit emotional learning, emotions-related attention and emotion perception (\nWhile current research has explored the neural mechanisms underlying the altered mood induced by acute aerobic exercise, the specific role of lateral and medial amygdala in this process has not yet been investigated. Furthermore, existing studies have some limitations such as relatively small sample sizes and gender homogeneity in some cases. Therefore, this study aims to utilize resting-state functional magnetic resonance imaging (rs-fMRI) to investigate the functional connectivity (FC) patterns of the amygdala with other brain regions in a larger and more diverse sample following a moderate-intensity acute aerobic exercise intervention. By comparing FC differences between the basolateral and medial amygdala and other brain regions, this study seeks to elucidate the distinct contributions of these amygdala subregions to the neural mechanisms underlying exercise-induced mood changes.\nEighty-three college students aged 18–22 years were recruited for the present study. The minimum sample size was determined using G*Power (Version 3.1) for a repeated-measures ANOVA (2 groups × 2 time points, within-between interaction), with α = 0.05, power = 0.95, and an expected moderate effect size of\nEligible participants were randomized into an acute aerobic exercise intervention group (AG,\nDemographic information of aerobic exercise group and control group.\nBMI, body mass index; PARS-3, Physical activity rating scale.\nThe study protocol was approved by the ethical committee of the Institute of Psychology, Chinese Academy of Sciences (approval number:\n\nAll baseline demographic variables, including gender, age, academic year, and BMI (calculated from height and weight), as well as physical activity level as measured by the PARS-3, were included as covariates in all subsequent analyses.\n\n\nParticipants were scheduled for a single laboratory visit, during which all experimental procedures were administered over the course of approximately half a day(\nThe exercise interventions were administered employing a MONARK 834 cycle ergometer (MONARK, Varberg, Sweden). The exercise protocol commenced with a 5-minute warm-up phase, succeeded by 20 min of moderate-intensity aerobic exercise at a controlled work rate and 5-minute cool-down phase. During 20-min exercise phase, the average pedaling rate was maintained at 77 ± 4 rpm. Following the exercise period, a five-minute cool-down phase was implemented, during which participants gradually reduced their pedaling rate and workload to slow down their heart rate and breathing.\nThe warm-up phase was initiated at a workload corresponding to 60 % of participants' APMHR, approximately yielding a power output of 25 W and a pedaling rate of 50 rpm. The workload was progressively augmented until the exercise intensity reached 60–69 % of the APMHR, as reflected by an average pedaling rate of 77 ± 4 rpm. Throughout the intervention, HR, RPE scales, power output in watts, and revolutions per minute were monitored at 2-minute intervals. After the exercise intervention, participants were instructed to rest until their HR reverted to baseline levels. Only once this criterion was met, the post-test session MRI scan was conducted, ensuring completion within a 10-minute timeframe.\nIn the control group, participants were tasked with engaging in a 30-minute session of reading neutral textual material without causing mood swings, conducted within a noise-free environment. Throughout this duration, their heart rate was monitored. Notably, the use of cell phones was strictly prohibited during this period.\nAll neuroimaging data were acquired using the Siemens 3T Trio system (Siemens, Erlangen, Germany). The functional images of resting states were obtained using an echo-planar imaging (EPI) sequence, employing the following scanning parameters: Repetition Time (TR) = 2000 ms, Echo Time (TE) = 30 ms, flip angle (FA) = 90°, slice thickness = 3.0 mm, field of view (FOV) = 200×200 mm\nFor data preprocessing and analysis, the Data Processing & Analysis for Brain Imaging (DPABI) toolbox was employed (\nROIs were selected from the Brainnetome Atlas, a template that divides the entire brain into 246 subregions based on brain structure and connectivity (\nTo specifically assess the impact of the acute exercise intervention on the change in resting-state functional connectivity (rs-FC), a whole-brain, voxel-wise mixed-effect analysis was performed for each of the four predefined amygdala subregion ROIs. This analysis was conducted using the mixed-effect analysis module within the DPABI toolbox. For each seed ROI, a model was constructed with FC as the dependent variable. The model included a between-subject factor of Group (Aerobic Exercise vs. Control), a within-subject factor of Time (Pre- vs. Post-intervention), and crucially, the Group × Time interaction term. Our primary analysis focused on the statistical map of this interaction, which identifies brain regions where the change in FC from pre- to post-intervention differed significantly between the two groups. To control for potential artifacts arising from head motion, a critical source of variance in rs-fMRI, the mean Framewise Displacement (FD) value from each scan was included as a time-varying covariate of no interest in the model. The resulting statistical maps for the Group × Time interaction were corrected for multiple comparisons using Gaussian Random Field (GRF) theory. The significance thresholds were set at a voxel-level\nStatistical analyses for demographic and behavioral data were performed using SPSS (Social Science Statistical Package) version 26.0 (IBM, Armonk, NY, USA). A two-tailed p value < 0.05 was considered statistically significant for all analyses unless otherwise specified.\nGroup differences in baseline demographics (age, BMI, PARS-3) were tested with independent-sample\nGeneralized Estimating Equations (GEE) assessed Group × Time interactions on mood scores, including age, gender, BMI, and PARS-3 scores as covariates. The GEE model parameters were determined based on model estimation, employing a non-structured working correlation matrix to account for within-subject dependencies. As a sensitivity analysis to further examine the robustness of significant intervention effects on mood, post-intervention mood scores were also analyzed using ANCOVA. In these models, the respective baseline mood score for each dimension, along with age, gender, BMI, and PARS-3 scores, were included as covariates.\nWithin the exercise group (AG), Pearson correlation coefficients were calculated to explore the relationship between significant changes in functional connectivity (ΔFC values from brain regions showing significant group differences in the fMRI analysis) and changes in mood scores (ΔMood from PANAS and A-POMS, calculated as post-intervention minus pre-intervention scores).\nBaseline data analysis on mood revealed no significant intergroup differences in other emotions except for the positive affect dimension of PANAS and the vigor dimension of A-POMS, where the AG group scored significantly higher than the CG group (see Table S1).\nGEE analysis revealed significant time main effects in scores for the NA of PANAS, and all dimensions of A-POMS (\nMood questionnaire scores before and after acute aerobic exercise.\nAbbreviations: PA, positive affect; NA, negative affect; TMD, Total Mood Disturbance, the total score of Abbreviated Profile of Mood State scale.\nGEE analysis results of the influence of exercise on emotion.\nTo further validate the robustness of our findings and to account for potential baseline differences that might confound the intervention effects, we conducted a sensitivity analysis using a covariance analysis (ANCOVA). In addition to age, gender, BMI, and physical activity level, we included baseline mood scores as covariates in the model to examine the group differences in post-intervention mood scores. The results revealed that the group difference for esteem (\nThe whole-brain, ROI-to-voxel analysis revealed significant Group × Time interaction effects on functional connectivity for all four amygdala subregion seeds (See\nSpecifically, acute exercise enhanced the left medial amygdala's (mAmyg) connectivity with right superior parietal lobule (SPL) and Heschl's gyrus. For the right mAmyg, the intervention strengthened its FC with multiple large clusters, including the bilateral middle and inferior frontal gyrus, left OFC, left cerebellum (Crus II), right SPL and middle occipital gyrus (MOG). For the left lateral amygdala (lAmyg), we observed significantly increased ∆FC with clusters in the cerebellum (lobules VI, VII, VIII, IX). Finally, the right lateral amygdala (R lAmyg) showed greater increases in ∆FC with the right medial OFC and the left caudate/nucleus accumbens (NAc).\nTo explore the clinical significance of these neural changes, we correlated the ∆FC values with changes in mood scores within the acute aerobic exercise group (\nCorrelation analysis of emotional changes and functional connectivity changes in acute aerobic exercise group. Each panel displays a significant correlation found within the aerobic exercise group (\nThe enhanced connectivity of the left mAmyg with the right SPL was associated with a greater reduction in Negative Affect (\nSimilarly, enhanced connectivity from the right mAmyg to several regions was also linked to greater reductions in Negative Affect. These regions included the left OFC (\nThe right lAmyg showed associations with a particularly diverse range of mood dimensions. Enhanced connectivity between the right lAmyg and the right OFC was related to decreases across four negative indices: Negative Affect (\nTo our knowledge, this is one of the first studies to investigate the distinct roles of amygdala subregions in mood improvements following a single bout of acute aerobic exercise in a large sample of young adults (the published database: doi.org/10.57760/sciencedb.14222 (\nFirstly, this study has uncovered the critical role of right amygdala-OFC circuits in mediating the mood-enhancing effects of exercise, revealing distinct contributions from medial and lateral subregions. The observed enhancement of amygdala-OFC connectivity is broadly in line with previous work (\nA notable finding of this study was the elucidation of the neural mechanisms underlying the increase in exercise-induced vigor. Vigor is conceptualized not merely as a simple positive mood, but as a multifaceted state of positive arousal encompassing feelings of physical strength, emotional energy, and cognitive liveliness (\nThis study also revealed that acute aerobic exercise led to enhanced FC between the bilateral medial amygdala and the right SPL. And this enhancement was associated with decreases in negative emotion. The SPL are generally associated with functions such as selective attention and working memory, and the control network and dorsal attention network containing it play roles in reallocating attention during emotion regulation and emotion control (\nOur study found that acute exercise significantly enhanced self-esteem, a key behavioral outcome that aligns with a large body of research showing the capacity of exercise to bolster positive affective states (\nA striking pattern that emerged from our findings is the pronounced right-hemisphere lateralization of exercise-induced changes in amygdala connectivity. While we observed significant effects originating from both left and right subregions, the most extensive and diverse associations with mood improvement were linked to the right medial and right lateral amygdala. This observation aligns with a body of literature suggesting a specialized role for the right amygdala in the processing of emotionally salient stimuli and the automatic generation of affective states (\nIt is important to acknowledge several limitations in this study. Firstly, the current investigation only examined the effects of moderate-intensity aerobic exercise on emotions. Previous research has indicated that high-intensity exercise might have a more pronounced impact on emotion improvement, particularly in individuals with high levels of anxiety (\nIn conclusion, this study provides compelling evidence that acute aerobic exercise improves mood by engaging distinct and lateralized neural circuits centered on different amygdala subregions. Our findings converge on a unifying theme: acute aerobic exercise appears to function as a potent form of automatic emotion regulation. This form of regulation, as delineated by\nSpecifically, we identified several potential pathways underlying these benefits. These include: (1) enhanced top-down regulation of negative affect via OFC-amygdala circuits; (2) improved attentional resource allocation through parietal-amygdala pathways; (3) modulation of self-evaluative emotions like self-esteem via cerebellar-amygdala connections; and (4) a boost in motivation and reward processing related to feelings of vigor through striatal-amygdala pathways.\nFuture research should further validate the roles of these distinct processes in exercise-induced mood improvement. It is essential to explore the interplay between top-down automatic emotion regulation, attentional shifting, self-evaluation, and the motivational effects of engaging the brain's reward circuitry. Understanding how these specific amygdala-based pathways contribute to well-being will be crucial for developing more targeted and effective exercise-based interventions for emotional disorders.\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.", "topic": "Brain"}
{"pmid": "38400241", "pmcid": "12308364", "title": "The Slit–Robo signalling pathway in nervous system development: a comparative perspective from vertebrates and invertebrates", "publication_year": "2025", "abstract": "During nervous system development, growing axons find their targets with the help of guidance cues. These cues, which can be secreted molecules provided by neighbouring cells or transmembrane proteins mediating cell–cell contacts with the growing axons, act as either chemoattractants or chemorepellents. Over the last decades, several axon guidance molecules have been identified. One of the classical guidance cues is the Slit protein. Slit is a secreted protein, initially identified in a genetic screen in the fruit fly", "full_text": "A key event in our comprehension of axonal guidance and cell migration during development came with the discovery of Slit proteins. Slit was first detected in the fly embryonic midline glial cells of the ventral nerve cord (VNC), from where it is released to the extracellular space and distributed along axon tracts. Slit is a member of the protein family containing EGF-like repeats [\nThe discovery of Robo came out of a\nSubsequent research revealed Slit as a ligand for the Robo receptor in both vertebrates and\nSlit is an extracellular matrix-secreted glycoprotein [\nAll Slit proteins share a common structural framework (\nSchematic representation of Slit and Robo structure. (A) Mammals encode three Slit and\nThe specific functions of Slit fragments were investigated\nSlit fragments interactions. Information about the specific functions of Slit fragments has only been recently investigated\nRecently, a Slit metalloprotease able to generate Slit-N and Slit-C was identified. This protein is a member of the BMP1/Tolloid family of Astacin-like metalloproteases and was named Tolkin (Tok; also known as Tolloid-related, Tlr, or Piranha). In\nRobo is a single-pass transmembrane receptor belonging to a subfamily of the immunoglobulin (Ig) superfamily proteins, with an apparent molecular weight of approximately 180 kDa for\nThe function of the Robo cytoplasmic domain in signalling has been studied in some detail. It is responsible for mediating the repulsive response, as demonstrated using a chimeric receptor consisting of the Robo ectodomain and Frazzled (Fra, a receptor involved in growth cone attraction that responds to the protein Netrin) cytoplasmic domain. In these experiments performed in\nRegarding the ectodomain, the requirement for the repulsive function of Robo1 Ig1 domain has been highlighted recently [\nIn the\nExpression patterns of Slit and Robo are dynamic during development and have been investigated in detail in some systems. Here we will give an overview that will be complemented in the next sections with more in-depth analysis for specific examples. In invertebrates such as the fly Slit is secreted from ventral midline cells during embryo development (\nMechanisms of Slit–Robo-mediated axon guidance in the mouse and fly central nervous system. (A) The fly VNC is the region in which Slit expression and function is best characterized. Slit is secreted by the midline glia and axons will respond to Slit according to the presence of Robo in the membrane of the growth cone. (B) In the mouse spinal cord, pre-crossing commissural axons do not express Robo1/2 (signal OFF) on their surface. However, as they reach the midline, they express Robo1/2 (signal ON) and respond to Slit secreted from the floor plate (FP) glia to exit this structure. P: posterior, A: anterior. (C) In the\nSlit distribution in the vertebrate brain is more complex than in the spinal cord, implying more diversified roles in axon guidance and other developmental processes in this region. For instance, in the mouse visual system, only Slit1 and Slit2 are expressed, whereas in zebrafish, Slit2 and Slit3 are both required and cooperate in guiding retinal ganglion cell axons [\nRobo1 and Robo2 proteins in the spinal cord are predominantly expressed in post-crossing axons [\nThis section provides a general overview of the regulatory mechanisms governing Slit and Robo expression, function and signalling output. Some of these mechanisms are described in more detail, along with specific examples, in the following sections.\nDuring nervous system development, the expression patterns of the Slit and Robo genes are closely regulated. The spatiotemporal expression of Slit and Robo is controlled by transcriptional regulation, which is essential for axonal pathfinding and neuronal migration. This fine regulation ensures that neurons migrate to the correct places and axons are guided to the proper targets. In pontine neurons (PN) along the anteroposterior (AP) axis during mouse hindbrain development, Robo2 is a direct target of Hoxa2, a homeodomain transcription factor of the Hox gene family [\nAnother layer of regulation is the post-transcriptional control. For instance, in the chicken spinal cord, the microRNA miR-92 suppresses Robo1 translation specifically in precrossing commissural axons. miR-92 binds to the 3′ UTR of\nRegarding post-translational regulation, during vertebrate commissural axon midline crossing, a ubiquitin-specific protease 33 (USP33) maintains the stability of Robo1 after its interaction with Slit. USP33 promotes Robo1 deubiquitination, which may prevent degradation of the Robo1 receptor and/or enable its recycling following Slit stimulation [\nSlit interacts not only with Robo receptors but also with other receptors, transmembrane proteins and extracellular matrix components, which act as co-receptors or help with its proper extracellular localization. For instance, Slit-C fragment (and probably full-length Slit) can bind to Dystroglycan, a transmembrane glycoprotein, through its laminin-G domain (\nSlit-Robo signalling. (A) The interaction with heparan sulfate proteoglycans (HSPGs) enhances Slit–Robo signalling in both vertebrates and\nIn vertebrates, cell-surface heparan sulfate proteoglycans (HSPGs) play a critical role in enhancing Slit–Robo signalling by binding to the D2 domain of Slit2 [\nIn addition to functioning as a ligand–receptor pair, Slit and Robo influence axon development through interactions with additional receptors and ligands, respectively (\nNotably, the mammalian Robo3 does not bind to Slit [\nUpon activation, Slit–Robo signalling initiates a series of intracellular events that can result in cytoskeletal reorganization and other cellular responses (\nIt has been shown that Robo1 receptors must be internalized via endocytosis to ensure proper intracellular signalling. Specifically, Chance & Bashaw [\nMoreover, Slit–Robo regulates neurogenesis in mammalian neural progenitor cells by interacting with the Notch pathway and transcriptionally activating the Notch effector Hes1. However, the activation of Hes1 by Robo1/2’s is independent of Notch signalling and is mediated by the Robo cytoplasmic domains [\nSmall GTPases are GTP-binding proteins that modify the cytoskeleton in order to control cell movement, and they play a central role in transducing signals downstream of activated Robo receptors. The small GTP-binding proteins act as molecular switches, cycling between active GTP-bound and inactive GDP-bound states to regulate various cellular processes, including cytoskeletal dynamics. Robo receptors recruit GTPases and their regulatory proteins, including GEFs (guanine nucleotide exchange factors), Dock/Nck (mammal Nck) and srGAPs (Slit–Robo GTPase activating proteins, in vertebrates) to modulate their activity and downstream signalling pathways. Thus, recruiting RhoGEF proteins downstream of the cytoplasmic Robo domain is required for Slit–Robo signalling [\nThe Slit–Robo signalling regulates actin dynamics by modulating the activity of small GTPases and other cytoskeletal regulators proteins. Wong\nUpon activation, the Robo ectodomain is cleaved and the rest of the protein undergoes internalization. This process seems to be required to activate repulsive signalling by the recruitment of Son of Sevenless (Sos) [\nAnother downstream effector of Slit–Robo signalling is Enabled (Ena). The Ena proline-rich protein family that promotes actin polymerization and cell motility. Ena interacts with the Robo receptor to enhance axonal repulsion by stopping the formation and extension of filopodia when responding to Slit [\nSlit–Robo signalling interacts with N-cadherin to regulate cell adhesion and neurite growth. This has been studied in chick retinal cells, where Slit binding to Robo receptors triggers Abl to interact with the intracellular domain (CC3) of Robo, facilitating a connection between the Robo and N-cadherin via the protein Cables a substrate of Abl and CDK5. Upon forming this protein complex, β-catenin is phosphorylated by Abl, leading to its dissociation from N-cadherin. As a result, cell attachment mediated by N-cadherin is compromised and neurites are not formed in cell culture experiments. Furthermore, phosphorylated β-catenin moves into the nucleus, to modify gene expression [\nThe VNC of\nSlit is the main repulsive guidance cue secreted by the midline glia, in\nNevertheless, Comm is not the only way that Slit and Robo availability can be modulated during the crossing of the midline in\nBesides its function in midline crossing, the Slit–Robo pathway is also involved in the positioning of longitudinal axonal tracks in the medio-lateral axis [\nIn vertebrates, axon guidance molecules have been studied in the commissural neurons of the spinal cord for decades (\nSlit–Robo classically regulates axon midline crossing by a repulsion mechanism, although the specific mechanisms used by the distinct Robo receptors diverge. Slits in the spinal cord are secreted by floor plate cells, which are located ventrally [\nIndeed, loss of Robo3 caused commissural axons to re-route and turn into the longitudinal axis without crossing the midline [\nRobo3 has multiple isoforms. Two of these isoforms [\nSlit–Robo signalling is also involved in guidance of motor axons. The spinal cord motor columns house the neurons that innervate the muscles, the motor neurons. These neurons extend their axons from the central nervous system (the spinal cord) to the periphery (muscle) and they exit through specific locations of the spinal cord. Robo2 and Slits mediate the exit of spinal accessory motor neurons through the lateral exit point possibly through a short-range attractive mechanism [\nMechanistically, the fine regulation of attractive cues and the repulsive activity of Slit–Robo signalling is required to ensure axons to be attracted to an intermediate target, such as the floor plate at the midline, and once they have reached it to continue their journey away from this structure. A reported mechanism for regulating the timing and location of repulsive activity involves the expression of Robo receptors in axonal growth cones by alternative splicing, discovering a previously unknown layer of regulation [\nAlternatively, insertion of Robo1 into the cell surface is modulated by RabGDI (Rab Guanine Nucleotide Dissociation Inhibitor) in chick embryos, but the mechanism is different from that of Comm. RabGDI is expressed specifically during midline crossing and promotes Robo1-containing vesicles to be shuttled to the growth cone membrane by Calsyntenin-1 [\nThe repulsive role of the Slit–Robo signalling has also been described in axons other than commissural axons. In zebrafish, axons that descend ipsilaterally from the ventral diencephalon are repelled from the midline by Robo2 [\nThere is a well-documented relationship between the Slit–Robo and Netrin-DCC signalling pathways during the development of the nervous system. Netrin is a secreted guidance cue which has been involved in attraction and repulsion of axons depending on the context [\nIn mammals, Netrin1-DCC and Slit–Robo signalling pathways also have opposing roles in spinal cord midline crossing. Although, by contrast to the fly, it was originally thought that the action of both pathways was sequential in their common goal to steer axons towards the floor plate (Netrin-DCC) and subsequently avoid re-entry into this intermediate target (Slit–Robo) [\nThe Slit–Robo pathway plays roles in several regions of the\nStrikingly, sex specific regulation of Robo has been implicated in the formation of sexually dimorphic neuronal circuits [\nThe retina of\nSimilar to the fly brain, in vertebrates Slit distribution in the developing brain is more complex than in the spinal cord (\nSlit–Robo mediated repulsion also modulates the pathfinding of axons that project from retinal ganglion cells (RGCs) in the eye to the brain in mammals. Normally, in mice, a relatively large proportion of RGC axons cross the midline to form the optic chiasm, whereas a relatively low proportion of axons remain ipsilateral [\nSo far there is little data supporting a function of Slit–Robo signalling in early neurogenesis of the fly CNS. One study has shown that Slit regulates asymmetric division of a population of ganglion mother cell (GMC1) derived from neuroblast NB4.2 in the VNC of the\nBy contrast, the function of Slit–Robo has been addressed in several studies using the mouse as a model. During early cortical development, neuroepithelial progenitors in the ventricular zone (apical side) divide symmetrically to increase their number. Later on, excitatory projection neurons are generated by asymmetrical divisions either from apical radial glia cells (direct neurogenesis) or from basal intermediate progenitor cells (indirect neurogenesis) [\nAnother aspect of neurogenesis is the process of newly born neurons detaching from the apical side soon after cell division. During retinogenesis, the newborn neurons also detach from the apical (outer) neuroepithelium. In zebrafish, the apical retraction of RGCs depends on Slit1b and Robo3 [\nBesides axonal guidance, Slit–Robo is also involved more widely in the architecture of brain neuropils by defining boundaries that limit the localization of different cellular populations. This function has been described in the optic lobe of\nThe fly optic lobe shares some similarities with the mammalian cortex, since neuronal migration has been shown to play a role in the assembly of neuronal circuits [\nSlit–Robo signalling regulates neuronal migration through the modulation of focal adhesion kinase (FAK) activity and cytoskeletal dynamics in vertebrates. Slit2 influences Cdc42 activity and lamellipodia formation on\nIn vertebrates Slit–Robo has also a role in neuronal migration. Slit was first discovered to influence directional migration of neuroblasts to the olfactory bulb [\nSame as the typical commissural neurons in the spinal cord, inferior olive neurons of the brainstem extend axons that cross the midline in route to their target in the contralateral side of the cerebellum. However, their somas do not cross this structure, despite their dorso-ventral migration towards the vicinity of the midline. Migrating inferior olive neurons express Robo3, and in its absence they still form the inferior olive nucleus in its proper location, albeit more slowly and with abnormal cellular shape and polarization [\nIn the spinal cord, dI1 neurons migrate slightly in the ventral direction from the dorsal area after their axons cross the midline. Robo2 is enriched in the ipsilateral subpopulation of dI1 neurons. In its absence, the cell bodies of ipsilateral neurons (but not the commissural population) are distributed in regions that are too ventral and/or medial. This effect of Robo2 seems to be a partial result of Slit2 action in the surrounding regions of dI1 neuron migration, since the phenotype in\nMotor neuron cell bodies are contained in the ventral horn of the spinal cord and brainstem. Most motor neurons are born close to their final residence (in the ventral progenitor area), therefore the notion that they do not migrate much. In the spinal cord and brainstem, the repulsive action of Slit–Robo signalling is crucial for motor neurons to reach their proper location, as in Robo1/2 or Slits1/2/3 mutant mice these cell bodies are misplaced in the floor plate, as opposed to be adjacent to it in control animals. This effect seems to be counteracted by Netrin-DCC signalling, suggesting a balance of attractive and repulsive forces to keep the neuronal somas in place [\nApart from the active migration that motor neurons need to undergo, it is crucial that these neurons do not migrate outside of the central nervous system into the periphery. Several studies have shown that guidance molecules prevent exit of motor neurons to the periphery by regulating the function of boundary cap cells [\nRobo1 influences the entrance and the migration of interneurons throughout the cerebral cortex at the stages when these neurons are\nA system that uses guidepost cells to physically narrow and permit the path of thalamic axons\nIn addition to tangential migration of inhibitory neurons, Robo1 and Robo4 play a role in the migration of excitatory pyramidal neurons to the cortex [\nWe have described so far that Slit–Robo signalling plays crucial roles during multiple aspects of nervous system development, extending beyond its classical role on axon guidance. Neurons form synapses with other neurons or different cell types to maintain the flow of information within a circuit. Unsurprisingly, Slit–Robo signalling has also been involved in synapse formation in vertebrates, although not too much is known about the mechanisms of this process. Morphological changes also occur, as neurons need to arborize and stratify on specific sites. In the zebrafish optic tectum, Slit1a inhibits arborization and premature maturation of axon terminals of retinal ganglion cells, and also reduces presynaptic sites via a Robo2-dependent and independent mechanism [\nSince the discovery of Slit and Robo proteins in late 1980s and 1990s, we have obtained a substantial amount of data that have allowed us to learn the way axons select their path in detail. In the future, we will probably understand better how complex interactions between this pathway and others involved in axonal pathfinding work together to steer the growth cone and establish the normal wiring patterns.\nAfter the initial discovery of Slit in flies, and later confirmation of an orthologue gene in vertebrates, we have unravelled several molecular details about the way this pathway works in both systems. Interestingly, there are big similarities, not only in the way the signal is transduced but also in the expression patterns. Notably, both models are still actively contributing towards a better understanding of the pathway. One pending task is that some aspects have been assessed only in\nA challenge for the future is to identify pharmacological modulators of this pathway, for which screenings using invertebrate models could provide an excellent platform for later validation in mammalian models. Drugs that modulate this pathway could aid in the treatment of neurodevelopmental disorders and cancer, as recent literature increasingly supports the role of Slit and Robo in these diseases.", "content_for_embedding": "A key event in our comprehension of axonal guidance and cell migration during development came with the discovery of Slit proteins. Slit was first detected in the fly embryonic midline glial cells of the ventral nerve cord (VNC), from where it is released to the extracellular space and distributed along axon tracts. Slit is a member of the protein family containing EGF-like repeats [\nThe discovery of Robo came out of a\nSubsequent research revealed Slit as a ligand for the Robo receptor in both vertebrates and\nSlit is an extracellular matrix-secreted glycoprotein [\nAll Slit proteins share a common structural framework (\nSchematic representation of Slit and Robo structure. (A) Mammals encode three Slit and\nThe specific functions of Slit fragments were investigated\nSlit fragments interactions. Information about the specific functions of Slit fragments has only been recently investigated\nRecently, a Slit metalloprotease able to generate Slit-N and Slit-C was identified. This protein is a member of the BMP1/Tolloid family of Astacin-like metalloproteases and was named Tolkin (Tok; also known as Tolloid-related, Tlr, or Piranha). In\nRobo is a single-pass transmembrane receptor belonging to a subfamily of the immunoglobulin (Ig) superfamily proteins, with an apparent molecular weight of approximately 180 kDa for\nThe function of the Robo cytoplasmic domain in signalling has been studied in some detail. It is responsible for mediating the repulsive response, as demonstrated using a chimeric receptor consisting of the Robo ectodomain and Frazzled (Fra, a receptor involved in growth cone attraction that responds to the protein Netrin) cytoplasmic domain. In these experiments performed in\nRegarding the ectodomain, the requirement for the repulsive function of Robo1 Ig1 domain has been highlighted recently [\nIn the\nExpression patterns of Slit and Robo are dynamic during development and have been investigated in detail in some systems. Here we will give an overview that will be complemented in the next sections with more in-depth analysis for specific examples. In invertebrates such as the fly Slit is secreted from ventral midline cells during embryo development (\nMechanisms of Slit–Robo-mediated axon guidance in the mouse and fly central nervous system. (A) The fly VNC is the region in which Slit expression and function is best characterized. Slit is secreted by the midline glia and axons will respond to Slit according to the presence of Robo in the membrane of the growth cone. (B) In the mouse spinal cord, pre-crossing commissural axons do not express Robo1/2 (signal OFF) on their surface. However, as they reach the midline, they express Robo1/2 (signal ON) and respond to Slit secreted from the floor plate (FP) glia to exit this structure. P: posterior, A: anterior. (C) In the\nSlit distribution in the vertebrate brain is more complex than in the spinal cord, implying more diversified roles in axon guidance and other developmental processes in this region. For instance, in the mouse visual system, only Slit1 and Slit2 are expressed, whereas in zebrafish, Slit2 and Slit3 are both required and cooperate in guiding retinal ganglion cell axons [\nRobo1 and Robo2 proteins in the spinal cord are predominantly expressed in post-crossing axons [\nThis section provides a general overview of the regulatory mechanisms governing Slit and Robo expression, function and signalling output. Some of these mechanisms are described in more detail, along with specific examples, in the following sections.\nDuring nervous system development, the expression patterns of the Slit and Robo genes are closely regulated. The spatiotemporal expression of Slit and Robo is controlled by transcriptional regulation, which is essential for axonal pathfinding and neuronal migration. This fine regulation ensures that neurons migrate to the correct places and axons are guided to the proper targets. In pontine neurons (PN) along the anteroposterior (AP) axis during mouse hindbrain development, Robo2 is a direct target of Hoxa2, a homeodomain transcription factor of the Hox gene family [\nAnother layer of regulation is the post-transcriptional control. For instance, in the chicken spinal cord, the microRNA miR-92 suppresses Robo1 translation specifically in precrossing commissural axons. miR-92 binds to the 3′ UTR of\nRegarding post-translational regulation, during vertebrate commissural axon midline crossing, a ubiquitin-specific protease 33 (USP33) maintains the stability of Robo1 after its interaction with Slit. USP33 promotes Robo1 deubiquitination, which may prevent degradation of the Robo1 receptor and/or enable its recycling following Slit stimulation [\nSlit interacts not only with Robo receptors but also with other receptors, transmembrane proteins and extracellular matrix components, which act as co-receptors or help with its proper extracellular localization. For instance, Slit-C fragment (and probably full-length Slit) can bind to Dystroglycan, a transmembrane glycoprotein, through its laminin-G domain (\nSlit-Robo signalling. (A) The interaction with heparan sulfate proteoglycans (HSPGs) enhances Slit–Robo signalling in both vertebrates and\nIn vertebrates, cell-surface heparan sulfate proteoglycans (HSPGs) play a critical role in enhancing Slit–Robo signalling by binding to the D2 domain of Slit2 [\nIn addition to functioning as a ligand–receptor pair, Slit and Robo influence axon development through interactions with additional receptors and ligands, respectively (\nNotably, the mammalian Robo3 does not bind to Slit [\nUpon activation, Slit–Robo signalling initiates a series of intracellular events that can result in cytoskeletal reorganization and other cellular responses (\nIt has been shown that Robo1 receptors must be internalized via endocytosis to ensure proper intracellular signalling. Specifically, Chance & Bashaw [\nMoreover, Slit–Robo regulates neurogenesis in mammalian neural progenitor cells by interacting with the Notch pathway and transcriptionally activating the Notch effector Hes1. However, the activation of Hes1 by Robo1/2’s is independent of Notch signalling and is mediated by the Robo cytoplasmic domains [\nSmall GTPases are GTP-binding proteins that modify the cytoskeleton in order to control cell movement, and they play a central role in transducing signals downstream of activated Robo receptors. The small GTP-binding proteins act as molecular switches, cycling between active GTP-bound and inactive GDP-bound states to regulate various cellular processes, including cytoskeletal dynamics. Robo receptors recruit GTPases and their regulatory proteins, including GEFs (guanine nucleotide exchange factors), Dock/Nck (mammal Nck) and srGAPs (Slit–Robo GTPase activating proteins, in vertebrates) to modulate their activity and downstream signalling pathways. Thus, recruiting RhoGEF proteins downstream of the cytoplasmic Robo domain is required for Slit–Robo signalling [\nThe Slit–Robo signalling regulates actin dynamics by modulating the activity of small GTPases and other cytoskeletal regulators proteins. Wong\nUpon activation, the Robo ectodomain is cleaved and the rest of the protein undergoes internalization. This process seems to be required to activate repulsive signalling by the recruitment of Son of Sevenless (Sos) [\nAnother downstream effector of Slit–Robo signalling is Enabled (Ena). The Ena proline-rich protein family that promotes actin polymerization and cell motility. Ena interacts with the Robo receptor to enhance axonal repulsion by stopping the formation and extension of filopodia when responding to Slit [\nSlit–Robo signalling interacts with N-cadherin to regulate cell adhesion and neurite growth. This has been studied in chick retinal cells, where Slit binding to Robo receptors triggers Abl to interact with the intracellular domain (CC3) of Robo, facilitating a connection between the Robo and N-cadherin via the protein Cables a substrate of Abl and CDK5. Upon forming this protein complex, β-catenin is phosphorylated by Abl, leading to its dissociation from N-cadherin. As a result, cell attachment mediated by N-cadherin is compromised and neurites are not formed in cell culture experiments. Furthermore, phosphorylated β-catenin moves into the nucleus, to modify gene expression [\nThe VNC of\nSlit is the main repulsive guidance cue secreted by the midline glia, in\nNevertheless, Comm is not the only way that Slit and Robo availability can be modulated during the crossing of the midline in\nBesides its function in midline crossing, the Slit–Robo pathway is also involved in the positioning of longitudinal axonal tracks in the medio-lateral axis [\nIn vertebrates, axon guidance molecules have been studied in the commissural neurons of the spinal cord for decades (\nSlit–Robo classically regulates axon midline crossing by a repulsion mechanism, although the specific mechanisms used by the distinct Robo receptors diverge. Slits in the spinal cord are secreted by floor plate cells, which are located ventrally [\nIndeed, loss of Robo3 caused commissural axons to re-route and turn into the longitudinal axis without crossing the midline [\nRobo3 has multiple isoforms. Two of these isoforms [\nSlit–Robo signalling is also involved in guidance of motor axons. The spinal cord motor columns house the neurons that innervate the muscles, the motor neurons. These neurons extend their axons from the central nervous system (the spinal cord) to the periphery (muscle) and they exit through specific locations of the spinal cord. Robo2 and Slits mediate the exit of spinal accessory motor neurons through the lateral exit point possibly through a short-range attractive mechanism [\nMechanistically, the fine regulation of attractive cues and the repulsive activity of Slit–Robo signalling is required to ensure axons to be attracted to an intermediate target, such as the floor plate at the midline, and once they have reached it to continue their journey away from this structure. A reported mechanism for regulating the timing and location of repulsive activity involves the expression of Robo receptors in axonal growth cones by alternative splicing, discovering a previously unknown layer of regulation [\nAlternatively, insertion of Robo1 into the cell surface is modulated by RabGDI (Rab Guanine Nucleotide Dissociation Inhibitor) in chick embryos, but the mechanism is different from that of Comm. RabGDI is expressed specifically during midline crossing and promotes Robo1-containing vesicles to be shuttled to the growth cone membrane by Calsyntenin-1 [\nThe repulsive role of the Slit–Robo signalling has also been described in axons other than commissural axons. In zebrafish, axons that descend ipsilaterally from the ventral diencephalon are repelled from the midline by Robo2 [\nThere is a well-documented relationship between the Slit–Robo and Netrin-DCC signalling pathways during the development of the nervous system. Netrin is a secreted guidance cue which has been involved in attraction and repulsion of axons depending on the context [\nIn mammals, Netrin1-DCC and Slit–Robo signalling pathways also have opposing roles in spinal cord midline crossing. Although, by contrast to the fly, it was originally thought that the action of both pathways was sequential in their common goal to steer axons towards the floor plate (Netrin-DCC) and subsequently avoid re-entry into this intermediate target (Slit–Robo) [\nThe Slit–Robo pathway plays roles in several regions of the\nStrikingly, sex specific regulation of Robo has been implicated in the formation of sexually dimorphic neuronal circuits [\nThe retina of\nSimilar to the fly brain, in vertebrates Slit distribution in the developing brain is more complex than in the spinal cord (\nSlit–Robo mediated repulsion also modulates the pathfinding of axons that project from retinal ganglion cells (RGCs) in the eye to the brain in mammals. Normally, in mice, a relatively large proportion of RGC axons cross the midline to form the optic chiasm, whereas a relatively low proportion of axons remain ipsilateral [\nSo far there is little data supporting a function of Slit–Robo signalling in early neurogenesis of the fly CNS. One study has shown that Slit regulates asymmetric division of a population of ganglion mother cell (GMC1) derived from neuroblast NB4.2 in the VNC of the\nBy contrast, the function of Slit–Robo has been addressed in several studies using the mouse as a model. During early cortical development, neuroepithelial progenitors in the ventricular zone (apical side) divide symmetrically to increase their number. Later on, excitatory projection neurons are generated by asymmetrical divisions either from apical radial glia cells (direct neurogenesis) or from basal intermediate progenitor cells (indirect neurogenesis) [\nAnother aspect of neurogenesis is the process of newly born neurons detaching from the apical side soon after cell division. During retinogenesis, the newborn neurons also detach from the apical (outer) neuroepithelium. In zebrafish, the apical retraction of RGCs depends on Slit1b and Robo3 [\nBesides axonal guidance, Slit–Robo is also involved more widely in the architecture of brain neuropils by defining boundaries that limit the localization of different cellular populations. This function has been described in the optic lobe of\nThe fly optic lobe shares some similarities with the mammalian cortex, since neuronal migration has been shown to play a role in the assembly of neuronal circuits [\nSlit–Robo signalling regulates neuronal migration through the modulation of focal adhesion kinase (FAK) activity and cytoskeletal dynamics in vertebrates. Slit2 influences Cdc42 activity and lamellipodia formation on\nIn vertebrates Slit–Robo has also a role in neuronal migration. Slit was first discovered to influence directional migration of neuroblasts to the olfactory bulb [\nSame as the typical commissural neurons in the spinal cord, inferior olive neurons of the brainstem extend axons that cross the midline in route to their target in the contralateral side of the cerebellum. However, their somas do not cross this structure, despite their dorso-ventral migration towards the vicinity of the midline. Migrating inferior olive neurons express Robo3, and in its absence they still form the inferior olive nucleus in its proper location, albeit more slowly and with abnormal cellular shape and polarization [\nIn the spinal cord, dI1 neurons migrate slightly in the ventral direction from the dorsal area after their axons cross the midline. Robo2 is enriched in the ipsilateral subpopulation of dI1 neurons. In its absence, the cell bodies of ipsilateral neurons (but not the commissural population) are distributed in regions that are too ventral and/or medial. This effect of Robo2 seems to be a partial result of Slit2 action in the surrounding regions of dI1 neuron migration, since the phenotype in\nMotor neuron cell bodies are contained in the ventral horn of the spinal cord and brainstem. Most motor neurons are born close to their final residence (in the ventral progenitor area), therefore the notion that they do not migrate much. In the spinal cord and brainstem, the repulsive action of Slit–Robo signalling is crucial for motor neurons to reach their proper location, as in Robo1/2 or Slits1/2/3 mutant mice these cell bodies are misplaced in the floor plate, as opposed to be adjacent to it in control animals. This effect seems to be counteracted by Netrin-DCC signalling, suggesting a balance of attractive and repulsive forces to keep the neuronal somas in place [\nApart from the active migration that motor neurons need to undergo, it is crucial that these neurons do not migrate outside of the central nervous system into the periphery. Several studies have shown that guidance molecules prevent exit of motor neurons to the periphery by regulating the function of boundary cap cells [\nRobo1 influences the entrance and the migration of interneurons throughout the cerebral cortex at the stages when these neurons are\nA system that uses guidepost cells to physically narrow and permit the path of thalamic axons\nIn addition to tangential migration of inhibitory neurons, Robo1 and Robo4 play a role in the migration of excitatory pyramidal neurons to the cortex [\nWe have described so far that Slit–Robo signalling plays crucial roles during multiple aspects of nervous system development, extending beyond its classical role on axon guidance. Neurons form synapses with other neurons or different cell types to maintain the flow of information within a circuit. Unsurprisingly, Slit–Robo signalling has also been involved in synapse formation in vertebrates, although not too much is known about the mechanisms of this process. Morphological changes also occur, as neurons need to arborize and stratify on specific sites. In the zebrafish optic tectum, Slit1a inhibits arborization and premature maturation of axon terminals of retinal ganglion cells, and also reduces presynaptic sites via a Robo2-dependent and independent mechanism [\nSince the discovery of Slit and Robo proteins in late 1980s and 1990s, we have obtained a substantial amount of data that have allowed us to learn the way axons select their path in detail. In the future, we will probably understand better how complex interactions between this pathway and others involved in axonal pathfinding work together to steer the growth cone and establish the normal wiring patterns.\nAfter the initial discovery of Slit in flies, and later confirmation of an orthologue gene in vertebrates, we have unravelled several molecular details about the way this pathway works in both systems. Interestingly, there are big similarities, not only in the way the signal is transduced but also in the expression patterns. Notably, both models are still actively contributing towards a better understanding of the pathway. One pending task is that some aspects have been assessed only in\nA challenge for the future is to identify pharmacological modulators of this pathway, for which screenings using invertebrate models could provide an excellent platform for later validation in mammalian models. Drugs that modulate this pathway could aid in the treatment of neurodevelopmental disorders and cancer, as recent literature increasingly supports the role of Slit and Robo in these diseases.", "topic": "Brain"}
{"pmid": "38351934", "pmcid": "12309524", "title": "Sex-specific alterations in the mRNA expression of histone\ndeacetylases (HDACs) in the rat brain following prolonged abstinence from\nmethamphetamine self-administration", "publication_year": "N/A", "abstract": "Methamphetamine (METH) use disorder (MUD) is a psychiatric disease that\nimposes substantial health burdens throughout the world. Significant\nsex-specific differences in misuse and relapse rates exist among human METH\nusers and in preclinical models of MUD. We have been using a METH\nself-administration (SA) model to identify molecular substrates of sex-related\nbehavioral manifestations. Rats were trained to self-administer METH (0.1\nmg/kg/injection, i.v.) over 20 days. Hippocampus (HIP), prefrontal cortex (PFC),\nnucleus accumbens (NAc), and dorsal striatum (dSTR) were dissected and used to\nmeasure mRNA expression of HDACs because of their potential involvement in MUD.\nCompared to females, males self-administered more METH. Quantitative PCR\nrevealed that control male rats had higher basal", "full_text": "", "content_for_embedding": "Methamphetamine (METH) use disorder (MUD) is a psychiatric disease that\nimposes substantial health burdens throughout the world. Significant\nsex-specific differences in misuse and relapse rates exist among human METH\nusers and in preclinical models of MUD. We have been using a METH\nself-administration (SA) model to identify molecular substrates of sex-related\nbehavioral manifestations. Rats were trained to self-administer METH (0.1\nmg/kg/injection, i.v.) over 20 days. Hippocampus (HIP), prefrontal cortex (PFC),\nnucleus accumbens (NAc), and dorsal striatum (dSTR) were dissected and used to\nmeasure mRNA expression of HDACs because of their potential involvement in MUD.\nCompared to females, males self-administered more METH. Quantitative PCR\nrevealed that control male rats had higher basal", "topic": "Brain"}
{"pmid": "38297018", "pmcid": "12310195", "title": "New Donor Selection Criteria Result in Optimal Outcomes of Kidneys from Uncontrolled Donation After the Circulatory Determination of Death", "publication_year": "N/A", "abstract": "", "full_text": "Kidney transplantation is the most effective treatment for patients with kidney failure and is associated with better survival and quality of life compared with chronic dialysis.\nDonation after the circulatory determination of death (DCDD) emerged in the 1990s as a promising alternative to address the disparity between organ supply and demand.\nSurprisingly, the limited implementation of uDCDD programs persists despite recommendations from the European Resuscitation Guidelines to consider uDCDD when advanced cardiopulmonary resuscitation (aCPR) is deemed unsuccessful.\nuDCDD represents a vast and underused source of kidneys for transplantation. Although the literature describing the outcomes of kidneys obtained from uDCDD donors is limited, available studies report satisfactory long-term graft survival.\nThe inclusion criteria for uDCDD donors have remained unchanged during the past 20 y, reflecting the limited research conducted on this type of donation.\nApplying stricter and more refined inclusion criteria may improve short-term outcomes compared with those traditionally reported. This study aims to share our experience with uDCDD kidney transplantation based on a protocol that incorporates new and easy-to-apply criteria for donor selection, potentially addressing some of the challenges and maximizing the benefits of this underused donor pool.\nData were prospectively obtained from all kidney recipients who received a graft from a local uDCDD donor between the start of our program (December 2013) and April 2024. Follow-up continued for all recipients until May 2024. The control group comprised all kidney transplant recipients during the same period who received organs from local DNDD donors meeting standard criteria as defined by the United Network for Organ Sharing—donors younger than 50 y or donors younger than 60 y with a maximum of one of the following: history of arterial hypertension, serum creatinine >130 mmol/L, or death due to cerebrovascular events.\nThe study protocol received institutional review board approval, adhering to Spanish legislation.\nThe donor selection criteria for our uDCDD program are described in Table\nCriteria for the selection of potential uDCDD kidney donors\nA-NRP, abdominal normothermic regional perfusion; E-CPR, extracorporeal-assisted cardiopulmonary resuscitation; uDCDD, uncontrolled donation after the circulatory determination.\nThe uDCDD protocol has been detailed previously.\nIf donor eligibility criteria were met, cardiac compression and mechanical ventilation would continue beyond futility to preserve donation opportunities during the transfer of the potential donor to the hospital. All potential uDCDD donors were transferred to the hospital with mechanical cardiac compression using the LUCAS 2 device.\nTo minimize WIT, the potential uDCDD donor was directly transferred to the intensive care unit (ICU), where death was declared and legal permission was obtained to proceed with postmortem preservation measures. The declaration of death was performed by the ICU physician on duty, who was independent of the donation and transplantation team. In Spain, the diagnosis of death based on circulatory criteria only requires a physician independent of the transplant team and the coordination team. This physician declared death after confirming that aCPR had been exhausted and that there was no indication for alternative therapeutic interventions, as well as after observing a “no-touch period” of 5 min of complete absence of cardiopulmonary activity, defined by the absence of respiratory movements and electrical activity on the electrocardiogram. Postmortem interventions entailed the cannulation of femoral vessels and the initiation of abdominal normothermic regional perfusion (A-NRP). A double-lumen cannula (ThruPort Systems, Edwards) was inserted into the femoral artery, and through it, an aortic occlusion balloon was placed, avoiding the need to cannulate the contralateral groin and minimizing WIT.\nRelatives were approached to discuss donation opportunities and assess whether organ donation was consistent with the person’s wishes and values. On consent and confirmation of medical eligibility, the donor was transferred to the operation theater for kidney recovery. All recovered kidneys underwent machine perfusion (LifePort; Organ Recovery System, Diegem, Belgium) for at least 2 h. Kidneys with a vascular resistance index (RI) >0.3 mm Hg/mL/min were discarded.\nRecipients of uDCDD kidney grafts received induction with thymoglobulin and immunosuppressive treatment based on the delayed introduction of tacrolimus, associated with mycophenolate mofetil and steroids. Standard criteria DNDD kidney recipients received standard immunosuppression (tacrolimus, mycophenolate mofetil, and steroids). According to the treating team, induction with thymoglobulin or basiliximab was associated with patients at high risk of acute rejection or DGF.\nThe no-flow period was defined as the time from CA to the start of aCPR. WIT extended from CA to the initiation of in situ preservation with A-NRP.\nPNF was defined as the failure of a graft to ever function. DGF was considered by the need for dialysis during the first week after transplantation. Death-censored graft survival was calculated from the date of transplantation to the date of irreversible graft failure, defined by return to long-term dialysis or retransplantation. In the event of death with a functioning graft, the follow-up period was censored at the date of death. The glomerular filtration rate (GFR) was estimated by using the Chronic Kidney Disease Epidemiology Collaboration equation.\nA descriptive analysis was performed, presenting categorical variables as absolute numbers and percentages and continuous variables as measures of central tendency and dispersion. Comparisons between uDCDD and standard criteria DNDD kidney recipients were conducted using the Student’s\nBetween December 2013 and April 2024, the EMS reported 109 potential uDCDD donors to the donor coordination team. In 69 cases, the uDCDD pathway was not activated for the eligibility criteria were not met in most cases or logistical problems such as being with a cDCDD or DNDD donor at the same time (see Figure\nStudy profile.\nBaseline donor and recipient data, as well as posttransplant outcomes, are summarized in Table\nBaseline features and outcomes of recipients of kidneys from uDCDD vs standard criteria DNDD donors\naCPR, advanced cardiopulmonary resuscitation; A-NRP, abdominal normothermic regional perfusion; CIT, cold ischemic time; cPRA, calculated panel-reactive antibody; DGF, delayed graft function; DNDD, donation after the neurological determination of death; ICU, intensive care unit; IQR, interquartile range; PNF, primary nonfunction; T0, time of cardiac arrest; uDCDD, uncontrolled donation after the circulatory determination of death.\nNo statistically significant differences were observed in the clinical or demographic characteristics of donors and recipients between the 2 groups. However, recipients of uDCDD donors had a significantly shorter median cold ischemic time (CIT) compared with recipients of DNDD kidneys (15 h [IQR, 9–21] versus 19 h [IQR, 15.5–22];\nThe incidence of PNF was low in both groups, with no cases reported in the standard DNDD cohort and only 1 case (2.3%) in the uDCDD group (\nSequential serum creatinine levels (A) and glomerular filtration rate (B) in kidney recipients from both groups (in black uDCDD; in gray DNDD). The bar represents the median and the error bars the interquartile range for each time point. A general linear model for a repeated measures test was used. At all-time points\nKaplan-Meier graphs. A, Death-censored kidney graft survival. B, No death-censored kidney graft survival. DNDD, donation after the neurological determination of death; uDCDD, uncontrolled donation after the circulatory determination of death.\nRecipients of uDCDD kidneys had a significantly higher probability of developing DGF in univariate analysis (odds ratio, 3.2; 95% confidence interval [CI], 1.4-7.2;\nOur experience with the transplantation of highly selected uDCDD kidneys confirms the feasibility of these programs and demonstrates that posttransplant outcomes can be comparable with those achieved with standard criteria DNDD donors. Published literature on uDCDD kidney transplants has often reported suboptimal outcomes,\nTo justify the value of uDCDD programs, most researches have compared the outcomes of recipients of kidneys from uDCDD donors with those from DNDD donors meeting expanded criteria.\nOne major limitation of uDCDD outcomes is the lack of evolution in donor acceptance criteria during the past 2 decades. Some centers in Spain continue to use unrestricted donor selection criteria established >20 y ago,\nOur protocol was designed to minimize these risks by excluding donors who did not meet at least 1 of 3 ideal criteria: donor age younger than 50 y, no-flow period of <10 min, or WIT of <120 min. These criteria, informed by previous studies, aimed to optimize organ utilization and posttransplant outcomes. Viglietti et al\nPNF remains the most concerning complication of uDCDD kidney transplantation, with reported incidences ranging from 1.8% to 20%.\nSeveral factors such as advance donor age, prolonged WIT, extended no-flow periods, or in situ cooling as a preservation method have been found to be associated with high rates of PNF in uDCDD.\nDespite a higher DGF rate in our uDCDD group (46%) compared with the DNDD group, this was lower than what was previously documented.\nOur 5-y graft survival rate of 92% surpasses previously reported rates\nWe also analyzed data on kidney function in the long term, which were again appropriate in recipients of kidneys from uDCDD donors, with no differences in serum creatinine or eGFR values at 1 and 5 y compared with recipients of ideal DNDD kidneys. The description of analytical parameters in our series is of great importance since most studies published on uDCDD kidney transplantation have only reported survival data, with no information on graft function.\nOne of the main discouraging issues in uDCDD is the high kidney discard rate, >30% in large Spanish studies.\nWe would like to emphasize that the favorable results obtained in our center with the transplantation of uDCDD kidneys were not related to a restrictive selection of kidney recipients. In fact, there were no differences between both recipient groups in donor age, recipient age, rate of sensitized patients, or the percentage of second recipient transplant.\nThe keystone of successful uDCDD programs is optimizing all logistical requirements with the clear goal of reducing warm ischemic injury. Although we cannot interfere with donor age or in the duration of no-flow periods that all EMS try to reduce as much as possible to increase the probability of return of spontaneous circulation and save lives, we are able to reduce the duration of intrahospital procedures to minimize the period until A-NRP. In this regard, the obvious feature that distinguishes our uDCDD program from others is that both the diagnosis of death and the start of A-NRP were performed in the ICU. The centralization of both phases of the process in the same unit helped us to avoid in-hospital transfers that turned out to be difficult in multimonitored patients subject to mechanical cardiac compression and mechanical ventilation. This approach reduces WIT and largely contributes to improving graft quality.\nIn our study, all kidneys were perfused on pulsatile hypothermic machine perfusion (HMP). Pulsatile HMP has shown to be effective in reducing the incidence of DGF, both in DNDD and DCDD, but has not been associated with a decreased risk of PNF.\nThere has been a debate concerning the coexistence of E-CPR and uDCDD programs. In our hospital, E-CPR is available, and to activate the uDCDD procedure, it is mandatory to check that there is no indication to activate the E-CPR pathway. The inclusion criteria for both programs are different.\nWe are aware of the ethical-legal barriers to setting up DCDD programs,\nOur study has several limitations. Our kidney survival data have included our learning curve in uDCDD, and the study period is >10 y, although the number of cases with a prolonged follow-up is still extremely low. We are also aware of the small sample size and that our study was not randomized. Finally, although we used a control group with similar characteristics, recipients were selected to receive a DNDD or a uDCDD graft based on an assessment of donor-recipient compatibility and other data by the treating team, so we cannot exclude a certain selection bias.\nIn summary, our results support that obtaining renal grafts from uDCDD donors is not only feasible but also provides results that are as good as those obtained from standard DNDD donors. Strict donor selection criteria, improved in-hospital logistics to reduce WIT, and the systematic use of A-NRP are key factors for success in terms of kidney utilization and posttransplant outcomes.\nThe authors wish to thank all members of the donor coordination units and kidney transplant teams of centers that provided data on kidney recipients transplanted with uDCDD grafts obtained at our center.", "content_for_embedding": "Kidney transplantation is the most effective treatment for patients with kidney failure and is associated with better survival and quality of life compared with chronic dialysis.\nDonation after the circulatory determination of death (DCDD) emerged in the 1990s as a promising alternative to address the disparity between organ supply and demand.\nSurprisingly, the limited implementation of uDCDD programs persists despite recommendations from the European Resuscitation Guidelines to consider uDCDD when advanced cardiopulmonary resuscitation (aCPR) is deemed unsuccessful.\nuDCDD represents a vast and underused source of kidneys for transplantation. Although the literature describing the outcomes of kidneys obtained from uDCDD donors is limited, available studies report satisfactory long-term graft survival.\nThe inclusion criteria for uDCDD donors have remained unchanged during the past 20 y, reflecting the limited research conducted on this type of donation.\nApplying stricter and more refined inclusion criteria may improve short-term outcomes compared with those traditionally reported. This study aims to share our experience with uDCDD kidney transplantation based on a protocol that incorporates new and easy-to-apply criteria for donor selection, potentially addressing some of the challenges and maximizing the benefits of this underused donor pool.\nData were prospectively obtained from all kidney recipients who received a graft from a local uDCDD donor between the start of our program (December 2013) and April 2024. Follow-up continued for all recipients until May 2024. The control group comprised all kidney transplant recipients during the same period who received organs from local DNDD donors meeting standard criteria as defined by the United Network for Organ Sharing—donors younger than 50 y or donors younger than 60 y with a maximum of one of the following: history of arterial hypertension, serum creatinine >130 mmol/L, or death due to cerebrovascular events.\nThe study protocol received institutional review board approval, adhering to Spanish legislation.\nThe donor selection criteria for our uDCDD program are described in Table\nCriteria for the selection of potential uDCDD kidney donors\nA-NRP, abdominal normothermic regional perfusion; E-CPR, extracorporeal-assisted cardiopulmonary resuscitation; uDCDD, uncontrolled donation after the circulatory determination.\nThe uDCDD protocol has been detailed previously.\nIf donor eligibility criteria were met, cardiac compression and mechanical ventilation would continue beyond futility to preserve donation opportunities during the transfer of the potential donor to the hospital. All potential uDCDD donors were transferred to the hospital with mechanical cardiac compression using the LUCAS 2 device.\nTo minimize WIT, the potential uDCDD donor was directly transferred to the intensive care unit (ICU), where death was declared and legal permission was obtained to proceed with postmortem preservation measures. The declaration of death was performed by the ICU physician on duty, who was independent of the donation and transplantation team. In Spain, the diagnosis of death based on circulatory criteria only requires a physician independent of the transplant team and the coordination team. This physician declared death after confirming that aCPR had been exhausted and that there was no indication for alternative therapeutic interventions, as well as after observing a “no-touch period” of 5 min of complete absence of cardiopulmonary activity, defined by the absence of respiratory movements and electrical activity on the electrocardiogram. Postmortem interventions entailed the cannulation of femoral vessels and the initiation of abdominal normothermic regional perfusion (A-NRP). A double-lumen cannula (ThruPort Systems, Edwards) was inserted into the femoral artery, and through it, an aortic occlusion balloon was placed, avoiding the need to cannulate the contralateral groin and minimizing WIT.\nRelatives were approached to discuss donation opportunities and assess whether organ donation was consistent with the person’s wishes and values. On consent and confirmation of medical eligibility, the donor was transferred to the operation theater for kidney recovery. All recovered kidneys underwent machine perfusion (LifePort; Organ Recovery System, Diegem, Belgium) for at least 2 h. Kidneys with a vascular resistance index (RI) >0.3 mm Hg/mL/min were discarded.\nRecipients of uDCDD kidney grafts received induction with thymoglobulin and immunosuppressive treatment based on the delayed introduction of tacrolimus, associated with mycophenolate mofetil and steroids. Standard criteria DNDD kidney recipients received standard immunosuppression (tacrolimus, mycophenolate mofetil, and steroids). According to the treating team, induction with thymoglobulin or basiliximab was associated with patients at high risk of acute rejection or DGF.\nThe no-flow period was defined as the time from CA to the start of aCPR. WIT extended from CA to the initiation of in situ preservation with A-NRP.\nPNF was defined as the failure of a graft to ever function. DGF was considered by the need for dialysis during the first week after transplantation. Death-censored graft survival was calculated from the date of transplantation to the date of irreversible graft failure, defined by return to long-term dialysis or retransplantation. In the event of death with a functioning graft, the follow-up period was censored at the date of death. The glomerular filtration rate (GFR) was estimated by using the Chronic Kidney Disease Epidemiology Collaboration equation.\nA descriptive analysis was performed, presenting categorical variables as absolute numbers and percentages and continuous variables as measures of central tendency and dispersion. Comparisons between uDCDD and standard criteria DNDD kidney recipients were conducted using the Student’s\nBetween December 2013 and April 2024, the EMS reported 109 potential uDCDD donors to the donor coordination team. In 69 cases, the uDCDD pathway was not activated for the eligibility criteria were not met in most cases or logistical problems such as being with a cDCDD or DNDD donor at the same time (see Figure\nStudy profile.\nBaseline donor and recipient data, as well as posttransplant outcomes, are summarized in Table\nBaseline features and outcomes of recipients of kidneys from uDCDD vs standard criteria DNDD donors\naCPR, advanced cardiopulmonary resuscitation; A-NRP, abdominal normothermic regional perfusion; CIT, cold ischemic time; cPRA, calculated panel-reactive antibody; DGF, delayed graft function; DNDD, donation after the neurological determination of death; ICU, intensive care unit; IQR, interquartile range; PNF, primary nonfunction; T0, time of cardiac arrest; uDCDD, uncontrolled donation after the circulatory determination of death.\nNo statistically significant differences were observed in the clinical or demographic characteristics of donors and recipients between the 2 groups. However, recipients of uDCDD donors had a significantly shorter median cold ischemic time (CIT) compared with recipients of DNDD kidneys (15 h [IQR, 9–21] versus 19 h [IQR, 15.5–22];\nThe incidence of PNF was low in both groups, with no cases reported in the standard DNDD cohort and only 1 case (2.3%) in the uDCDD group (\nSequential serum creatinine levels (A) and glomerular filtration rate (B) in kidney recipients from both groups (in black uDCDD; in gray DNDD). The bar represents the median and the error bars the interquartile range for each time point. A general linear model for a repeated measures test was used. At all-time points\nKaplan-Meier graphs. A, Death-censored kidney graft survival. B, No death-censored kidney graft survival. DNDD, donation after the neurological determination of death; uDCDD, uncontrolled donation after the circulatory determination of death.\nRecipients of uDCDD kidneys had a significantly higher probability of developing DGF in univariate analysis (odds ratio, 3.2; 95% confidence interval [CI], 1.4-7.2;\nOur experience with the transplantation of highly selected uDCDD kidneys confirms the feasibility of these programs and demonstrates that posttransplant outcomes can be comparable with those achieved with standard criteria DNDD donors. Published literature on uDCDD kidney transplants has often reported suboptimal outcomes,\nTo justify the value of uDCDD programs, most researches have compared the outcomes of recipients of kidneys from uDCDD donors with those from DNDD donors meeting expanded criteria.\nOne major limitation of uDCDD outcomes is the lack of evolution in donor acceptance criteria during the past 2 decades. Some centers in Spain continue to use unrestricted donor selection criteria established >20 y ago,\nOur protocol was designed to minimize these risks by excluding donors who did not meet at least 1 of 3 ideal criteria: donor age younger than 50 y, no-flow period of <10 min, or WIT of <120 min. These criteria, informed by previous studies, aimed to optimize organ utilization and posttransplant outcomes. Viglietti et al\nPNF remains the most concerning complication of uDCDD kidney transplantation, with reported incidences ranging from 1.8% to 20%.\nSeveral factors such as advance donor age, prolonged WIT, extended no-flow periods, or in situ cooling as a preservation method have been found to be associated with high rates of PNF in uDCDD.\nDespite a higher DGF rate in our uDCDD group (46%) compared with the DNDD group, this was lower than what was previously documented.\nOur 5-y graft survival rate of 92% surpasses previously reported rates\nWe also analyzed data on kidney function in the long term, which were again appropriate in recipients of kidneys from uDCDD donors, with no differences in serum creatinine or eGFR values at 1 and 5 y compared with recipients of ideal DNDD kidneys. The description of analytical parameters in our series is of great importance since most studies published on uDCDD kidney transplantation have only reported survival data, with no information on graft function.\nOne of the main discouraging issues in uDCDD is the high kidney discard rate, >30% in large Spanish studies.\nWe would like to emphasize that the favorable results obtained in our center with the transplantation of uDCDD kidneys were not related to a restrictive selection of kidney recipients. In fact, there were no differences between both recipient groups in donor age, recipient age, rate of sensitized patients, or the percentage of second recipient transplant.\nThe keystone of successful uDCDD programs is optimizing all logistical requirements with the clear goal of reducing warm ischemic injury. Although we cannot interfere with donor age or in the duration of no-flow periods that all EMS try to reduce as much as possible to increase the probability of return of spontaneous circulation and save lives, we are able to reduce the duration of intrahospital procedures to minimize the period until A-NRP. In this regard, the obvious feature that distinguishes our uDCDD program from others is that both the diagnosis of death and the start of A-NRP were performed in the ICU. The centralization of both phases of the process in the same unit helped us to avoid in-hospital transfers that turned out to be difficult in multimonitored patients subject to mechanical cardiac compression and mechanical ventilation. This approach reduces WIT and largely contributes to improving graft quality.\nIn our study, all kidneys were perfused on pulsatile hypothermic machine perfusion (HMP). Pulsatile HMP has shown to be effective in reducing the incidence of DGF, both in DNDD and DCDD, but has not been associated with a decreased risk of PNF.\nThere has been a debate concerning the coexistence of E-CPR and uDCDD programs. In our hospital, E-CPR is available, and to activate the uDCDD procedure, it is mandatory to check that there is no indication to activate the E-CPR pathway. The inclusion criteria for both programs are different.\nWe are aware of the ethical-legal barriers to setting up DCDD programs,\nOur study has several limitations. Our kidney survival data have included our learning curve in uDCDD, and the study period is >10 y, although the number of cases with a prolonged follow-up is still extremely low. We are also aware of the small sample size and that our study was not randomized. Finally, although we used a control group with similar characteristics, recipients were selected to receive a DNDD or a uDCDD graft based on an assessment of donor-recipient compatibility and other data by the treating team, so we cannot exclude a certain selection bias.\nIn summary, our results support that obtaining renal grafts from uDCDD donors is not only feasible but also provides results that are as good as those obtained from standard DNDD donors. Strict donor selection criteria, improved in-hospital logistics to reduce WIT, and the systematic use of A-NRP are key factors for success in terms of kidney utilization and posttransplant outcomes.\nThe authors wish to thank all members of the donor coordination units and kidney transplant teams of centers that provided data on kidney recipients transplanted with uDCDD grafts obtained at our center.", "topic": "Brain"}
{"pmid": "38248206", "pmcid": "12309689", "title": "Revealing rhythm categorization in human brain activity", "publication_year": "2025", "abstract": "Humans across cultures show an outstanding capacity to perceive, learn, and produce musical rhythms. These skills rely on mapping the infinite space of possible rhythmic sensory inputs onto a finite set of internal rhythm categories. What is the nature of the brain processes underlying rhythm categorization? We used electroencephalography to measure brain activity as human participants listened to a continuum of rhythmic sequences characterized by repeating patterns of two interonset intervals. Using frequency and representational similarity analyses, we show that brain activity does not merely track the temporal structure of rhythmic inputs but, instead, produces categorical representation of rhythms. These neural rhythm categories arise automatically, independent of any motor- or timing-related tasks, yet exhibit strong similarity with categorization observed in overt behavior. Together, these results and methodological advances constitute a critical step toward understanding the biological roots and diversity of musical behaviors across cultures.\nHuman brain activity shows automatic categorization of rhythm, independent of but similar to categories produced in behavior.", "full_text": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "content_for_embedding": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "topic": "Brain"}
{"pmid": "38185810", "pmcid": "12309511", "title": "Functional Neurologic Disorder, ", "publication_year": "N/A", "abstract": "Functional neurologic disorder is common and a significant cause of disability and stress in neurologic patients. The nature of this disorder has been unclear. Originally called hysteria, the disorder interested Charcot who postulated that a functional lesion,", "full_text": "", "content_for_embedding": "Functional neurologic disorder is common and a significant cause of disability and stress in neurologic patients. The nature of this disorder has been unclear. Originally called hysteria, the disorder interested Charcot who postulated that a functional lesion,", "topic": "Brain"}
{"pmid": "38140641", "pmcid": "12309689", "title": "Revealing rhythm categorization in human brain activity", "publication_year": "2025", "abstract": "Humans across cultures show an outstanding capacity to perceive, learn, and produce musical rhythms. These skills rely on mapping the infinite space of possible rhythmic sensory inputs onto a finite set of internal rhythm categories. What is the nature of the brain processes underlying rhythm categorization? We used electroencephalography to measure brain activity as human participants listened to a continuum of rhythmic sequences characterized by repeating patterns of two interonset intervals. Using frequency and representational similarity analyses, we show that brain activity does not merely track the temporal structure of rhythmic inputs but, instead, produces categorical representation of rhythms. These neural rhythm categories arise automatically, independent of any motor- or timing-related tasks, yet exhibit strong similarity with categorization observed in overt behavior. Together, these results and methodological advances constitute a critical step toward understanding the biological roots and diversity of musical behaviors across cultures.\nHuman brain activity shows automatic categorization of rhythm, independent of but similar to categories produced in behavior.", "full_text": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "content_for_embedding": "A fundamental function of the brain is to enable adaptive behavior in an environment full of remarkably diverse, dynamic sensory signals. Specifically, although constantly stimulated with a wide range of inputs, the brain does not treat each sensory input as a novel, unique event—a process that would be overwhelming for the organism—but, instead, categorizes it (\nA compelling illustration of this phenomenon is the central role categorization plays in human social interaction through musical rhythm (\nHence, rhythm categorization enables us to recognize musical rhythms, by allowing an infinite space of possible rhythmic sensory inputs to be carved up into a finite set of internal categories. Critically, the existence of rhythm categories has been corroborated empirically using a number of behavioral paradigms (\nWhat are the biological processes underlying rhythm categorization? One view is that rhythm categories stem from hard-wired neurobiological predispositions constraining internal representations of rhythmic inputs. In particular, it has been proposed that rhythms corresponding to mathematically simple ratios [i.e., small integer ratios based on a grid of equal time intervals and their grouping in twos, such as the 1:1:2 rhythm of “Jingle Bells” (\nHowever, a growing body of work points toward rhythm categorization as a plastic function, reflecting enculturation and social learning (\nTherefore, clarifying the interplay between hard-wired mechanisms and culture-driven neural plasticity appears a critical step to understand how socially meaningful categories of rhythm are produced and transmitted. Yet, this endeavor has proven particularly challenging so far, due to the lack of task-independent measures to capture rhythm categorization from neural responses. More broadly, task-free measures are ultimately key to probe rhythm categorization across the lifespan, cultures, and species and address long-standing questions regarding the nature and underlying mechanisms of human rhythmic behaviors. Here, we address this gap by providing neural evidence for rhythm categorization and underlying rhythm prototypes, thus advancing a critical step beyond previous findings limited to behavioral measures (\nUsing the fRSA approach, we provide direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain activity captured with surface electroencephalography (EEG) goes beyond mere tracking of acoustic temporal features of the rhythmic inputs and, instead, exhibits categorical representations. Moreover, we show that these neural rhythm categories emerge automatically, without any related explicit task, yet they are remarkably similar to the categorical structure reflected in sensorimotor reproduction of the same stimuli. Despite this automaticity, these rhythm categories are not fully explained by feedforward nonlinearities in the earliest stages of the ascending auditory pathway, as tested with a biomimetic model of auditory nerve responses. Therefore, by ruling out that the process of rhythm categorization merely reflects motor, instructional, or decisional biases, our results take a critical step forward in understanding the nature and neural pathways underlying this function fundamental to the human experience of music.\nUsing scalp EEG, we recorded brain activity of healthy adult participants (\n(\nWe used this rhythm continuum based on previous results consistently showing that rhythms from this continuum are perceived by Western participants as two discrete categories separated by a sharp perceptual boundary (\nFirst, we tested whether rhythm categorization was evident in the behavior by analyzing the intertap intervals (ITIs) produced in each condition when participants were instructed to tap the finger in synchrony with the rhythmic inputs (\n(\nAfter capturing the categorical structure using a standard approach by directly modeling the produced ITI ratios, we asked whether the same categories would emerge when analyzing the similarity, rather than the values of the produced ratios, across conditions. To this end, we built a representational similarity matrix (RSM) for each participant, based on the absolute differences in the average ITI ratios produced across all pairs of conditions. Each individual ITI RSM was then compared against several theoretical models of categorization differing in the position of the category boundary (yielding 10 distinct theoretical models in total;\nNext, we extended the ITI-based findings by testing whether a similar categorical structure could be observed when representing the series of tap onsets as a continuous signal. This constitutes a pivotal shift from the analysis of time intervals defined by discrete temporal markers toward the analysis of continuous data where the identification of temporal markers may be less straightforward (e.g., as in surface EEG). To this end, we applied the fRSA approach to the tapping responses represented as continuous time-varying signals with a unit impulse at the onset time of each executed tap (see\nWe used frequency-domain analysis to isolate and characterize the time course of the response signal in each condition (\nAs expected, based on this principle, the obtained spectra averaged across all participants and conditions exhibited peaks of magnitude at frequencies corresponding to the repetition rate of the rhythmic pattern and harmonics (i.e.,\nThere was a significant correlation between tap-onset RSMs and categorical models at the group level (permutation test,\nThe correlation between the RSM and the best-fitting categorical model selected for each participant was significantly smaller when considering the tap-onset signals rather than ITIs [paired\nTo further demonstrate the robustness of fRSA in revealing rhythm categories from continuous data, we applied the method to continuous force signals obtained directly from the tapping sensor. In addition to the mere temporal arrangement of the taps, the time course of the force signal also depends on their relative accentuation and overall kinematics of their execution, thus potentially offering yet another dimension that could reflect categorization of rhythmic inputs. Inspecting the grand-average magnitude spectrum of the tapping force revealed peaks at the rate of the rhythmic pattern repetition up to 16 Hz (\nAt the group level, the obtained tap-force RSMs showed clear correspondence with a categorical representational structure (permutation test,\n(\nThese results thus highlight the fRSA approach as a conservative yet sensitive method to capture categorical structures from continuous smooth signals beyond relying on discrete homogenous temporal markers. Notably, these results also indicate that including additional information about the way each tap movement is executed (here, including both tap-onset and force information) yields representational structures overall consistent with those obtained with tap timing alone. In line with prior work (\nOverall, our analyses of the tapping data (i) confirm that, in accordance with previous studies, the set of two-interval rhythms used in the current study elicited internal representation of two separable rhythm categories and (ii) validate the fRSA method as a robust and sensitive tool to identify rhythm categories using continuous time-varying response signals beyond the need to extract discrete temporal markers. These constitute critical prerequisites for proceeding to the analysis of EEG data.\nWe recorded brain activity using EEG as participants listened to the rhythmic stimuli without performing any overt movement. As for the tapping, trial-averaged preprocessed EEG responses were transformed into the frequency domain using Fourier transform. Here, we only considered frequencies of interest up to 8 Hz, as this frequency cutoff captured all significant responses at consecutive harmonics of the rhythm repetition rate as observed in the grand-average EEG magnitude spectrum (i.e., first six harmonics of the rhythmic pattern repetition;\nFor each participant, we built a neural RSM based on the similarity of real and imaginary Fourier coefficients at the frequencies of interest concatenated across all 64 EEG channels, thus accounting for any individual differences in response topography. The neural RSMs exhibited significant categorical structure at the group level (permutation test,\nNotably, we observed comparable evidence of neural categorization when limiting the analysis to responses averaged across nine frontocentral channels (fig. S4). This pool of channels was selected based on the fact that they have been shown to consistently capture EEG responses to repeating acoustic rhythms across previous studies (\nFinally, having acknowledged the theoretical advantages of frequency-domain analysis, we asked whether a time-domain analysis would yield comparable results. Notably, we first low-pass filtered the EEG response at 10 Hz to capture its frequency range as determined from the magnitude spectrum above (thus still partly capitalizing on the ability to estimate response bandwidth in the frequency domain). RSMs were then built by correlating the average time course of the response over the duration of the rhythmic pattern across all pairs of conditions. We observed strong evidence of neural categorization (group-level permutation test,\nIn our fRSA analysis of the EEG responses reported above, we controlled for potential contribution of low-level tracking of the sensory input to the obtained categorical structure by partialing out the acoustic RSM (\nThe model provided faithful simulation of physiological processes associated with cochlear nonlinearities, inner hair cell transduction process, the synapse between the hair cell and the auditory nerve, and the associated firing rate adaptation. For each condition, we simulated the time course of instantaneous firing rate in the auditory nerve elicited by the corresponding rhythmic sequence (fig. S6A). An auditory nerve RSM was then obtained from these simulated responses by applying the same fRSA analysis as for the EEG responses above.\nCrucially, the auditory nerve RSM closely resembled the acoustic RSM (fig. S6B), indicating that the modeled early subcortical representations mainly followed the temporal structure of the stimulus sequences. More specifically, the obtained auditory nerve RSM did not show significant rhythm categorization (no significant correlation with a categorical model; permutation test,\nOur results indicate that both behavioral and neural responses to the rhythmic inputs do not simply reflect acoustic features but, instead, exhibit representational geometries consistent with the existence of two distinct rhythm categories, a smaller one spanning ratios from about 0.50 to 0.55 (1:1 to 1.2:1) and another, bigger one spanning from 0.56 to 0.67 (1.3:1 to 2:1). However, is there a correspondence between the categories reflected in the brain and behavior?\nTo answer this question, we first assessed the overlap between the representations measured from the brain and behavior of each participant, by correlating the neural RSM (considering frequencies up to 8 Hz and all 64 channels) with the corresponding tap-force RSM (considering frequencies up to 16 Hz), while partialing out the shared similarity structure driven by the acoustic stimulus. The neural RSMs significantly correlated with the tap-force RSMs at the group level (permutation test,\nNext, we examined the location of the categorical boundary separating the two rhythm categories observed in the EEG and tap-force RSMs. Across participants, the location of this boundary in the best-fitting categorical model was remarkably similar for the EEG (median boundary ratio = 0.56 and bootstrapped 95% CI = 0.54 to 0.57;\nThis result was further corroborated by considering all theoretical models of categorization differing in the position of the category boundary, rather than a single best-fitting model. Indeed, the distribution of correlation coefficients obtained across all categorical models separately for each participant showed marked similarity between the neural and behavioral (tap-force) responses (mean\nUsing fRSA, our results provided evidence for categorical structure in continuous response signals. Building on this, we sought to further characterize the specific form of the responses making up each identified category. Indeed, the categorical structure observed here could have been driven by any feature of the response that systematically differed across the two categories (i.e., discriminated) and remained consistent for conditions within each category (i.e., generalized). Thus, the relevant property could have been, for example, the response amplitude at an arbitrary yet systematic latency. Alternatively, categorization may have emerged from the temporal profile of the response, whereby a consistent trajectory would repeat according to a recurring pattern of time intervals with a given ratio.\nWhile the latter hypothesis was supported by the analysis of tapping responses above, characterizing the relevant features of EEG responses requires a more general approach that does not rely on the presence of homogeneous discrete temporal events. To this end, we carried out an exploratory analysis of response trajectory in each condition based on the similarity with a set of prototypical temporal templates.\nAs shown in fig. S7, the prototypes consisted of continuous signals made of unit impulses arranged over time to create a repeating pattern of two time intervals (pattern duration was set to 750 ms, as for the stimuli). Across prototypes, ratios of the constituent intervals were equally spaced between 0.50, i.e., the 1:1 edge of the condition continuum tested here, and 0.84, i.e., beyond the 2:1 ratio (0.67) corresponding to the other edge of the condition continuum, yielding a total of 76 prototype signals. This set of prototypes thus corresponded to a wide and fine gradient of two-interval ratios [note that ratios from 0.84 up to 0.99 were not included in this set of prototypes as these ratios would result in shortest interval durations shorter than 127.5 ms, thus likely reaching motor constraints for one-to-one sensorimotor synchronization of finger tapping in nonmusicians (\nNext, we assessed which prototype was the most similar to the response in each condition. Critically, this question cannot be answered by directly comparing the time course of the prototype and the response, as this would impose unnecessary assumptions about the shape and phase lag of the repeated trajectory [for further discussion, see (\nAs a first step, we confirmed that this profile of (a)symmetries was compatible with the categories we have identified in the analysis of complex Fourier coefficients. To this aim, we compared the neural RSMs based on magnitudes at the frequencies of interest (up to 8 Hz at all 64 channels) to the corresponding RSMs built by considering the real and imaginary Fourier coefficients used in the fRSA above. While accounting for the acoustic RSM, we observed significant partial correlation between the magnitude and complex RSMs at the group level (permutation test,\nHaving established that the categorical structure of the responses was well captured in the magnitude spectrum, we moved to testing which prototype offered the best characterization of the response in each condition. To this end, we correlated the vector of magnitudes at the frequencies of interest taken from the spectrum of each prototype with the same vector obtained from the magnitude spectrum of the continuous response signal (either tapping force or EEG) averaged across all participants, separately for each condition. We used the grand average instead of individual participants’ spectra to further improve the signal-to-noise ratio (particularly for the EEG responses) in view of optimizing the identification of potential underlying prototypes. The statistical distribution of the maximally correlated prototype in each condition was estimated by bootstrapping (i.e., by repeatedly building the grand-average spectrum from a resampled pool of participants; see Materials and Methods and fig. S7).\nFor the continuous tap-force signals, the identified prototypes were not gradually changing across conditions, as would be expected if the response was following the temporal structure of the corresponding auditory stimuli (\n(\nWe then assessed whether responses within each of the categories identified by the fRSA approach detailed above showed high similarity to particular prototypes. To this aim, we created marginal distributions by collapsing the distribution of maximally correlated prototypes across conditions, separately for the smaller and bigger categories. Local peaks in these marginal distributions were identified by sliding a narrow window through the prototype continuum and quantifying whether the distribution inside the window was higher than in its local neighborhood (\nOn the one hand, in the four conditions corresponding to the smaller category (i.e., between stimulus ratios 0.50 and 0.55), the distribution of prototypes maximally correlated with the tapping response peaked at a ratio near 0.53 (\nTogether, these observations point toward underlying prototypes that would not align with small, mathematically simple, integer ratios (i.e., 0.50 and 0.67, corresponding to 1:1 and 2:1, respectively). Instead, the participants seemed to soften or sharpen the produced interval ratio in a way broadly consistent with observations from other behavioral studies (\nNext, we assessed which prototypes were the most similar to the neural responses. The observed distribution of prototypes maximally correlated with the EEG responses appeared to partly follow the interval ratios of the rhythmic patterns presented in each condition (\nOn the one hand, there seemed to be prominent similarity with prototypes corresponding to small integer ratios, indicated by significant peaks near 0.50 (i.e., isochrony;\nThe current study provides direct evidence for neural categorization of rhythm in humans. Specifically, we show that brain responses to rhythmic patterns do not merely reflect the physical temporal structure of the acoustic input. Rather, the structure of the neural responses across conditions is compatible with the existence of two distinct rhythm categories, consistent with behavioral measures from the same participants and in line with a large body of prior behavioral work (\nThus far, research on rhythm categorization has been restricted to behavioral measures, due to the lack of a method allowing rhythm categorization to be captured from neural data. Our results extend previous findings from discrimination and sensorimotor synchronization studies (\nHere, we move a critical step beyond behavioral studies by showing here that profiles of discrimination/generalization compatible with perceptual categorization can emerge from neural activity even without engagement in related explicit judgement tasks that may be sensitive to decisional and cognitive factors potentially driven by task demands (\nThe automaticity of the neural categorization captured here could be suggestive of a low-level process rooted in nonlinearities of the earliest auditory processing stages. However, our modeling results argue against this possibility. Indeed, the neural categories of rhythm identified here could not be explained by nonlinear transformations occurring at the earliest, peripheral stage of the ascending auditory pathway. Rather, our findings highlight a transformation from the representational geometry observed based on modeled auditory nerve responses, mainly tracking the physical temporal structure of the stimuli, toward the categorical representation measured with scalp EEG and consistent with behavioral responses.\nNonetheless, our data do not exclude that rudiments of this transformation could be found already in subcortical auditory nuclei, as has been proposed for the internal representation of periodic beat and meter elicited by rhythmic inputs structured according to an evenly spaced, isochronous grid of time intervals (\nOur results are compatible with the view of characteristic “warping” of the representational space where two rhythmic inputs are rendered more similar when they activate internal representation of the same category, as compared to physically equidistant rhythms internally assigned into different categories (\nA potential candidate to account for the observed rhythm categorization could be adaptation. While the absence of a categorical structure in the simulated auditory nerve responses argues against the role of fast low-level adaptation, a slower adaptation produced at later stages of the auditory pathway could, in principle, contribute to the neural categorization observed in scalp EEG activity (\nInstead, the observed categorical boundary might be compatible with a categorization process driven by detection of dissimilarity between the two intervals composing the rhythm (\nAll in all, delineating the specific representations underlying the rhythm categories identified in the current study will thus likely require going beyond electrical field potentials recorded with scalp EEG. For example, future work recording single neuron responses in the human temporal (as well as parietal and frontal) cortices appears a promising avenue to progress in our understanding of the neural processes supporting rhythm categorization (\nCategorical representational geometry has often been closely associated with the concept of a prototype (\nIndeed, there is abundant behavioral evidence that perception of two-interval rhythms, such as the ones used in the current study, is pulled toward either prototypical 1:1 or 2:1 interval ratios (\nOur exploratory analysis of the similarity between continuous tapping responses and a set of prototypical rhythmic templates was broadly in line with a shift from exact integer ratios. Within the smaller category, the tapping dynamics mostly resembled a rhythm prototype with a ratio of 0.53, while the larger category featured tapping dynamics highly similar to both a softened (~0.63) and a minimally sharpened (~0.68) prototype. It is worth noting that part of this effect may be explained by a pull toward the center of the range of rhythmic ratios presented in the condition continuum tested here. This possibility could be addressed in future work by adapting the rhythmic stimuli such as to locate the putative small–integer ratio prototypes in the center rather than edges of the condition continuum.\nNotably, the tendency to converge toward a limited number of rhythm prototypes was also observed in the EEG responses. That is, there was a general alignment between the prototypes identified in tapping and in the neural responses, with observed prototypes compatible with bias away from mathematically simplest integer ratios. This result thus argues against a critical role of kinematic constraints in driving these more complex ratio prototypes. At the same time, EEG responses featured additional prototypes that were not encountered in the tapping responses. These prototypes included other complex ratios (0.58; approximately corresponding to 4:3) but also small integer ratios such as 0.50 (1:1) and 0.75 (3:1), the latter being in line with prior work probing the internal representation of rhythm indirectly via transient EEG responses elicited by expectation violations (\nTaken together, the heightened similarity to multiple rhythm prototypes per category observed in neural and behavioral data could reflect fingerprints of distinct underlying mechanisms [including individual differences (\nBuilding on recent advances in systems neuroscience (\nCompared to previous findings, the fRSA approach goes a critical step beyond methods relying on identification of time intervals between discrete events, which have proven difficult to apply beyond a small set of highly specific responses such as finger tapping (\nAccordingly, the fRSA approach constitutes an important methodological advance, as it allows rhythm categorization to be probed directly from the dynamics of a wide range of neural responses (e.g., spiking rates, field potentials, oscillatory power fluctuations) at tempi that are ecologically valid [sometimes remarkably fast (\nThis approach thus appears particularly well suited to address long-standing questions about the primitives and roots of musical rhythm, particularly the relative contribution of universal neurobiological constraints shared across species and culture-driven plasticity developing over the course of life through social learning. For example, it could allow us to track how neural rhythm categories develop over the lifespan from birth, how they are shaped by cultural experience or body movement, and how this plasticity is supported by a network of brain regions shared in part by nonhuman species. Therefore, the framework developed in the current study appears promising to bridge the gap between recently found universality of some rhythmic structures in music on the one hand and the vast interindividual and cross-cultural diversity specific to human rhythm perception and production on the other hand.\nEighteen participants (mean age ± SD = 26.0 ± 4.8 years, 13 females) were recruited in Brussels, Belgium. They reported various levels of musical and dance training (musical training: mean ± SD = 3.7 ± 6.2 years, range: 0 to 21 years, 11 participants never had any musical training; dance training: mean ± SD = 2.7 ± 3.9 years, range: 0 to 12 years, 9 participants never had any dance training). Given prior evidence that rhythm categorization is affected by enculturation rather than the mere amount of practice (\nThe stimuli consisted of two-interval rhythmic patterns generated using MATLAB R2022a (MathWorks). The two-interval rhythmic pattern was produced by presenting three auditory events (here, three identical tones) over time, while keeping the total duration of the pattern constant. In such a two-interval pattern, the first interval thus corresponds to the time between the onset of the first and the second event (IOI1, i.e., first IOI), while the second interval (IOI2) is defined as the time between the second and the third event. If the pattern is seamlessly looped, then the third event of one pattern also constitutes the first event of the subsequent pattern (\nThe durations of the two intervals composing repeated two-interval patterns can be expressed as a ratio. For instance, if a given two-interval pattern exhibits a first interval that is twice as long as the second one, we can refer to that pattern as a 2:1 rhythm (\nThe 13 two-interval rhythmic patterns were generated using an identical pure tone of 50-ms duration with a carrier frequency of 300 Hz and a 10-ms linear onset/offset ramp. The patterns had a fixed total duration of 750 ms. This pattern duration was chosen since, on the one hand, it rendered unimanual tapping along with the stimuli reasonably comfortable for adults without musical training (\nThe experiment consisted of six listening blocks and three tapping blocks, with the two types of blocks presented in alternation (a tapping block after every two listening blocks). In all blocks, the 13 different stimulus sequences were presented once in a randomized order.\nDuring the listening blocks, participants were instructed to avoid any unnecessary movement and muscular tension and fixate a cross displayed in front of them to minimize the presence of muscular and ocular artifacts in the EEG recording. Moreover, to ensure attention to the stimulus sequences, we used a task orthogonal to rhythm categorization whereby participants were required to detect transient volume drops in the sequences. The volume drops were obtained by decreasing the amplitude of four consecutive rhythmic patterns within a stimulus sequence to 85% of their amplitude. For each stimulus sequence, there could be one volume drop (occurring in two of the six presentations over all listening blocks), two volume drops (one of six), or none (three of six). After listening to the stimulus sequence without moving, participants verbally reported the number of detected volume drops and received immediate feedback.\nDuring the tapping blocks, participants were instructed to tap in synchrony with the tones using the index finger of their preferred hand. Tapping was performed on a custom-made analog device (hereafter referred to as the “tapping box”) that was positioned by the participants’ side. Participants were instructed not to tap before the beginning of the stimulus sequences to obtain a valid period of baseline before trial onset (stimulus sequences were repeated when not meeting this criterion). Participants were also required not to wait too long to start tapping after the beginning of each stimulus sequence.\nThe experiment was implemented in MATLAB R2016b (MathWorks, Natick, MA) using the Psychophysics Toolbox extensions (\nWe recorded brain activity using a 64-channel BioSemi Active Two EEG system (BioSemi, Amsterdam, Netherlands) with two additional channels placed on the left and right mastoids. Recording sites included standard 10-20 system locations (channel coordinates can be found at\nAn accelerometer was placed on the head of the participants to monitor whether participants complied with the instructions and avoided head movement during the listening blocks. The signals from all the channels and the accelerometer were digitized at a sample rate of 1024 Hz.\nTapping responses measured as tapping onsets and continuous force signal were recorded using the tapping box connected to the BioSemi Active Two EEG system’s Analog Input Box. The surface of the tapping box was made of a conductive hard material, thus providing clear tactile feedback. While tapping also produced a small amount of auditory feedback, this was substantially attenuated by the ear inserts used to deliver the auditory stimuli (see above). The device recorded tapping onsets as moments in which the finger got in contact with the conductive surface and closed an electrical circuit. Simultaneously, the force exerted by the finger was recorded as a continuous signal using a six-axis force sensor (FT48224, ATI Industrial Automation, NC). The latency and jitter of the captured signals were below 1 ms, as measured with an oscilloscope.\nThe tapping onsets were digitized as triggers, while the force signal was digitized as the continuous signal coming from six different sensors of the tapping box at a sampling rate of 1024 Hz. In addition, we also recorded a copy of the delivered acoustic signal through the BioSemi Active Two EEG system’s Analog Input Box to control for latency in the recording system, which was digitized at 1024 Hz.\nTo simulate responses elicited by the rhythmic stimuli in a set of auditory nerve fibers, we used an auditory nerve model developed by Bruce\nEEG and behavioral data were analyzed using Letswave 6 (\nA Butterworth high-pass filter (fourth order, cutoff at 0.1 Hz) and low-pass filter (fourth order, cutoff at 64 Hz) were applied to raw continuous EEG data to remove slow drifts and responses at very high frequencies irrelevant to the current study. We subsequently downsampled the data to 256 Hz (i.e., by a factor of 4) to facilitate data handling and storage. We segmented the continuous data from −5 s to 27.5 s with respect to the onset of each stimulus sequence before performing artifact rejection. Following visual inspection of the data, we linearly interpolated noisy channels with the three closest neighboring channels (two channels in 1 participant, one channel in 2 participants, and no channels in the remaining 15 participants).\nWe then applied independent component analysis (ICA) to remove artifactual components due to blinks and eye movements. ICA matrices were computed from data preprocessed the same way as described above, except that we used a higher high-pass filter cutoff (1 Hz; fourth-order Butterworth filter) to improve artifact classification accuracy (\nAfter artifact rejection, the data were resegmented from 0 to 22.5 s (i.e., total duration of individual stimulus sequences) relative to stimulus sequence onset. The duration of the resulting epochs thus corresponded to an exact integer multiple of the rhythmic pattern duration, hence preventing spectral leakage of responses at the frequencies of interest (determined as 1/pattern duration and harmonics) into the surrounding frequency bins after applying the Fourier transform (\nThe data were re-referenced to average mastoids with the goal of maximizing the EEG responses to the acoustic stimuli (\nTo extract relevant features characterizing the neural response in each condition, we capitalized on the fact that the spectrum of any signal that is systematically repeated with a fixed repetition rate (i.e., periodically) will only contain peaks at specific frequencies corresponding the repetition rate (i.e.,\nFollowing a procedure adopted in previous frequency-tagging studies (\nAfter artifact rejection, we applied a fourth-order Butterworth low-pass filter with a 10-Hz cutoff (i.e., to match the frequency range showing significant consecutive harmonics in the obtained EEG spectra, as measured using the\nTo analyze ITIs, we adopted a procedure followed in previous sensorimotor synchronization studies (\nThe obtained tap onsets were then used to calculate the ITIs separately for each repetition of the rhythmic pattern. Each tone in the given repetition was matched with the closest mean asynchrony–corrected tap. Then, we measured the time interval between the first and the second tap (ITI1) and the time interval between the second tap and the tap paired with the first tone of the directly following rhythm repetition (ITI2). The ITI ratio was calculated as\nSeparately for each participant, condition, and tapping trial, we created a continuous time-domain signal with duration corresponding to the length of the stimulus sequence and 256-Hz sampling rate. The value of each sample corresponding to a tap-onset time was set to 1 (i.e., a unit impulse) and 0 otherwise. Note that all tap onsets detected by the tapping box were used without any further preprocessing (i.e., unlike for the ITI analysis above).\nFor each participant, condition, and tapping trial, the continuous tapping force recorded from the six force sensors of the tapping box was segmented from −1 to 22.5 s with respect to onset of the stimulus sequences. For each sensor, the force signal recorded over the trial duration was baseline corrected by subtracting at each time point the averaged signal over 1 s before trial onset to correct for potential offsets present in the recordings. The signal from the six sensors was then combined (using the device calibration matrix) to obtain the continuous tapping force orthogonal to the tapping box. The obtained tap-force signals were resegmented from 0 to 22.5 s (i.e., stimulus sequence duration) relative to the onset of the stimulus sequences and downsampled to 256 Hz.\nThe continuous responses (both continuous tap-onset time series and tapping force signals) were averaged across trials corresponding to different repetitions of the same condition, and an FFT was applied to obtain a response spectrum for each condition and participant. We assessed the significance of the responses at frequencies of interest at the group level (see fig. S3). These computations were performed on the magnitude spectrum averaged across all participants and conditions, following the same steps as for the EEG responses.\nThe average ITI ratios were collapsed across participants and fitted either with a linear or a sigmoid model. Parameters were estimated by minimizing the least-squares error, and the performance of each model was evaluated using leave-one-participant-out cross-validation. The sigmoid model was also fitted separately for each individual participant.\nTo test whether the tapped interval ratios overall significantly deviated from the stimulus ratios, we divided the range of stimulus ratios into 13 equal bins, computed histogram of the number of ITI ratios in each bin, and compared it to the one obtained under null hypothesis of a uniform distribution using the χ\nThe continuous tapping force signals were also analyzed in the time domain, similarly to the EEG responses. To do so, a fourth-order Butterworth low-pass filter with an 18-Hz cutoff was first applied to the preprocessed continuous tapping force signals (i.e., matching the frequency range whereby significant responses were identified using the procedure described above; see fig. S3). Similarly to the EEG analysis, the data were then segmented into successive chunks of 750-ms duration, starting from onset time until the end of the stimulus sequence, thus yielding 90 epochs (3 trials × 30 pattern repetitions) per condition and participant, downsampled by a factor of 4, demeaned, and averaged separately for each condition and participant.\nNeural and behavioral data were analyzed in the RSA framework (\nThe acoustic RSM was obtained by taking the ratio between the first IOI and the total duration of the rhythmic pattern and computing the absolute difference of this value across all pairs of conditions. The resulting difference values were subtracted from 1 to yield a similarity matrix with ones on the diagonal. This RSM thus reflects the equal spacing of the rhythmic ratios along the condition continuum, i.e., a linear decrease in similarity across conditions (\nFor each participant, a neural RSM was obtained as follows. First, the real and imaginary coefficients of the complex Fourier spectrum at the frequencies of interest (i.e., harmonics of the rhythmic pattern repetition rate where a significant response was observed at the group level) were extracted separately for each of the 64 channels. Then, separately for each condition, these values were concatenated into a feature vector, yielding number of dimensions equal to\nWe then built behavioral RSMs from the various tapping signals (i.e., from discrete to continuous tapping signals). Individual ITI RSMs were obtained by computing the absolute difference between the produced average ITI ratio across all pairs of conditions. To capture similarity, the difference values were subtracted from 1 as for the acoustic RSM. Moreover, individual tap-onset RSMs were built, as for the neural responses, using feature vectors obtained by concatenating the real and imaginary coefficients at the frequencies of interest from the complex spectrum of the continuous tap-onset signals separately for each condition and participant (number of dimensions = 2 ×\nFurther, we computed an RSM with the responses obtained from the auditory nerve model. As for the neural responses, the complex spectrum of the auditory nerve model responses was calculated for each condition using FFT. To obtain an RSM directly comparable with the neural RSM, we extracted real and imaginary coefficients at the same frequencies of interest as for the EEG responses and concatenated them into a feature vector separately for each condition. The auditory nerve RSM was obtained by calculating Pearson’s correlation between feature vectors across all pairs of conditions.\nTheoretical models of rhythm categorization were built based on the fundamental definition of a categorization function: maximal similarity of responses across conditions within the same category (thus setting pairwise similarity across these conditions to 1) and maximal dissimilarity of responses across different categories (thus setting the pairwise similarity of the corresponding conditions to 0).\nGiven that the stimuli used in the current study are expected to elicit the perception of two rhythm categories (\nTo investigate whether a response reflected rhythm categorization, we performed partial correlations between individual RSMs and the RSM of each theoretical model of rhythm categorization while partialing out the acoustic RSM to account for any representational structure driven by the stimulus (\nSignificance of the result for each individual participant was evaluated using permutation testing (5000 iterations), with the aim to probe whether the partial correlation coefficient with the best-fitting categorical model is higher than what would be expected from chance. In each iteration, we randomly shuffled the values of the lower triangular response RSM and computed the partial correlation between the shuffled response RSM and the theoretical categorical models while partialing out the acoustic RSM. The correlation value corresponding to the winning theoretical categorical model identified from shuffled response RSM was stored for each iteration. These values were used to build a null distribution of correlation values for statistical testing.\nSignificance was also assessed at the group level (permutation test, 10,000 iterations) to test whether the group-averaged partial correlation coefficient with the participant-wise best-fitting model is significantly higher than expected from chance. For each iteration, we shuffled all individual RSMs and found the best-fitting categorical model for each participant in the same way as described above. The average correlation coefficient of the best-fitting categorical model was stored to build a null distribution.\nA\nNext, we were interested in comparing the RSMs obtained for different kinds of responses, such as ITI RSM, tap-onset RSM, tap-force RSM, and neural RSM. These comparisons were carried out using the following methods.\nTo test whether two RSMs shared similar structure beyond what could be explained by the stimuli, we calculated Spearman’s partial correlation between the lower triangular parts of both RSMs while including the acoustic RSM as a covariate. Significance of the correlation was tested using a permutation test (5000 iteration), where partial correlation was computed from randomly shuffled RSMs on each iteration to build a null distribution of correlation coefficients. The same permutation procedure was used to establish the significance of the average correlation coefficient across participants, i.e., a group level test (10,000 iterations).\nTo test how prominent was the observed categorical structure at the group-level between two kinds of responses, the individual Spearman’s correlation coefficients obtained for the best-fitting model were Fisher transformed and further compared across the two kinds of responses using a paired\nLast, to test whether two kinds of responses showed a similar category boundary position, we took into account the fact that theoretical models with similar category boundary positions were highly correlated. Hence, instead of relying on the best-fitting model, we considered all the possible models. For each response, individual Spearman’s partial correlation coefficients were obtained by correlating the corresponding RSM with each of the 10 theoretical models of categorization. This yielded vectors of 10 correlation coefficients, which were first Fisher transformed and then correlated (Pearson’s correlation) between the two kinds of responses separately for each participant. The obtained correlation values were then Fisher transformed and tested against zero using a one-sample\nTo shed light on the properties of the signals making up the categorical structure observed from brain and behavioral responses, we evaluated the similarity of the continuous response (either tapping or EEG) in each condition and a set of prototypical signals.\nWe built 76 time-domain prototypes with the same duration as the rhythmic sequences (i.e., 22.5 s). Each prototype was made of 30 seamlessly repeating 750-ms rhythmic patterns comprising two impulses arranged over time to create two IOIs with a given ratio. Across prototypes, the IOI ratios were equally spaced between a ratio of 0.50 (i.e., 1:1 ratio) and 0.84 (i.e., with contrast in duration between the two intervals sharper than the 3:1 ratio).\nThen, we characterized the temporal structure of each prototype, that is, the profile of (a)symmetries within the span of the repeating 750-ms pattern by computing the spectrum of each prototype using FFT and extracting a vector of magnitudes at the frequencies of interest, as selected for the analyzed responses (i.e., harmonics of the pattern repetition rate up to 8 and 16 Hz for neural and tapping responses, respectively).\nLikewise, we used FFT to obtain the magnitude spectrum of the analyzed response separately for each condition and participant. To minimize the contribution of broadband noise to the magnitudes measured at the frequencies of interest, we applied a noise correction procedure by subtracting from each frequency bin of interest the local noise baseline approximated as the average magnitude at eight surrounding frequency bins (four on each side, excluding the immediately adjacent bins to avoid potential remaining spectral leakage).\nThe similarity between each prototype signal and the response in each condition was evaluated using a bootstrapping procedure (\nTo test whether the distribution of maximally correlated prototypes was concentrated near the stimulus ratio in each condition, we tested whether the maximally correlated prototype fell within a narrow window centered on the stimulus ratio. The width of the window was delimited by the midpoints between successive ratios on the condition continuum. That is, the window started halfway between the tested and the directly preceding condition and ended halfway between the tested and the directly following condition. The number of maximally correlated prototypes inside the window was then subtracted from the number falling outside of the window, in each case weighted according to the size of the respective range (i.e., by dividing these numbers by the rhythmic ratio range covered by the “in” and “out” window, respectively) (\nA similar approach was used to localize peaks in the distribution of maximally correlated prototypes. This was done by first splitting the condition continuum into two segments based on the category boundary identified using fRSA (median category boundary across participants) and collapsing the distributions separately for each of the two obtained subsets of conditions. For each subset, local peaks in the pooled distribution were identified by first calculating a kernel smoothing function estimate using MATLAB’s “ksdensity” function and locating peaks higher than 10% of the average density value using the “findpeaks” function. To test the significance of each located peak, we counted the maximally correlated prototypes within a narrow window centered on the peak (width corresponding to the spacing between neighboring stimuli on the condition continuum) and compared this to the counts falling in regions directly flanking the window (equivalent to half the window width on each side). Statistical significance was evaluated by repeating the bootstrap procedure 500 times and computing a", "topic": "Brain"}
{"pmid": "38074317", "pmcid": "12310036", "title": "Impact of web accessibility on cognitive engagement in individuals without disabilities: Evidence from a psychophysiological study", "publication_year": "N/A", "abstract": "Web accessibility features on websites are designed for individuals with disabilities that include low vision and cognitive impairments, but such features can benefit everyone. This study investigates the impact of accessibility features of the web on ambient/focal visual attention and cognitive processing in individuals without disabilities. The study involved 20 participants reading news websites with different levels of low vision and cognitive-related accessibility features while their eye movements and heart rate variability were monitored. The findings show that cognitive engagement declined over time when no accessibility enhancements were present. The study also demonstrates that enhancing cognitive accessibility leads to increased user cognitive engagement, while low vision accessibility features make websites easier to read. These findings are corroborated by self-reports and psychophysiological measures, such as eye-tracking metrics and heart rate variability. The effects from these psychophysiological measures, together with participants’ self-reports, support the benefits of enhancing web accessibility features for all users. The implications for future website design are also discussed.", "full_text": "Accessibility barriers on websites can significantly impact people with disabilities, limiting their access to information and services [\nAlthough it is widely accepted that accessibility to content levels can benefit all users, this assertion has not been rigorously validated by empirical research [\nThe present study aims to address this research gap by examining the cognitive engagement experienced by users without disabilities when interacting with websites of varying accessibility levels. The examination will focus on two attentional patterns: ambient and focal processing. Ambient attentional processing refers to the broad attentional state of being aware of the surroundings, whereas focal attentional processing involves a selective focus on a specific task or a limited area of the visual field [\nTo advance the understanding of accessibility in web design, this study makes three key contributions:\nTo situate this study within the broader context, we review key works in web accessibility and biosignals in cognitive engagement, highlighting relevant findings that inform our research.\nWeb accessibility is a crucial component of contemporary digital environments, ensuring that users with varying abilities can effectively access and interact with online content. Research consistently demonstrates that web accessibility not only benefits users with disabilities but also enhances the experience for users without disabilities.\nSchmutz et al. [\nSimilarly, Vollenwyder et al. [\nYesilada et al. [\nA systematic literature review by Campoverde-Molina et al. [\nResearch consistently shows that accessibility features not only support users with disabilities but also enhance the overall user experience. However, while the studies reviewed showed benefits for all users, they were few and were based on perceived attributes, not quantitative physiological ones.\nOnline content providers seek to attract users’ attention by keeping their emotional, cognitive, and behavioral engagement, fostering a connection between a user and content [\nLagun and Lalmas [\nFor example, Arapakis et al. [\nDynamics of focal attention can be examined with the duration of fixations combined with the amplitude of saccades that follow these fixations [\nThe level of focal attention is also related to the time spent on task [\nAccording to motivational control theory [\nThe eye-tracking method has been successfully applied in various contexts of website design. One notable application is in online shopping and personalization [\nHeart rate represents the number of heartbeats per minute. Heart rate variability (HRV), defined as the variation in the time interval (ms) between sequential heartbeats (Inter Beat Interval – IBI), is an index of autonomic control of the heart [\nHRV is seen as effective as eye-movement metrics for detecting information processing, as HRV correlates with cognitive functioning [\nStudies that compared passive (non-task related) HRV, as in the systematic review by Forte et al. [\nWhen HRV was measured during task performance (active task-related HRV), Luque-Casado et al. [\nTo the best of our knowledge, there are no studies measuring resting or active HRV in relation to website accessibility. HRV metrics, such as the variability of heart rate inter-beat intervals, provide continuous and objective indicators of cognitive engagement. By analyzing time-domain heart rate variability, we aim to determine whether enhancing websites with accessibility features keeps users cognitively engaged with time-on-task. The analyses of heart rate variability will complement our understanding of how accessibility features impact interaction with websites.\nPrevious research suggests that accessibility features facilitate digital content perception in groups with low vision [\nIn our work, we address this gap by triangulating eye-tracking, heart rate variability, and self-reports to provide evidence that accessibility features are beneficial for users without disabilities. To this end, we investigate the dynamics of ambient/focal visual attention and heart rate variability in response to digital content differing in accessibility levels. In addition, we accounted for individual differences in cognitive resources by including working memory capacity as a covariant in our analyses. Working memory is known to play a critical role in sustaining attention and managing cognitive engagement during mental tasks, including reading and information processing [\nFor our interaction with digital content, we chose two leading news portals, The New York Times (NYT) and BBC News. The original websites were modified to incorporate accessibility enhancements for low vision, such as adjustments to letter, word, and line spacing, as well as features for cognitive impairments, including the use of simplified language and the removal of distracting content. The enhanced websites were evaluated in comparison to their original, unmodified versions.\nAlthough previous research has shown that accessibility features can improve usability for diverse populations, most studies have focused on subjective user experience. In this study, we employed physiological measurements to evaluate cognitive engagement during interaction with websites enhanced with low vision and cognitive accessibility features. To the best of our knowledge, there is no published study measuring the effect of website accessibility features on adults without disabilities triangulating multimodal data of self-assessment, eye-tracking, and heart rate measures. By focusing on eye metrics, heart rate variability, and self-report questions, our hypotheses directly examine whether improvements in website accessibility support deeper attention and more sustained information processing.\nOne of the contributions of the present study is the empirical demonstration of how accessibility features help users focus their attention on digital content for a longer time and reduce their cognitive effort. Another contribution of the present study is the methodological approach employed to understand the impact of accessibility features on cognitive engagement, ensuring the reliability and objectivity of the method. Finally, integrating these accessibility features into digital environments could be useful for all users. With this understanding, we believe that digital content designers will be better equipped to incorporate accessibility features into website design, potentially promoting user engagement and enhancing overall user experience.\nThe present study examines the impact of accessibility features on users’ cognitive engagement during interaction with news website content. In general, we hypothesized that cognitive and low vision accessibility enhancements to the websites affect cognitive information processing differently, which led us to formulate two specific hypotheses.\nThis section outlines the approach used to evaluate the impact of web accessibility features on user engagement. Using psychophysiological tools such as eye-tracking and heart rate variability, we measured user interaction with different web content. The following subsections describe participants, websites’ design, data collection, and the statistical methods employed to analyze cognitive engagement under varying accessibility conditions.\nTwenty non-native English-speaking participants were recruited to attend the study (from 15/10/2024 to 31/10/2024). All participants (12 females; average age\nTo evaluate the impact of the various accessibility features, two websites were selected and modified to create four distinct versions: (1) the original pages without enhancements, (2) pages with enhancements for low vision accessibility, (3) pages with enhancements for cognitive accessibility, and (4) pages with both low vision and cognitive accessibility enhancements.\nThe chosen websites were from the\n(1a) The original version of website. (1b) Low vision accessibility enhanced website. (1c) Cognitive accessibility enhanced website. (1d) Cognitive and low vision accessibility enhanced website.\nFor the pages with low vision accessibility enhancements, the original pages were modified in order to ensure that they followed the guidelines of the\nHave color contrast conforming to the WCAG\nDo not rely on color alone to convey information\nSupport adjusting the text size\nHave letter, word, and line spacing conforming to the WCAG\nReflow the content when magnified\nFor the pages with cognitive accessibility enhancements, the original pages were also modified with the same goal of following WCAG. In particular, we ensured that the websites (details in\nUse simplified language\nDisplay visual aids to supplement written information\nAvoid distracting content\nGive enough time for users to read information\nHave simplified layouts\nThese modifications ensured that the websites were prepared to systematically test the impact of different accessibility enhancements, focusing on low vision and cognitive accessibility. Nevertheless, it is important to note that both pages were already quite accessible without the enhancements, particularly with regard to low vision accessibility.\nParticipants were initially introduced to the versions of the NYT page, followed by the versions of the BBC page. However, the four versions of each website were presented in a randomized order. Each version of the website was presented for 60 seconds. After interacting with each version of each website, participants were asked to answer three questions about (1) the article on the website, (2) the understandability of the website, and (3) the readability of the website (see questions in\nThe Gazepoint GP3-HD eye tracker with a sampling rate of 150\nWorking memory capacity was measured by visual digit span task [\nParticipants gave written informed consent and were briefed on the study before performing the visual DSPAN task to assess their working memory capacity. All information was collected anonymously. Each participant then sat at a distance of 570 mm from the eye tracker, placing their left middle and ring fingers on a finger sensor. A 5-point calibration and validation process was conducted to map gaze positions before starting the task.\nThe participants were presented with the websites for 60 seconds. Subsequently, the comprehension question was displayed for 45 seconds, while the understandability and readability questions were displayed for 15 seconds in order to obtain the participants’ responses. The duration of the presentation of web pages and questions was chosen based on the sensitivity of psychophysiological measures to time-on-task. The main reason was to avoid fatigue effects and time overload when considering the whole experiment. Previous studies using eye-tracking in web contexts have also used similar durations to assess attention and user engagement [\n(1) The experiment began with the DSPAN task. (2) The physiological measurement phase was repeated eight times for all conditions (four NYT and four BBC) for each participant. First, the website was displayed for 60 seconds. Participants viewed the website in a browser and they were free to scroll and read the articles. Next, three questions appeared in a row to assess comprehension, understanding, and readability. Participants had 45, 15, and 15 seconds respectively to respond verbally. (3) The experiment concluded with the NASA-TLX task.\nThe finger sensor was cleaned with an alcohol wipe before use by each participant. Ambient light levels were measured, with an average light level of 11 lux recorded for the participants. The study was approved by the Ethical Review Board at the Faculty of Psychology, SWPS University (decision no: 47/2023).\nFor the preprocessing of raw eye-tracking data into fixations and saccades, we employed a non-parametric speed-based algorithm. The algorithm estimates velocity thresholds per person and uses the duration lower threshold of 80\nThe standard deviation of the RR intervals (beat-to-beat intervals), a standard time domain HRV measure [\nThe preprocessing was carried out using R computational language for statistical computing [\nIn this section, we present the results of our quantitative analysis, focusing first on the data, including self-reported metrics such as readability, understandability, and task performance. We then explore the findings from the psychophysiological measurements to understand how accessibility enhancements may influence cognitive engagement.\nThe statistical analyses were conducted with the R language for statistical computing [\nThe second set of analyses focused on scrutinizing the eye-movement metrics associated with the focus of attention, encompassing the second-order metric of\nThe third set of analyses delved into the biomarkers of cognitive engagement, testing differences in the length of heart inter-beat intervals and heart rate variability as dependent variables. For testing the time-related hypotheses, in the second and third sets of ANCOVAs, we compared changes in eye-movement and HRV characteristics at the beginning of the website presentation with the end of the website reading time. To do that, in the ANCOVA model, we included a time epoch as the third independent variable leading to\nExamining hypothesis 1.1, ANCOVA results on the self-assessment of understandability showed that websites with cognitive accessibility enhancements tended to be more understandable (\n(3a) The main effect of cognitive accessibility enhancements on website understandability. (3b) The main effect of low vision accessibility enhancements on website readability.\nAnalogous ANCOVA for readability self-evaluation revealed a statistically significant main effect of low vision accessibility (\nThe ANCOVA for accuracy of content comprehension showed no significant effects.\nThe analysis of ambient/focal attention dynamics depicted with coefficient\n(4a) Portraits the main effect of cognitive accessibility enhancements. (4b) Portraits the main effect of time-on-task. (4c) Portraits an interaction effect of cognitive and low vision accessibility enhancements.\nSecond, the main effect of time-on-task also reached the statistical significance level (\nThird, the analysis showed a statistically significant effect of the three-way interaction of cognitive and low vision accessibility enhancements in time (\nThe results of the analyses of the first-order eye-movement-based metrics related to cognitive processing, average fixation duration, fixation count, and total fixation time revealed that the only significant effects were the main effects of time-on-task. The results of those three analyses are presented in\nWe obtained a statistically significant interaction between time-on-task and cognitive accessibility enhancements on websites, quantified by working memory capacity, (\nThe graph shows the relationship between users’ working memory capacity and heart rate variability while reading websites with and without cognitive accessibility enhancements depending on the time-on-task epoch. Dot-dashed lines present slopes for the relationship between working memory capacity and HRV when reading websites with cognitive accessibility enhancement. Solid lines represent the slopes for the same relationship while reading websites without cognitive accessibility enhancements.\nWhen verifying hypothesis 2.3, the ANCOVA for heart rate variability as a dependent variable showed a marginally significant effect for low vision accessibility features (\nNext, we verify the hypothesis 2.3. The ANCOVA with the mean heart rate inter-beat interval (average IBI) as a dependent variable revealed a statistically significant main effect of low vision accessibility enhancements (\nThe above main effects were quantified by a significant interaction effect of time-on-task and low vision accessibility (\nThe graph shows effect of time-on-task and the average heart rate inter-beat intervals in low vision accessibility. Two lines represent different conditions of low vision accessibility. The solid line represents the original condition, showing a slight increase in heart rate variability from the beginning to the end of the task. The dashed line represents the enhanced condition. This line shows a stable and higher heart rate variability across the task with minimal change between the beginning and end.\nThe overall purpose of the present study was to understand the impact of accessibility features, specifically ones related to low vision and cognitive accessibility, on cognitive engagement for users without disabilities. In general, we hypothesized that cognitive and low vision accessibility enhancements to the websites will affect cognitive information processing differently. First, we expected that cognitive accessibility enhancements yield users’ cognitive engagement (focal attention and thorough information processing). Furthermore, we expected that websites’ accessibility enhancements promote keeping the cognitive engagement over time of website reading. Second, low vision accessibility enhancements were expected to facilitate the readability of the websites and the ease of information processing presented on them. Thanks to a unique methodological design, we intended to test those hypotheses using self-reports and psychophysiological measures based on eye movements and heart rate variability.\nIn line with\nOur analyses also showed a decrease in cognitive processing depth over time during the reading of news websites. Values of eye-movement characteristics (average eye fixation duration, fixation count, and total fixation time,\nThe decrease in cognitive engagement might be the effect of cognitive resource depletion over interaction time with the website. The present study revealed that website accessibility enhancements can stop this trend of cognitive engagement loss. We demonstrated that cognitive accessibility enhancements help users maintain focal attention until the last time epoch of website reading (hypothesis 1.3).\nIn partial support of hypothesis 1.4, the results revealed that cognitive accessibility enhancements are also beneficial for people with high cognitive resources. At the conclusion of the website reading tasks, we observed that lower heart rate variability, which indicates deeper cognitive processing, was associated with a higher working memory capacity among users. Shorter and more uniform time intervals between heartbeats (lower heart rate variability and shorter inter-beat intervals) may occur when someone is paying close attention. The present finding suggests that enhancing cognitive accessibility provides more consistent inter-beat intervals [\nWe may conclude that the incorporation of cognitive accessibility features, such as simplified language and the avoidance of distracting context, can enhance the user experience and facilitate the maintenance of user cognitive engagement (see also [\nAs the cognitive accessibility enhancements yield focal attention when reading news websites, also low vision accessibility features facilitate the readability of the sites, making them easier to process, as predicted in\nOur study found no significant differences in reading comprehension between the original websites and the modified versions controlled for low vision and cognitive accessibility. This outcome may be attributed to the randomized order in which the page versions were presented to participants. The randomization could have led some participants to encounter the more understandable versions first, which may have facilitated their comprehension of the content. As a result, this initial exposure likely enhanced their ability to answer questions about the article, regardless of the version they encountered afterward, thereby leveling the overall comprehension scores across different versions. Another potential reason might be that the limited task time forced participants to engage with the main ideas of the websites, regardless of the comprehension of the articles. While our psychophysiological measures detected statistical differences in cognitive engagement between conditions, the reading comprehension requires a longer sustained interaction with the content. Furthermore, the participants in this study were university students with high cognitive resources, which may have minimized the observed differences in reading comprehension between the original and enhanced website conditions.\nThis study distinguishes itself through the use of advanced psychophysiological methods, specifically eye-tracking and HRV measurements, to examine cognitive engagement in users without disabilities interacting with web content enhanced for accessibility. Unlike traditional self-report methods, which can be influenced by bias and subjective interpretation, these objective measurements provide continuous, real-time data on users’ physiological responses [\nBy employing these innovative methodologies, the study uncovers several unique insights into the benefits of accessibility features. For example, the integration of eye-tracking data revealed that cognitive accessibility features, such as the use of simplified language and the removal of distracting elements, can sustain users’ focal attention over extended periods, preventing the typical decline in cognitive engagement observed in standard web content [\nThe findings of this study offer several practical implications for web designers and developers, emphasizing the broad benefits of integrating accessibility features beyond compliance with legal standards [\nBased on the study’s findings, several guidelines can be proposed to optimize web design. First, designers should prioritize the use of simplified language and structured content that supports cognitive accessibility, as these elements have been shown to sustain user attention. Avoiding distracting elements, such as intrusive ads and complex visual layouts, can further aid in maintaining focus. Second, designers should ensure adequate contrast ratios, customizable text sizes, and appropriate spacing in line with guidance for low vision accessibility. These adjustments not only support users with visual impairments but also make the content easier to read and process for all users, thereby broadening the appeal and usability of the website. By adopting these practices, web designers can create more inclusive online environments that cater to a diverse audience, potentially enhancing both user satisfaction and site performance metrics.\nThis study will be of significant interest to public sector organizations, particularly local government and the education sector. A recent study focusing on council websites in Spain found that although councils assert that their websites comply with accessibility standards, there has been minimal progress in achieving full accessibility. Enhancing web accessibility has the potential to boost citizen engagement at the local level while also improving well-being, credibility, transparency, and trust in local public institutions [\nOverall, the present experiment has aligned with our expectations. However, it is not free from some limitations.\nFirst, our participants were non-native English speakers interacting with English websites. Given the large volume of English content available on the Web, this is most likely an extremely common scenario, making it a valuable subject for investigation. As part of our adaptations, we simplified the content of the pages, and this change may, or may not, have a more pronounced impact on non-native speakers. Therefore, future studies may want to replicate the results on intralingual material. Accessibility features applied to websites in a foreign language may improve the understandability and readability of the content for non-native readers. In addition, the study used a within-subjects design to compare cognitive engagement with and without accessibility features, controlling for individual differences. We acknowledge that language background may influence the effects of accessibility features. Barnitz [\nSecond, two popular news platforms, the BBC and NYT websites, were selected for testing. Both pages were already relatively accessible, particularly for users with low vision. Despite this, we still observed significant results, suggesting the potential for an even greater impact on websites with lower accessibility levels. Future studies could explore these enhancements in more controlled environments with varying levels of accessibility to better understand their effects across different accessibility baselines.\nThird, it can be noted that the relatively small sample size may have an impact on the generalisability of the results. We mitigate this limitation by applying a within-subjects design. To provide further information on the sensitivity of the outcome, the reported results were also assessed with a post hoc power analysis. Moreover, as far as we know, no previous studies have specifically examined the research question of the present study. However, further validation and a bigger sample are necessary to explore the broader impact of these findings on different groups of users.\nFourth, the duration of the presented material, websites and questions, was chosen based on the sensitivity of psychophysiological measures to time-on-task and to provide consistency across participants. However, longer durations may reflect more realistic web browsing behavior. Future studies could consider free browsing content in web design with accessibility features to assess cognitive engagement.\nFifth, the study focused on the impact of accessibility enhancements on cognitive engagement in users without disabilities. Future studies may want to replicate our findings on different samples. Further investigation may be conducted on enhanced websites examined by individuals with low vision and cognitive impairments. Testing the impact of cognitive and low vision accessibility features is also important from the perspective of the aging of society. Older adults observe a decline in cognitive and sensory functioning [\nThis study provides compelling evidence that accessibility features, such as cognitive enhancements and low vision adjustments, significantly improve cognitive engagement for all users, not just those with disabilities. Our findings demonstrate that web design practices focused on accessibility can enhance ambient/focal visual attention, sustain cognitive focus, and reduce mental fatigue for everyone, leading to a more inclusive and engaging user experience. By empirically validating these benefits through both subjective and objective measures, the study underscores the universal value of accessible design.\nTheoretical implications of this research extend the understanding of web accessibility beyond its traditional scope, advocating for a more inclusive design approach that benefits diverse user groups. Practically, our findings offer concrete guidelines for web designers, such as using simplified language, minimizing visual clutter, and ensuring proper text contrast and spacing. These insights are relevant not only for enhancing user experience but also for informing policy and regulatory frameworks that govern digital accessibility standards.\nMethodologically, this study is among the first to employ a combination of eye-tracking and heart rate variability measures to explore cognitive engagement in the context of web accessibility. This multi-modal innovative approach provides a richer, more nuanced understanding of how users interact with accessible content, offering a valuable template for future research in this field. The use of psychophysiological data allowed us to capture continuous, objective insights into user behavior, revealing patterns of engagement that might have been missed by traditional survey methods alone.\nIn conclusion, this study contributes to the growing body of evidence that web accessibility features benefit all users, advocating for a shift toward more inclusive web design practices. By demonstrating that accessibility features enhance cognitive engagement and user readability, our research emphasizes the importance of integrating these features into mainstream digital environments. Ultimately, the findings promote a more accessible, user-friendly web for everyone, aligning with the principles of universal design and inclusive technology.", "content_for_embedding": "Accessibility barriers on websites can significantly impact people with disabilities, limiting their access to information and services [\nAlthough it is widely accepted that accessibility to content levels can benefit all users, this assertion has not been rigorously validated by empirical research [\nThe present study aims to address this research gap by examining the cognitive engagement experienced by users without disabilities when interacting with websites of varying accessibility levels. The examination will focus on two attentional patterns: ambient and focal processing. Ambient attentional processing refers to the broad attentional state of being aware of the surroundings, whereas focal attentional processing involves a selective focus on a specific task or a limited area of the visual field [\nTo advance the understanding of accessibility in web design, this study makes three key contributions:\nTo situate this study within the broader context, we review key works in web accessibility and biosignals in cognitive engagement, highlighting relevant findings that inform our research.\nWeb accessibility is a crucial component of contemporary digital environments, ensuring that users with varying abilities can effectively access and interact with online content. Research consistently demonstrates that web accessibility not only benefits users with disabilities but also enhances the experience for users without disabilities.\nSchmutz et al. [\nSimilarly, Vollenwyder et al. [\nYesilada et al. [\nA systematic literature review by Campoverde-Molina et al. [\nResearch consistently shows that accessibility features not only support users with disabilities but also enhance the overall user experience. However, while the studies reviewed showed benefits for all users, they were few and were based on perceived attributes, not quantitative physiological ones.\nOnline content providers seek to attract users’ attention by keeping their emotional, cognitive, and behavioral engagement, fostering a connection between a user and content [\nLagun and Lalmas [\nFor example, Arapakis et al. [\nDynamics of focal attention can be examined with the duration of fixations combined with the amplitude of saccades that follow these fixations [\nThe level of focal attention is also related to the time spent on task [\nAccording to motivational control theory [\nThe eye-tracking method has been successfully applied in various contexts of website design. One notable application is in online shopping and personalization [\nHeart rate represents the number of heartbeats per minute. Heart rate variability (HRV), defined as the variation in the time interval (ms) between sequential heartbeats (Inter Beat Interval – IBI), is an index of autonomic control of the heart [\nHRV is seen as effective as eye-movement metrics for detecting information processing, as HRV correlates with cognitive functioning [\nStudies that compared passive (non-task related) HRV, as in the systematic review by Forte et al. [\nWhen HRV was measured during task performance (active task-related HRV), Luque-Casado et al. [\nTo the best of our knowledge, there are no studies measuring resting or active HRV in relation to website accessibility. HRV metrics, such as the variability of heart rate inter-beat intervals, provide continuous and objective indicators of cognitive engagement. By analyzing time-domain heart rate variability, we aim to determine whether enhancing websites with accessibility features keeps users cognitively engaged with time-on-task. The analyses of heart rate variability will complement our understanding of how accessibility features impact interaction with websites.\nPrevious research suggests that accessibility features facilitate digital content perception in groups with low vision [\nIn our work, we address this gap by triangulating eye-tracking, heart rate variability, and self-reports to provide evidence that accessibility features are beneficial for users without disabilities. To this end, we investigate the dynamics of ambient/focal visual attention and heart rate variability in response to digital content differing in accessibility levels. In addition, we accounted for individual differences in cognitive resources by including working memory capacity as a covariant in our analyses. Working memory is known to play a critical role in sustaining attention and managing cognitive engagement during mental tasks, including reading and information processing [\nFor our interaction with digital content, we chose two leading news portals, The New York Times (NYT) and BBC News. The original websites were modified to incorporate accessibility enhancements for low vision, such as adjustments to letter, word, and line spacing, as well as features for cognitive impairments, including the use of simplified language and the removal of distracting content. The enhanced websites were evaluated in comparison to their original, unmodified versions.\nAlthough previous research has shown that accessibility features can improve usability for diverse populations, most studies have focused on subjective user experience. In this study, we employed physiological measurements to evaluate cognitive engagement during interaction with websites enhanced with low vision and cognitive accessibility features. To the best of our knowledge, there is no published study measuring the effect of website accessibility features on adults without disabilities triangulating multimodal data of self-assessment, eye-tracking, and heart rate measures. By focusing on eye metrics, heart rate variability, and self-report questions, our hypotheses directly examine whether improvements in website accessibility support deeper attention and more sustained information processing.\nOne of the contributions of the present study is the empirical demonstration of how accessibility features help users focus their attention on digital content for a longer time and reduce their cognitive effort. Another contribution of the present study is the methodological approach employed to understand the impact of accessibility features on cognitive engagement, ensuring the reliability and objectivity of the method. Finally, integrating these accessibility features into digital environments could be useful for all users. With this understanding, we believe that digital content designers will be better equipped to incorporate accessibility features into website design, potentially promoting user engagement and enhancing overall user experience.\nThe present study examines the impact of accessibility features on users’ cognitive engagement during interaction with news website content. In general, we hypothesized that cognitive and low vision accessibility enhancements to the websites affect cognitive information processing differently, which led us to formulate two specific hypotheses.\nThis section outlines the approach used to evaluate the impact of web accessibility features on user engagement. Using psychophysiological tools such as eye-tracking and heart rate variability, we measured user interaction with different web content. The following subsections describe participants, websites’ design, data collection, and the statistical methods employed to analyze cognitive engagement under varying accessibility conditions.\nTwenty non-native English-speaking participants were recruited to attend the study (from 15/10/2024 to 31/10/2024). All participants (12 females; average age\nTo evaluate the impact of the various accessibility features, two websites were selected and modified to create four distinct versions: (1) the original pages without enhancements, (2) pages with enhancements for low vision accessibility, (3) pages with enhancements for cognitive accessibility, and (4) pages with both low vision and cognitive accessibility enhancements.\nThe chosen websites were from the\n(1a) The original version of website. (1b) Low vision accessibility enhanced website. (1c) Cognitive accessibility enhanced website. (1d) Cognitive and low vision accessibility enhanced website.\nFor the pages with low vision accessibility enhancements, the original pages were modified in order to ensure that they followed the guidelines of the\nHave color contrast conforming to the WCAG\nDo not rely on color alone to convey information\nSupport adjusting the text size\nHave letter, word, and line spacing conforming to the WCAG\nReflow the content when magnified\nFor the pages with cognitive accessibility enhancements, the original pages were also modified with the same goal of following WCAG. In particular, we ensured that the websites (details in\nUse simplified language\nDisplay visual aids to supplement written information\nAvoid distracting content\nGive enough time for users to read information\nHave simplified layouts\nThese modifications ensured that the websites were prepared to systematically test the impact of different accessibility enhancements, focusing on low vision and cognitive accessibility. Nevertheless, it is important to note that both pages were already quite accessible without the enhancements, particularly with regard to low vision accessibility.\nParticipants were initially introduced to the versions of the NYT page, followed by the versions of the BBC page. However, the four versions of each website were presented in a randomized order. Each version of the website was presented for 60 seconds. After interacting with each version of each website, participants were asked to answer three questions about (1) the article on the website, (2) the understandability of the website, and (3) the readability of the website (see questions in\nThe Gazepoint GP3-HD eye tracker with a sampling rate of 150\nWorking memory capacity was measured by visual digit span task [\nParticipants gave written informed consent and were briefed on the study before performing the visual DSPAN task to assess their working memory capacity. All information was collected anonymously. Each participant then sat at a distance of 570 mm from the eye tracker, placing their left middle and ring fingers on a finger sensor. A 5-point calibration and validation process was conducted to map gaze positions before starting the task.\nThe participants were presented with the websites for 60 seconds. Subsequently, the comprehension question was displayed for 45 seconds, while the understandability and readability questions were displayed for 15 seconds in order to obtain the participants’ responses. The duration of the presentation of web pages and questions was chosen based on the sensitivity of psychophysiological measures to time-on-task. The main reason was to avoid fatigue effects and time overload when considering the whole experiment. Previous studies using eye-tracking in web contexts have also used similar durations to assess attention and user engagement [\n(1) The experiment began with the DSPAN task. (2) The physiological measurement phase was repeated eight times for all conditions (four NYT and four BBC) for each participant. First, the website was displayed for 60 seconds. Participants viewed the website in a browser and they were free to scroll and read the articles. Next, three questions appeared in a row to assess comprehension, understanding, and readability. Participants had 45, 15, and 15 seconds respectively to respond verbally. (3) The experiment concluded with the NASA-TLX task.\nThe finger sensor was cleaned with an alcohol wipe before use by each participant. Ambient light levels were measured, with an average light level of 11 lux recorded for the participants. The study was approved by the Ethical Review Board at the Faculty of Psychology, SWPS University (decision no: 47/2023).\nFor the preprocessing of raw eye-tracking data into fixations and saccades, we employed a non-parametric speed-based algorithm. The algorithm estimates velocity thresholds per person and uses the duration lower threshold of 80\nThe standard deviation of the RR intervals (beat-to-beat intervals), a standard time domain HRV measure [\nThe preprocessing was carried out using R computational language for statistical computing [\nIn this section, we present the results of our quantitative analysis, focusing first on the data, including self-reported metrics such as readability, understandability, and task performance. We then explore the findings from the psychophysiological measurements to understand how accessibility enhancements may influence cognitive engagement.\nThe statistical analyses were conducted with the R language for statistical computing [\nThe second set of analyses focused on scrutinizing the eye-movement metrics associated with the focus of attention, encompassing the second-order metric of\nThe third set of analyses delved into the biomarkers of cognitive engagement, testing differences in the length of heart inter-beat intervals and heart rate variability as dependent variables. For testing the time-related hypotheses, in the second and third sets of ANCOVAs, we compared changes in eye-movement and HRV characteristics at the beginning of the website presentation with the end of the website reading time. To do that, in the ANCOVA model, we included a time epoch as the third independent variable leading to\nExamining hypothesis 1.1, ANCOVA results on the self-assessment of understandability showed that websites with cognitive accessibility enhancements tended to be more understandable (\n(3a) The main effect of cognitive accessibility enhancements on website understandability. (3b) The main effect of low vision accessibility enhancements on website readability.\nAnalogous ANCOVA for readability self-evaluation revealed a statistically significant main effect of low vision accessibility (\nThe ANCOVA for accuracy of content comprehension showed no significant effects.\nThe analysis of ambient/focal attention dynamics depicted with coefficient\n(4a) Portraits the main effect of cognitive accessibility enhancements. (4b) Portraits the main effect of time-on-task. (4c) Portraits an interaction effect of cognitive and low vision accessibility enhancements.\nSecond, the main effect of time-on-task also reached the statistical significance level (\nThird, the analysis showed a statistically significant effect of the three-way interaction of cognitive and low vision accessibility enhancements in time (\nThe results of the analyses of the first-order eye-movement-based metrics related to cognitive processing, average fixation duration, fixation count, and total fixation time revealed that the only significant effects were the main effects of time-on-task. The results of those three analyses are presented in\nWe obtained a statistically significant interaction between time-on-task and cognitive accessibility enhancements on websites, quantified by working memory capacity, (\nThe graph shows the relationship between users’ working memory capacity and heart rate variability while reading websites with and without cognitive accessibility enhancements depending on the time-on-task epoch. Dot-dashed lines present slopes for the relationship between working memory capacity and HRV when reading websites with cognitive accessibility enhancement. Solid lines represent the slopes for the same relationship while reading websites without cognitive accessibility enhancements.\nWhen verifying hypothesis 2.3, the ANCOVA for heart rate variability as a dependent variable showed a marginally significant effect for low vision accessibility features (\nNext, we verify the hypothesis 2.3. The ANCOVA with the mean heart rate inter-beat interval (average IBI) as a dependent variable revealed a statistically significant main effect of low vision accessibility enhancements (\nThe above main effects were quantified by a significant interaction effect of time-on-task and low vision accessibility (\nThe graph shows effect of time-on-task and the average heart rate inter-beat intervals in low vision accessibility. Two lines represent different conditions of low vision accessibility. The solid line represents the original condition, showing a slight increase in heart rate variability from the beginning to the end of the task. The dashed line represents the enhanced condition. This line shows a stable and higher heart rate variability across the task with minimal change between the beginning and end.\nThe overall purpose of the present study was to understand the impact of accessibility features, specifically ones related to low vision and cognitive accessibility, on cognitive engagement for users without disabilities. In general, we hypothesized that cognitive and low vision accessibility enhancements to the websites will affect cognitive information processing differently. First, we expected that cognitive accessibility enhancements yield users’ cognitive engagement (focal attention and thorough information processing). Furthermore, we expected that websites’ accessibility enhancements promote keeping the cognitive engagement over time of website reading. Second, low vision accessibility enhancements were expected to facilitate the readability of the websites and the ease of information processing presented on them. Thanks to a unique methodological design, we intended to test those hypotheses using self-reports and psychophysiological measures based on eye movements and heart rate variability.\nIn line with\nOur analyses also showed a decrease in cognitive processing depth over time during the reading of news websites. Values of eye-movement characteristics (average eye fixation duration, fixation count, and total fixation time,\nThe decrease in cognitive engagement might be the effect of cognitive resource depletion over interaction time with the website. The present study revealed that website accessibility enhancements can stop this trend of cognitive engagement loss. We demonstrated that cognitive accessibility enhancements help users maintain focal attention until the last time epoch of website reading (hypothesis 1.3).\nIn partial support of hypothesis 1.4, the results revealed that cognitive accessibility enhancements are also beneficial for people with high cognitive resources. At the conclusion of the website reading tasks, we observed that lower heart rate variability, which indicates deeper cognitive processing, was associated with a higher working memory capacity among users. Shorter and more uniform time intervals between heartbeats (lower heart rate variability and shorter inter-beat intervals) may occur when someone is paying close attention. The present finding suggests that enhancing cognitive accessibility provides more consistent inter-beat intervals [\nWe may conclude that the incorporation of cognitive accessibility features, such as simplified language and the avoidance of distracting context, can enhance the user experience and facilitate the maintenance of user cognitive engagement (see also [\nAs the cognitive accessibility enhancements yield focal attention when reading news websites, also low vision accessibility features facilitate the readability of the sites, making them easier to process, as predicted in\nOur study found no significant differences in reading comprehension between the original websites and the modified versions controlled for low vision and cognitive accessibility. This outcome may be attributed to the randomized order in which the page versions were presented to participants. The randomization could have led some participants to encounter the more understandable versions first, which may have facilitated their comprehension of the content. As a result, this initial exposure likely enhanced their ability to answer questions about the article, regardless of the version they encountered afterward, thereby leveling the overall comprehension scores across different versions. Another potential reason might be that the limited task time forced participants to engage with the main ideas of the websites, regardless of the comprehension of the articles. While our psychophysiological measures detected statistical differences in cognitive engagement between conditions, the reading comprehension requires a longer sustained interaction with the content. Furthermore, the participants in this study were university students with high cognitive resources, which may have minimized the observed differences in reading comprehension between the original and enhanced website conditions.\nThis study distinguishes itself through the use of advanced psychophysiological methods, specifically eye-tracking and HRV measurements, to examine cognitive engagement in users without disabilities interacting with web content enhanced for accessibility. Unlike traditional self-report methods, which can be influenced by bias and subjective interpretation, these objective measurements provide continuous, real-time data on users’ physiological responses [\nBy employing these innovative methodologies, the study uncovers several unique insights into the benefits of accessibility features. For example, the integration of eye-tracking data revealed that cognitive accessibility features, such as the use of simplified language and the removal of distracting elements, can sustain users’ focal attention over extended periods, preventing the typical decline in cognitive engagement observed in standard web content [\nThe findings of this study offer several practical implications for web designers and developers, emphasizing the broad benefits of integrating accessibility features beyond compliance with legal standards [\nBased on the study’s findings, several guidelines can be proposed to optimize web design. First, designers should prioritize the use of simplified language and structured content that supports cognitive accessibility, as these elements have been shown to sustain user attention. Avoiding distracting elements, such as intrusive ads and complex visual layouts, can further aid in maintaining focus. Second, designers should ensure adequate contrast ratios, customizable text sizes, and appropriate spacing in line with guidance for low vision accessibility. These adjustments not only support users with visual impairments but also make the content easier to read and process for all users, thereby broadening the appeal and usability of the website. By adopting these practices, web designers can create more inclusive online environments that cater to a diverse audience, potentially enhancing both user satisfaction and site performance metrics.\nThis study will be of significant interest to public sector organizations, particularly local government and the education sector. A recent study focusing on council websites in Spain found that although councils assert that their websites comply with accessibility standards, there has been minimal progress in achieving full accessibility. Enhancing web accessibility has the potential to boost citizen engagement at the local level while also improving well-being, credibility, transparency, and trust in local public institutions [\nOverall, the present experiment has aligned with our expectations. However, it is not free from some limitations.\nFirst, our participants were non-native English speakers interacting with English websites. Given the large volume of English content available on the Web, this is most likely an extremely common scenario, making it a valuable subject for investigation. As part of our adaptations, we simplified the content of the pages, and this change may, or may not, have a more pronounced impact on non-native speakers. Therefore, future studies may want to replicate the results on intralingual material. Accessibility features applied to websites in a foreign language may improve the understandability and readability of the content for non-native readers. In addition, the study used a within-subjects design to compare cognitive engagement with and without accessibility features, controlling for individual differences. We acknowledge that language background may influence the effects of accessibility features. Barnitz [\nSecond, two popular news platforms, the BBC and NYT websites, were selected for testing. Both pages were already relatively accessible, particularly for users with low vision. Despite this, we still observed significant results, suggesting the potential for an even greater impact on websites with lower accessibility levels. Future studies could explore these enhancements in more controlled environments with varying levels of accessibility to better understand their effects across different accessibility baselines.\nThird, it can be noted that the relatively small sample size may have an impact on the generalisability of the results. We mitigate this limitation by applying a within-subjects design. To provide further information on the sensitivity of the outcome, the reported results were also assessed with a post hoc power analysis. Moreover, as far as we know, no previous studies have specifically examined the research question of the present study. However, further validation and a bigger sample are necessary to explore the broader impact of these findings on different groups of users.\nFourth, the duration of the presented material, websites and questions, was chosen based on the sensitivity of psychophysiological measures to time-on-task and to provide consistency across participants. However, longer durations may reflect more realistic web browsing behavior. Future studies could consider free browsing content in web design with accessibility features to assess cognitive engagement.\nFifth, the study focused on the impact of accessibility enhancements on cognitive engagement in users without disabilities. Future studies may want to replicate our findings on different samples. Further investigation may be conducted on enhanced websites examined by individuals with low vision and cognitive impairments. Testing the impact of cognitive and low vision accessibility features is also important from the perspective of the aging of society. Older adults observe a decline in cognitive and sensory functioning [\nThis study provides compelling evidence that accessibility features, such as cognitive enhancements and low vision adjustments, significantly improve cognitive engagement for all users, not just those with disabilities. Our findings demonstrate that web design practices focused on accessibility can enhance ambient/focal visual attention, sustain cognitive focus, and reduce mental fatigue for everyone, leading to a more inclusive and engaging user experience. By empirically validating these benefits through both subjective and objective measures, the study underscores the universal value of accessible design.\nTheoretical implications of this research extend the understanding of web accessibility beyond its traditional scope, advocating for a more inclusive design approach that benefits diverse user groups. Practically, our findings offer concrete guidelines for web designers, such as using simplified language, minimizing visual clutter, and ensuring proper text contrast and spacing. These insights are relevant not only for enhancing user experience but also for informing policy and regulatory frameworks that govern digital accessibility standards.\nMethodologically, this study is among the first to employ a combination of eye-tracking and heart rate variability measures to explore cognitive engagement in the context of web accessibility. This multi-modal innovative approach provides a richer, more nuanced understanding of how users interact with accessible content, offering a valuable template for future research in this field. The use of psychophysiological data allowed us to capture continuous, objective insights into user behavior, revealing patterns of engagement that might have been missed by traditional survey methods alone.\nIn conclusion, this study contributes to the growing body of evidence that web accessibility features benefit all users, advocating for a shift toward more inclusive web design practices. By demonstrating that accessibility features enhance cognitive engagement and user readability, our research emphasizes the importance of integrating these features into mainstream digital environments. Ultimately, the findings promote a more accessible, user-friendly web for everyone, aligning with the principles of universal design and inclusive technology.", "topic": "Brain"}
{"pmid": "38018635", "pmcid": "12296467", "title": "Astrocytes: Therapeutic targets for stroke", "publication_year": "N/A", "abstract": "Stroke is the leading cause of mortality globally, ultimately leading to severe, lifelong neurological impairments. Patients often suffer from a secondary cascade of damage, including neuroinflammation, cytotoxicity, oxidative stress, and mitochondrial dysfunction. Regrettably, there is a paucity of clinically available therapeutics to address these issues. Emerging evidence underscores the pivotal roles of astrocytes, the most abundant glial cells in the brain, throughout the various stages of ischemic stroke. In this comprehensive review, we initially provide an overview of the fundamental physiological functions of astrocytes in the brain, emphasizing their critical role in modulating neuronal homeostasis, synaptic activity, and blood–brain barrier integrity. We then delve into the growing body of evidence that highlights the functional diversity and heterogeneity of astrocytes in the context of ischemic stroke. Their well-established contributions to energy provision, metabolic regulation, and neurotransmitter homeostasis, as well as their emerging roles in mitochondrial recovery, neuroinflammation regulation, and oxidative stress modulation following ischemic injury, are discussed in detail. We also explore the cellular and molecular mechanisms underpinning these functions, with particular emphasis on recently identified targets within astrocytes that offer promising prospects for therapeutic intervention. In the final section of this review, we offer a detailed overview of the current therapeutic strategies targeting astrocytes in the treatment of ischemic stroke. These astrocyte-targeting strategies are categorized into traditional small-molecule drugs, microRNAs (miRNAs), stem cell-based therapies, cellular reprogramming, hydrogels, and extracellular vesicles. By summarizing the current understanding of astrocyte functions and therapeutic targeting approaches, we aim to highlight the critical roles of astrocytes during and after stroke, particularly in the pathophysiological development in ischemic stroke. We also emphasize promising avenues for novel, astrocyte-targeted therapeutics that could become clinically available options, ultimately improving outcomes for patients with stroke.", "full_text": "Stroke is a heterogeneous category of central nervous system (CNS) disorders characterized by sudden, focal interruptions of cerebral blood flow, usually caused by a blockage of blood vessels (ischemic stroke) or a localized intracranial bleeding event (hemorrhagic stroke) that results in long-term neurologic impairments. Ischemic stroke, which accounts for approximately 80% of all strokes, is typically caused by cardiogenic embolism, cerebral microcirculatory dysfunction, atherosclerosis affecting arteries within or outside of the brain, or coagulation disorders (Barthels and Das, 2020; He et al., 2024). Stroke is the second leading cause of death and disability worldwide (Feigin et al., 2022). Following a stroke, patients commonly experience hemiparesis and speech and cognitive impairments, among other chronic neurological dysfunctions (Boyne et al., 2023; Johnston et al., 2024; Messimeris et al., 2024). Additionally, these individuals may suffer from persistent neurological complications such as movement disorders, depression, tremors, and dysphagia, all of which can significantly impact their functional recovery, quality of life, and survival (Lekoubou et al., 2023). Therefore, identification of effective therapeutic methods to reduce the global economic burden of stroke, including various acute care interventions, surveillance, and rehabilitation, is urgently needed.\nAstrocytes, also known as astroglia or astroglial cells, are star-shaped cells that account for approximately 40% of the total cells in the brain (Trujillo-Estrada et al., 2019; Zhou et al., 2019). They originate from radial glial cells in the ventricular region of the embryo. Astrocytes are traditionally divided into fibrous and protoplasmic types according to their anatomical location and morphology. Fibrous astrocytes are typically localized in the white matter of the CNS. These cells are usually elongated in shape, with long, slender processes and fewer branches at synapses. Protoplasmic astrocytes, on the other hand, are typically found in gray matter and represent a more common type of astrocyte (Hasel and Liddelow, 2021). Their cell bodies are rounded, with short and densely packed branches. The morphological variations in these cells are closely associated with the functions of different brain regions (Bugiani et al., 2022; Hasel et al., 2023). Gray matter astrocytes form tripartite synapses with neurons, where they envelop neurons and facilitate information transfer between neighboring neuron cells by releasing neurotransmitters. White matter, characterized by its myelin-rich composition, relies significantly on fibrous astrocytes for the facilitation of myelination and remyelination processes, which are crucial for the formation and maintenance of the axonal myelin sheaths essential for efficient electrical signal propagation (Xu et al., 2023). Despite the morphological and functional diversities exhibited by astrocytes, they interact with neurons, oligodendrocytes, endothelial cells of blood vessels, and other parenchymal cells in the CNS, collectively constructing intricate neural networks (Köhler et al., 2021). As integral components of normal neural function, astrocytes have become a research focus for the treatment of ischemic brain injury (\nThe roles of astrocytes in stroke treatment.\nThe five main pathways through which astrocytes influence the stroke-affected brain, as reviewed in this article, are shown at the edges of the circle. Six strategies for targeting astrocytes to treat stroke are depicted. ADEVs: Astrocyte-derived extracellular vesicles; AQP4: aquaporin 4; ARE: antioxidant responsive elements; BBB: blood–brain barrier; GSH: glutathione; IGF-1: insulin-like growth factor 1; NO: nitric oxide; Nrf2: nuclear factor erythroid 2-related factor 2; ROS: reactive oxygen species; TNF: tumor necrosis factor.\nIn this review, we first provide an overview of the basic physiological functions of astrocytes in normal or ischemic brains. We then discuss emerging insights into the diverse roles of astrocytes in ischemic stroke, including regulating neuroinflammation, reactive oxygen species, mitochondrial transportation, water homeostasis, and the blood‒brain barrier (BBB). We also highlight the probable key cellular and molecular targets identified in these studies. Finally, we review the frontier astrocyte-targeting intervention strategies for ischemic stroke, further categorizing them into traditional small chemical molecules, microRNAs, stem cells, reprogramming, hydrogels, and astrocyte-derived extracellular vesicles, with the goal of promoting novel astrocyte-targeting therapeutic development.\nIn this narrative review, articles published from inception to November 2024 were retrieved from PubMed. The following keywords were used to identify relevant research: “ischemic stroke,” “astrocyte,” “stroke,” “reactive astrocyte,” “neuron,” “middle cerebral artery occlusion,” “MCAO,” “synapse,” “oxidative damage,” “neuroinflammation,” “blood–brain barrier,” “microRNA,” “hydrogel,” “stem cell,” and “extracellular vesicle.” We comprehensively accessed the literature via various combinations of the above search terms and initially reviewed the titles and abstracts. Once the content was deemed relevant to our aim, we assessed the entire article. The majority of references (\nTimeline and key milestones in astrocyte research on stroke.\nBBB: Blood‒brain barrier.\nAstrocytes are the most abundant cell type in the CNS and play crucial roles in various physiological activities, including synaptic formation and maturation, neurotransmitter metabolism, ion balance, and metabolic modulation of neuronal cells, as well as the formation and maintenance of the BBB, as shown in\nRoles of astrocytes in the CNS.\n(A) Modulation of synaptic formation. TSP1/2 secreted by astrocytes bind to α2δ-1 receptors on synapses and activate Rac1, which further stimulates actin, promoting continuous synapse growth. Hevin secreted by astrocytes bridges Nrx-1α on the presynapse and NL-1β on the postsynaptic space to promote synaptogenesis. (B) Modulation of synaptic transportation and ion homeostasis. Excessive glutamate released from the presynapse can activate NMDARs and AMPARs localized on the postsynapse, resulting in high Ca\nProtoplasmic astrocytes located in gray matter participate in tripartite synapses and play a dynamic role in synaptic development and regulation (Zhou et al., 2019). During development, they promote synaptic formation by secreting molecules that organize synapses. Thrombospondins (TSP1/2), extracellular glycoproteins secreted by astrocytes, modulate synaptogenesis in the CNS and are essential for excitatory synaptic formation, with implications for neurological disorders (Qin et al., 2019a). The α2δ-1 protein, a membrane-anchored glycoprotein highly expressed in the brain, binds to TSP ligands from astrocytes, promoting excitatory synapse formation by recruiting NMDARs to the postsynaptic surface and facilitating dendritic spine maturation (Ablinger et al., 2020). Rac1, a key downstream signaling molecule in the TSP-α2δ-1 pathway, plays a critical role in synaptic remodeling. Upon TSP binding to α2δ-1 on neurons, Rac1 is activated, leading to actin reorganization that supports synaptic structural maturation (Risher et al., 2018; Fan et al., 2021). Although TSP1/2 expression decreases after brain development, its upregulation in the adult cortex following brain injury triggers extensive synaptic remodeling (Wang et al., 2021). High endothelial venule protein/SPARC-like 1 (Hevin/SPARCL1), another synaptic protein secreted by astrocytes, interacts with neurexin, which is involved in presynaptic vesicle release, and neuroligin, which is associated with postsynaptic proteins and neurotransmitter receptors. Hevin bridges neurexin and neuroligin, crucially influencing the formation and maturation of both excitatory and inhibitory synapses (Chen et al., 2022), as shown in\nIn addition to their role in synaptic development, astrocytes play crucial roles in maintaining the homeostasis of the microenvironment surrounding neuronal synapses. Glutamate is the main excitatory neurotransmitter in neuronal signal transmission. The normal concentration of glutamate in the CNS facilitates neuronal communication, but excessive extracellular glutamate can lead to neuronal overexcitation, causing excitotoxicity and even affecting neuronal survival (Andersen et al., 2021). The glutamate transporters (GLAST and GLT-1), which are located on the astrocyte membrane, can take up excessive amounts of extracellular glutamate, reducing the glutamate concentration in the microenvironment and keeping neurons in a normal physiological state (Pajarillo et al., 2019). The actively taken up glutamate is converted into glutamine within astrocytes and then transported back to neurons via membrane-bound glutamine transporters of the SLC38A family. SLC38A, also known as the glutamine transporter solute carrier family 38 member A, participates in the transportation of extracellular glutamate, aiding glutamine synthesis in astrocytes and converting glutamine back into glutamate in presynaptic neurons. Ultimately, glutamine is packaged into synaptic vesicles. This process is known as the Glu‒Gln cycle, which aids in maintaining the normal neurotransmission state of neuronal cells (Andersen et al., 2021). These cascaded events collectively highlight the intricate but precise interactions between astrocytes and neurons in maintaining normal synaptic function and responding efficiently to neurological challenges, as shown in\nAstrocytes are crucial for sustaining neuronal milieu stability and proper CNS functions through the regulation of neurotransmitter uptake and metabolism, as well as the modulation of ion channels and transporters. Astrocytes are integral in maintaining the balance of ions such as K\nGlutamate, the principal excitatory neurotransmitter in the CNS, can lead to neuronal hyperexcitability and neurotoxicity when released excessively. To counteract this effect, astrocytes express several Na\nUnder hypoxic conditions such as stroke injury, the activity of critical ion exchangers, including Na\nAstrocytes convert glutamate into γ-glutamylcysteine (γ-EC) via glutamate-cysteine ligase (GCL), which is subsequently catalyzed by glutathione synthetase (GS) to produce glutathione (GSH), a crucial antioxidant. Mitochondria are the powerhouses of the cell, providing ATP through oxidative phosphorylation in the mitochondrial respiratory chain, where electrons are generated by the tricarboxylic acid cycle (Han et al., 2021a). Neurons are critically dependent on ATP produced by mitochondria and are, therefore, exceptionally vulnerable to mitochondrial dysfunction. Moreover, mitochondria, which are enriched with redox enzymes, serve as the primary sites for reactive oxygen species (ROS) production within cells. Given their high oxygen demand, substantial content of polyunsaturated lipids, and limited antioxidant defenses, neurons are especially susceptible to ROS-induced oxidative damage (Singh et al., 2019). GSH, a key antioxidant, plays a vital role in reducing excessive ROS and other potential sources of ROS, thereby alleviating oxidative stress and preserving cellular redox homeostasis (Lapenna, 2023). As depicted in\nIn summary, astrocytes are crucial for maintaining neuronal stability and CNS function by regulating the uptake of neurotransmitters, particularly glutamate, which can be neurotoxic when excessively released. They express key transporters such as GLT-1 and GLAST to take up glutamate from synapses, converting it into glutamine via glutamine synthetase for recycling. During conditions such as stroke, astrocytes enhance ion exchanger functionality to mitigate excitotoxicity, highlighting their pivotal role in neuroprotection. Moreover, astrocytes contribute to antioxidant defense by synthesizing GSH, which is essential for reducing oxidative stress and preserving neuronal integrity. Overall, astrocytes are indispensable for maintaining ion balance, neurotransmitter recycling, and oxidative protection, which are crucial for CNS health and function.\nNeurons require a large amount of energy to maintain normal physiological function. However, neurons themselves cannot store energy substrates. In contrast, astrocytes can store metabolites rich in energy, including glycogen. Moreover, astrocytes exhibit greater glycolytic efficiency compared with neurons (Xue et al., 2022). To address oxygen deficiency, astrocytes can convert stored glycogen into glucose and supply energy substrates to neurons to meet their high energy demands (Chen et al., 2023). As shown in\nAdditionally, astrocytes can provide energy to neurons under hypoxic conditions through fatty acid metabolism and ketone body synthesis. Enhanced ketone body synthesis occurs when astrocytes are exposed to ischemic hypoxia and glucose-deprived environments. Like lactate, ketone bodies in astrocytes can also be delivered to neurons through monocarboxylate transporter proteins. During ischemia‒reperfusion, intracellular ketone bodies in neurons can be converted into acetyl-CoA and participate in the tricarboxylic acid cycle to generate ATP (Takahashi, 2022).\nTaken together, these results underscore the multifaceted role of astrocytes in managing and optimizing energy resources under ischemic conditions, highlighting their critical function in neuronal survival and brain resilience. Specifically, interventions aimed at increasing the efficiency of the lactate shuttle or ketone body synthesis pathways may offer new avenues to increase the energy supply of neurons, potentially improving outcomes in stroke patients. Future research could explore the potential of enhancing astrocyte metabolic functions as a novel therapeutic strategy to ameliorate ischemic stroke injury. Such therapeutic approaches could provide a significant breakthrough in treating not only ischemic stroke but also other neurological disorders where energy supply mismatches are evident.\nThe BBB is a crucial component in maintaining the homeostasis of the brain’s microenvironment (Jia et al., 2024; Wang et al., 2025). Its functions include regulating the permeability of vascular walls and restricting the passage of ions, molecules, and cells from the blood into the CNS, thus forming an isolating interface between neural tissue and the external environment (Candelario-Jalil et al., 2022). Additionally, the BBB regulates the transport of nutrients and metabolites and facilitates the expulsion of neurotoxic substances (Lochhead et al., 2020). The BBB is composed of endothelial cells, pericytes, smooth muscle cells, and astrocytes (Langen et al., 2019; Pociūtė et al., 2024). Astrocyte–endothelium interactions are essential for electrolyte homeostasis in the brain, both under normal and pathological states. These interactions strengthen tight junctions, reduce the gap junctional areas of endothelial cells, and contribute to the regulation of cerebral blood flow (Schiera et al., 2024). As depicted in\nThe integral role of astrocytes in maintaining BBB integrity and functionality suggests an important therapeutic target in the treatment of ischemic stroke and other neurovascular disorders. The development of strategies to enhance astrocyte‒endothelial interactions could lead to improved control of BBB permeability and stability, potentially reducing the pathological consequences of disrupted homeostasis. These advancements may pave the way for novel interventions aimed at preserving brain health and improving neurological outcomes following vascular challenges.\nAstrocytes can transform from a resting state to an activated state in response to neurodegeneration, pathogen infection, or cerebral ischemia. This transformation triggers a series of changes in gene expression, cellular morphology, and cellular function. Within hours of ischemia, the gene expression of astrocytes is altered in response to factors such as hypoxia, neuronal cell death, neurotransmitter release, and hemorrhage. Cytokines released by injured neurons and glial cells within the ischemic core and penumbra—such as tumor necrosis factor alpha (TNF-α), transforming growth factor, ciliary neurotrophic factor (CNTF), interleukin-1 (IL-1), and IL-6—induce astrocyte polarization. Astrocytes near the infarct core exhibit polarized morphology, whereas those located further from the lesion area display stellate morphology (Pekny et al., 2019). In the days following ischemia, astrocytes in the ischemic penumbra rapidly proliferate, with some migrating toward the infarct border, where they form glial scars by secreting extracellular matrix (ECM) molecules. These glial scars help protect healthy brain tissue by sealing the lesion site and preventing the infiltration of white blood cells. However, astrocytes within the scar also secrete inhibitory molecules that impede axonal regeneration, thus hindering neuronal recovery (Chen et al., 2023; Liang et al., 2024).\nReactive astrocytes exhibit considerable heterogeneity across the brain in various disease models, reflecting distinct functional profiles. Early transcriptome studies proposed that mouse astrocytes adopt either a neurotoxic (‘A1’) or a neuroprotective (‘A2’) phenotype in response to specific stimuli, such as cytokines released by microglia exposed to lipopolysaccharides or following ischemic stroke (Dimitrova-Shumkovska et al., 2020; Guttenplan et al., 2020). These phenotypes were characterized by a set of marker genes, with A1 astrocytes associated with neurotoxicity and A2 astrocytes associated with neuroprotection, as depicted in\nRoles of activated astrocytes in stroke.\n(A) Effects of astrocytes on neurons. Astrocyte A1 triggers an inflammatory response and neurotoxicity that disrupts neurons, whereas astrocyte A2 triggers an anti-inflammatory response and protects neurons through GSH. (B) Effects of astrocytes on the BBB. Astrocyte A1 releases inflammatory factors to disrupt the BBB, whereas astrocyte A2 releases anti-inflammatory factors to repair the BBB and promote the migration of vascular endothelial cells through VEGF. ARE: Antioxidant response elements; BBB: blood–brain barrier; BDNF: brain-derived neurotrophic factor; CCL-2: Chemokine (CC-motif) ligand 2; CNTF: ciliary neurotrophic factor; CXCL1: C-X-C motif chemokine ligand 1; GDNF: glial cell derived neurotrophic factor; GSH: glutathione; IFN-β: interferon-β; IL-10: interleukin-10; IL-1β: interleukin-1β; IL-6: interleukin-6; KLK6: kallikrein related peptidase 6; MCP-1: monocyte chemoattractant protein-1; MMPs: matrix metalloproteinases; NF-κB: nuclear factor kappa-B; NO: nitric oxide; Nrf2: nuclear factor E2-related factor 2; NT-3: neurotrophin-3; PARK7: Parkinson’s disease protein 7; ROS: reactive oxygen species; S-100β: S100 calcium-binding protein B; SDF1: stromal cell-derived factor 1; STAT3: signal transducer and activator of transcription 3; TNF-α: tumor necrosis factor-α; VEGF: vascular endothelial growth factor.\nTo better define astrocyte phenotypes, single-cell RNA sequencing has become an increasingly valuable tool for understanding the heterogeneity of both basal and reactive astrocytes (Mathys et al., 2019; Al-Dalahmah et al., 2020; Batiuk et al., 2020). Batiuk et al. (2020) utilized this approach to analyze the cortex and hippocampus of adult mice and identified five distinct molecular subtypes of astrocytes: AST1, AST2, AST3, AST4, and AST5. They also mapped the spatial distribution of these subtypes and investigated their roles within the CNS, as summarized in\nClassification of astrocytes based on single-cell RNA sequencing and spatial omics\nFurther investigations utilizing single-cell RNA sequencing in the cortical tissue of mice following ischemic stroke have provided valuable insights into the dynamic changes in astrocyte function (Ma et al., 2022; Zheng et al., 2022). At 12 hours poststroke, astrocytes predominantly exert neuroprotective effects by activating pathways such as oxidative phosphorylation, gap and tight junctions, and ferroptosis. The upregulation of the oxidative phosphorylation pathway suggests an energy deficit and increased energy demand in astrocytes during the acute phase of ischemia (Ma et al., 2022). Additionally, compared with astrocytes at 24 hours poststroke, more mitochondrial genes were upregulated at 12 hours, which may reflect either a greater proportion of dying cells or more severe metabolic dysfunction at this early time point (Ma et al., 2022). The activation of gap and tight junction pathways at 12 hours poststroke suggests the involvement of the blood–brain barrier and the enhancement of intercellular connections (Ma et al., 2022). In contrast, at 24 hours poststroke, pathways related to protein synthesis, lysosomal activity, and antigen processing and presentation were more prominently activated, suggesting a shift in astrocyte function toward enhanced interactions with the surrounding microenvironment. At this stage, astrocytes play a more prominent role as inflammatory mediators, releasing inflammatory signals such as cytokine‒cytokine receptor interactions, chemokine signaling, TNF signaling, nuclear factor kappa-B activation, and IL-17 signaling (Ma et al., 2022). This temporal shift in astrocyte function highlights their dynamic roles in both neuroprotection and inflammation during the progression of ischemic stroke. These findings demonstrate the advantages of single-cell RNA sequencing in recognizing the heterogeneity of reactive astrocytes at different time points following stroke.\nIn conclusion, astrocytes exhibit a dynamic and complex response to ischemic stroke characterized by a series of cellular and molecular changes. This response may promote neuroprotection in the early stages but later contributes to inflammation and immune regulation. The development of single-cell RNA sequencing technologies and spatial omics has greatly enhanced our ability to study astrocyte functions and their specific roles in disease, providing crucial insights for the development of effective therapies aimed at reducing neuronal damage and promoting recovery after stroke.\nUnlike the basic functions of healthy astrocytes outlined above, astrocytes undergo significant morphological and functional changes following neurological disorders such as stroke, primarily due to the disruption of brain homeostasis (Brandebura et al., 2023). These changes are characterized by hypertrophy, increased expression of glial fibrillary acidic protein, and the synthesis and release of inflammatory mediators and neurotrophic factors. Collectively, these alterations define the phenotype known as “reactive astrogliosis” (Chung et al., 2024). These changes impact the basic functions of astrocytes and have both beneficial and detrimental effects on the CNS.\nUnder hypoxic conditions, reactive astrogliosis inhibits synapse formation (Li et al., 2024), whereas uncontrolled astrocyte proliferation leads to the formation of glial scars, which hinder neuronal recovery (Zhu et al., 2022). Hypoxia enhances the activity of astrocyte ion exchangers, disrupting ionic balance and increasing neuronal excitability (Hernández et al., 2021). Additionally, reactive astrocytes release inflammatory mediators and ROS, intensifying neuroinflammation (Rauf et al., 2022) and compromising BBB integrity, which results in brain edema and the infiltration of toxic molecules into the brain (Huang et al., 2020).\nIn this section, we focus on summarizing the regulatory roles of astrocytes after stroke in neuroinflammation, oxidative damage, mitochondrial function, water homeostasis, and BBB integrity. Furthermore, the potential targets and related mechanisms in astrocytes for ischemic stroke therapy are summarized in\nPotential targets and related mechanisms in astrocytes for ischemic stroke therapy\nAng-1: Angiopoietin-1; AQP4: aquaporin 4; ARF1: ADP-ribosylation factor 1; BBB: blood‒brain barrier; BMP-2: bone morphogenetic protein 2; Akt: protein kinase B; E2: 17β-estradiol; ERK: extracellular signal-regulated kinase; GSDMD: gasdermin D; GSK-3β: glycogen synthase kinase 3β; JAK: Janus kinase; JNK: c-Jun N-terminal kinases; LCN2: lipocalin-2; LRP1: low-density lipoprotein receptor-related protein 1; MAPK: mitogen-activated protein kinase; NAD\nNeuroinflammation serves as a protective physiological response to preserve normal brain function. Nonetheless, excessive secretion of inflammatory mediators and persistent inflammation can cause significant damage to the CNS. In the brain, microglia and astrocytes orchestrate the release of inflammatory factors, including TNF-α, growth factors, adhesion molecules, and chemokines, following injury, such as stroke (Rauf et al., 2022), as depicted in\nFollowing ischemic brain injury, microglia in the vicinity of the damaged area are initially activated, followed by the activation of astrocytes. These activated astrocytes then stimulate and attract microglia from distant regions to migrate to the injury site, promoting the secretion of inflammatory factors and triggering a cascading, amplified neuroinflammatory response (Qin et al., 2019b). Sustained local inflammation leads to the upregulation of multiple signaling pathways in astrocytes. Notably, studies have shown that cerebral ischemia in mice induces the upregulation of the JAK/STAT pathway, which contributes to overload of the NLRP3 inflammasome. This overactivity promotes astrocyte activation and the subsequent release of astrocyte-derived IL-1β, thereby intensifying neuroinflammation (Xu et al., 2021a; Chen et al., 2022). Furthermore, inhibition of the elevated expression of lipocalin-2 in poststroke astrocytes can reduce NLRP3/Gasdermin D-induced pyroptosis (Li et al., 2023). Mitochondrial dysfunction has been reported to induce NLRP3 inflammasome activation. In turn, NLRP3 activation can elicit mitochondrial injury, leading to astrocyte release and increased production of ROS (Zhang et al., 2024). Inhibition of JAK2 through inhibitors reduces signal transducer and activator of transcription 3 (STAT3) phosphorylation, decreases the production of proinflammatory cytokines such as IL-1, IL-6, IL-8, and TNF-α, and provides neuroprotection following ischemia (Kumari et al., 2024), as illustrated in\nIn summary, astrocytes are crucial in regulating neuroinflammation throughout ischemic stroke. Reactive astrocytes not only facilitate the recruitment and activation of microglia but also amplify inflammatory responses through the secretion of various cytokines and chemokines. The dysregulation of inflammatory signaling pathways in astrocytes, such as the JAK/STAT and NF-κB pathways, contributes to neuroinflammatory exacerbation, potentially leading to neuronal damage. Specific inhibition of these pathways in astrocytes can therefore alleviate inflammatory responses and offer neuroprotection, underscoring the therapeutic potential of modulating astrocyte inflammatory activity in neuroinflammatory conditions after ischemic stroke.\nUnder normal conditions, the ROS generated in brain cells can be eliminated through the catalase pathway to maintain the intracellular balance of free radicals. Cerebral ischemia‒reperfusion results in excessive ROS production, leading to the accumulation of free radicals, causing oxidative stress and, subsequently, neuron damage (Shen et al., 2021). Mitochondria are the primary site of ROS production. During brain hypoxia, the respiratory chain is halted, leading to decreased mitochondrial membrane potential and significantly elevated ROS production. Moreover, elevated ROS levels sustain prolonged activation of the mitochondrial permeability transition pore and inner membrane anion channel, modifying both the intra- and intermitochondrial redox balance. This disruption leads to increased ROS production, reinforcing the ROS-induced ROS release mechanism. This cycle underscores that mitochondria are the primary contributors to ROS generation, which is pivotal in the pathogenesis of oxidative stress in stroke and other cerebral injuries (Zhu et al., 2022).\nMitochondria contain abundant antioxidants and enzymes such as superoxide dismutase, catalase, and GSH, which can eliminate excess accumulated ROS and other free radicals to counteract oxidative stress. Studies have shown that upregulating the expression of antioxidants in mitochondria can reduce the generation of ROS, thereby inhibiting the activation of microglia and astrocytes and effectively improving learning and memory impairments in mice caused by hypoxia (Han et al., 2020).\nIn addition, astrocytes can utilize the GSH antioxidant system to counteract the oxidative stress induced by excessive ROS. Compared with neurons, astrocytes exhibit greater glycolytic efficiency. Partial glycolytic products in astrocytes generate nicotinamide adenine dinucleotide phosphate through the pentose phosphate pathway, which can be used to reduce GSH. Glutathione peroxidase utilizes GSH to reduce ROS, thereby eliminating the excessive accumulation of ROS and alleviating oxidative stress between neurons and astrocytes(Shen et al., 2021). Increasing the levels of GSH and nicotinamide adenine dinucleotide phosphate can reduce ROS levels in the brains of rats after ischemia, thus inhibiting oxidative stress and preserving neurological function (Wei et al., 2019).\nIn addition to producing ROS, astrocytes also produce nitric oxide (NO). On one hand, a low dose of NO can induce vasodilation, improving the blood flow supply to the ischemic penumbra and exerting protective effects during the early stage of stroke. On the other hand, excessive NO reacts with O\nNuclear factor erythroid 2-related factor 2 (Nrf2) is involved in the regulation of oxidative stress in astrocytes. Following hypoxia, ROS stimulation leads to the binding of Nrf2 with antioxidant response elements (AREs), regulating the transcription of antioxidant enzymes (Han et al., 2022; Liu et al., 2023), as shown in\nIn summary, a precise understanding of the variation in mitochondrial dynamics in astrocytes and an analysis of their roles during ROS generation could lead to the development of targeted therapies to normalize mitochondrial function and alleviate oxidative stress. Additionally, investigating astrocytic antioxidant pathways, including the GSH system and Nrf2-mediated responses, may lead to intervention strategies that strengthen these pathways and reduce ROS-induced damage. Further research into the dual role of NO produced by astrocytes could also offer strategies to utilize its beneficial effects while minimizing its harmful effects, potentially promoting neurological recovery in stroke patients.\nAs discussed above, mitochondria play a critical role in the oxidative stress response induced by stroke. Following cerebral ischemia, an insufficient oxygen supply to mitochondria results in the generation of large amounts of ROS within neurons (Shen et al., 2021). Neurons have a limited capacity to activate glycolysis and defend against oxidative stress, increasing their susceptibility to cerebral ischemic injury. Conversely, damaged mitochondria released from neurons can be rapidly degraded within astrocytes, thereby mitigating oxidative stress after stroke (Fairley et al., 2022). Some studies have shown that mitochondria can be transported between astrocytes and neurons, thereby increasing intracellular ATP levels, promoting neuronal survival and alleviating brain damage (Ni et al., 2022). Rb1 has been shown to block the production of ROS in mitochondria, suppress the activation of astrocytes, and facilitate the transfer of functional mitochondria from astrocytes to neurons following stroke. Thus, it has promising potential in stroke therapy (Jung et al., 2020; Liu et al., 2023). Moreover, in a mouse model of intracerebral hemorrhage, functional mitochondria released by astrocytes can also be transferred to microglia to stimulate the activation of M2 microglia. This process enhances their phagocytic capacity, leading to improved brain hematoma clearance and functional recovery (Jung et al., 2020). Whether this regulatory mechanism is also present in ischemic stroke warrants further investigation and validation in future studies. In astrocytes, low-density lipoprotein receptor-related protein-1 has been shown to suppress glucose uptake, glycolysis, and lactate production, consequently leading to reduced lactylation of ADP-ribosylation factor 1. Recent research has shown that astrocytic low-density lipoprotein receptor-related protein-1 mediates the transfer of healthy mitochondria from astrocytes to damaged neurons by inhibiting ADP-ribosylation factor 1 prenylation, thereby providing protection against brain ischemia‒reperfusion injury in MCAO mice (Zhou et al., 2024).\nTaken together, future research should focus on exploring the molecular pathways that enable the transport of mitochondria between astrocytes and neurons, as well as the roles of various neuroprotective agents, such as ginsenoside Rb1, in enhancing this process. Additionally, understanding the specific conditions in which mitochondria are transferred among microglia, astrocytes and neurons and the resulting effects on neuron recovery could unveil new therapeutic avenues for ischemic stroke, potentially leading to the development of targeted treatments that harness the neuroprotective properties of astrocytes.\nCytotoxic edema is caused by disruption of ionic homeostasis and is characterized mainly by excessive cell swelling. Aquaporins, which are predominantly expressed in epithelial cells and astrocytes within the CNS, are critical for maintaining water homeostasis. Astrocytic endfeet surrounding the BBB regulate neural microenvironmental water homeostasis through AQP4 channels (Zhou et al., 2022). Increased AQP4 expression in astrocytes increases water influx, leading to astrocytic swelling and exacerbating brain edema at the injury site (Kitchen et al., 2020). Inhibition of AQP4 expression in mice reduces astrocytic swelling, thereby alleviating the neurogenic pain associated with brain edema in stroke models (Kitchen et al., 2020; Sun et al., 2022). The sulfonylurea receptor 1-transient receptor potential M4 channel was first discovered in postischemic astrocytes. Sulfonylurea receptor 1-transient receptor potential M4 and pH-sensitive Na\nThe BBB is a multicellular vascular structure that separates the CNS from the external environment, serving as a barrier to strictly control the entry and exit of substances through tight junctions between endothelial cells, preventing exogenous neurotoxic substances from entering the CNS and protecting the brain from external pathogen interference. This tight connection involves three transmembrane proteins, namely, claudins, occludin, and junctional adhesion molecules, which collectively maintain the low permeability of the BBB (Qiu et al., 2021; Lu and Wen, 2024).\nDuring ischemic stroke, however, the BBB is disrupted. This disruption leads to vascular edema, breakdown of tight junctions between endothelial cells, altered BBB permeability, and entry of toxic molecules from the external environment into the brain (Huang et al., 2020; Yang and Torbey, 2020; Xue et al., 2023). In this context, astrocytes play a crucial role in regulating the BBB following ischemic stroke. Nhe1 is the primary pathway for Na\nOn one hand, astrocytes can exacerbate inflammation and disrupt BBB integrity by upregulating the expression of various factors, such as vascular endothelial growth factor (VEGF), chemokines (monocyte chemoattractant protein-1, stromal cell-derived factor 1 and chemokine (CC-motif) ligand 5), cytokines (TNF-α, IL-1β, IL-6, and IL-15), ROS, matrix metalloproteinases (MMPs), and lipocalin-2. Specifically, VEGF secreted by astrocytes can decrease the expression of endothelial cell tight junctions, exacerbating BBB damage (Kim et al., 2020), as shown in\nDuring the acute phase, BBB disruption and astrocyte activation lead to the secretion of inflammatory factors, directly or indirectly exacerbating BBB dysfunction. The upregulation of IL-1β increases the expression of VEGF-A in activated astrocytes, which disrupts endothelial cell tight junctions by downregulating claudin-5 expression, thereby increasing BBB permeability and worsening brain injury (Li et al., 2022). Conversely, in the late stage of stroke, VEGF can promote vascular regeneration and improve neurological function (Moon et al., 2021). Therefore, VEGF plays different roles in BBB maintenance during different stages of the stroke process.\nStroke leads to the secretion of MMPs by astrocytes, which disrupt tight junction-associated proteins of endothelial cells and ECM molecules. Inhibiting MMP2/9 can enhance BBB integrity, reduce astrocyte activation, and alleviate cognitive impairment in mice (Ji et al., 2023). Secreted by astrocytes, sonic hedgehog (Shh) is a glycoprotein that regulates the expression of zonula occludens-1 and claudin-5 to protect the integrity of the BBB. Research has shown that Shh upregulates angiopoietin-1 (Ang-1) to modulate endothelial cell tight junctions, thereby alleviating stroke-induced brain edema (Michinaga et al., 2024).\nAstrocytes act as both mediators of BBB disruption and facilitators of its repair. Future studies should further elucidate the contradictory roles of astrocyte-secreted factors, such as VEGF and MMPs. This knowledge will enable the development of interventions that effectively control their harmful effects while enhancing their regenerative potential. In addition, a comprehensive exploration of the protective mechanisms of molecules such as insulin-like growth factor-1 and Shh is crucial. Understanding these mechanisms may optimize therapeutic strategies and improve overall outcomes in stroke patients.\nThe potential of astrocytes as therapeutic targets for ischemic stroke has been demonstrated through various approaches, including chemical medications, miRNA therapy, stem cell therapy, cellular reprogramming, hydrogel applications, and astrocyte-derived extracellular vesicles (ADEVs), each offering promising therapeutic avenues.\nThe advantages of medication therapy for ischemic stroke lie in its convenience and affordability. However, it is limited by drug tolerance, poor efficacy, drug interactions, and other factors (Marto et al., 2021). MiRNA therapy aims to regulate miRNA levels, potentially mitigating the detrimental effects of ischemic stroke and promoting the recovery of neurological function by influencing cellular functions and biological processes (Eyileten et al., 2018; Vijayan et al., 2018). Stem cell therapy utilizes the pluripotency and regenerative capabilities of stem cells to repair damaged tissues and promote neuroregeneration. However, these methods face challenges such as allogeneic transplant rejection, difficulties in controlling cell differentiation posttransplantation, and risks of tumorigenesis (Gautam et al., 2020; Mazini et al., 2020; Hoang et al., 2022). Reprogramming involves the in situ conversion of endogenous glial cells into functional neurons for CNS repair, thereby avoiding the uncertainties mentioned above (Sharif et al., 2021). Hydrogels, as carriers, have achieved promising results through drug‒cell combination therapy or the coadministration of small molecules and exosomes (Damian et al., 2021; Fan et al., 2024). ADEVs regulate cellular activities by carrying neuroprotective factors, suggesting potential applications in the treatment of brain injuries and neurodegenerative diseases. However, their functions are influenced by environmental factors and inflammation (Wang et al., 2024).\nIn recent years, research on the role of astrocytes in stroke, along with emerging therapeutic approaches targeting these glial cells, has proliferated. Despite these advances, each approach faces technical challenges and limitations, requiring further research and technological advancements to optimize its application in the treatment of ischemic stroke. The therapeutic strategies related to astrocytes following stroke are summarized in\nSummary of astrocyte-targeted therapeutic strategies following stroke\nADEVs: Astrocyte-derived extracellular vesicles.\nMedication therapy is a commonly used treatment method for ischemic stroke. Its advantages include convenience, affordability, and the accumulation of extensive clinical experience over time. However, medication may have side effects and limitations, such as drug tolerance, poor efficacy, and drug interactions. Moreover, relying solely on medication often cannot achieve desirable brain tissue repair and regeneration in ischemic stroke treatment (Marto et al., 2021).\nMedications targeting astrocytes have shown potential in promoting neurological recovery after ischemic stroke. For example, matrine treatment can reduce the quantity of neuroprotective-phenotype astrocytes while increasing the number of neurotoxic-phenotype astrocytes. It also enhances the expression of the tight junction proteins Claudin 5 and Occludin. This collectively aids in reducing neurological injuries poststroke (Jing et al., 2021). Celastrol decreases ROS levels by upregulating Nrf2 in astrocytes, eventually reducing neuronal damage (Hong et al., 2023). The small peptide OM-LV20 can upregulate pituitary adenylate cyclase-activating peptide type 1 receptor expression in astrocytes, thereby increasing the expression of tryptophan hydroxylase 1 in these cells, which in turn protects them from oxidative stress-induced damage (Yin et al., 2022). Another promising drug candidate, potassium 2-(1-hydroxypentyl)-benzoate, which is currently in phase II clinical trials for ischemic stroke, has demonstrated protective effects on neurons, particularly in the presence of astrocytes. Studies have revealed that potassium 2-(1-hydroxypentyl)-benzoate therapy stimulates the release of astrocyte-derived neurotrophic factors, including brain-derived neurotrophic factor and nerve growth factor, thereby reducing neuronal death after ischemic stroke (Liu et al., 2017). Salidroside has been reported to inhibit astrocyte overreaction and glial scar formation by suppressing the AKT/GSK3β signaling pathway (Dong et al., 2021). Consequently, salidroside also shows promise in the treatment of ischemic stroke.\nEdaravone is a neuroprotective agent clinically employed in China and other Asian countries for treating ischemic stroke. It prevents neuronal death by inhibiting lipid peroxidation, scavenging free radicals, acting as an antioxidant, inhibiting ischemic brain edema, and improving neurological function (Kawasaki et al., 2020). Borneol is an organic compound derived from herbs with analgesic, anti-inflammatory, and anti-epileptogenic properties (Wang et al., 2024). It has been demonstrated that combining edaravone and (+)-borneol effectively inhibits microglia and astrocyte polarization toward a proinflammatory phenotype (Wang et al., 2024). This combination also reduces the infiltration of inflammatory cells such as leukocytes, alleviates neuroinflammation, improves the permeability of the BBB, and significantly reduces cerebral infarctions (Huang et al., 2022; Wang et al., 2024). The present phase III clinical trial demonstrated that female patients who received intravenous edaravone dexborneol treatment within 48 hours post-stroke exhibited better postoperative recovery outcomes than did those treated with edaravone (Xu et al., 2021b). In addition, intravenous administration of ABT263, an antiaging drug, to rats following stroke resulted in the elimination of senescent astrocytes in the brain injury area, reduced the infarcted area of the rat brain, and improved neurological function (Lim et al., 2021).\nThe emergence of drugs targeting astrocytes for the treatment of poststroke neural injury underscores the essential role of astrocytes in the modulation of neurological function after stroke. Further elaboration of astrocyte function during the poststroke period will contribute to an in-depth understanding of the cellular response after stroke and facilitate the research and development of astrocyte-targeting therapeutic drugs for stroke.\nMiRNAs constitute a class of noncoding, single-stranded RNA molecules involved in the posttranscriptional regulation of gene expression and play active roles in cell proliferation, differentiation and function (Kim et al., 2018). miRNA therapy involves regulating the expression levels of miRNAs to intervene in gene expression, thereby impacting cellular functions and biological processes. Its advantages lie in its high specificity and regulatory precision, enabling precise control over specific genes (Eyileten et al., 2018). However, miRNA therapy still faces challenges in aspects such as the effectiveness of delivery vehicles, selection of target genes, safety, which require further research for resolution (Vijayan et al., 2018). In summary, manipulating miRNA levels through nucleic acid–based therapies represents a promising strategy for developing targeted treatments that could mitigate the detrimental effects of ischemic stroke and promote neurological recovery.\nTargeted miRNA therapy is neuroprotective against stroke-related inflammation, glutamate excitotoxicity and glial scar formation. Research suggests that specific miRNAs positively regulate reactive astrocyte activity and function poststroke (Sun et al., 2019b; Li et al., 2021). MiR-210 targets the NF-κB pathway in astrocytes, reducing the expression of the proinflammatory factors IL-6 and C–X–C motif chemokine ligand 10 to inhibit the inflammatory cascade (Kieran et al., 2022). Inhibiting miR-182 enhances astrocyte viability and reduces the release of the inflammatory factor NO during lipopolysaccharide stimulation, thereby exerting an anti-inflammatory effect (Long et al., 2024). Conversely, astrocytes pretreated with berberine and subjected to OGD secreted greater levels of miR-182-5p than did astrocytes treated with only OGD. Upon intravenous injection into MCAO model mice, miR-182-5p-enriched exosomes facilitate neuronal uptake and reduce Rac1 expression, thereby attenuating neuroinflammation (Ding et al., 2023). These results indicate that the regulatory roles of miRNAs in cells can significantly vary across different disease contexts, highlighting the complexity of miRNA-mediated regulation. GLT-1 maintains extracellular homeostasis and attenuates ischemic injury by transferring accumulated glutamate. It has been reported that miR-124 enhances GLT-1 expression in astrocytes via protein kinase B and mechanistic target of rapamycin pathways during cerebral ischemia, improving stroke outcomes (Long et al., 2024). Neuroglial scar formation impedes axonal regeneration and functional recovery postischemic stroke. STAT3 regulates astrogliosis, and miR-124 expression is significantly elevated in M2 microglia exosomes. This elevation could decrease STAT3 expression in astrocytes, reduce neuroglial scar formation, and promote neurological function recovery after stroke (Li et al., 2021). Recent meta-analysis results indicate that miR-124-3p is specifically expressed in the CNS and that its expression is lower in stroke patients than in healthy controls (Wang et al., 2023). Therefore, miR-124-3p could serve as a potential clinical diagnostic biomarker for ischemic stroke.\nRecent studies have demonstrated the successful transdifferentiation of fibroblasts and oligodendrocytes into neurons through miRNA manipulation (Han et al., 2021b; Gu et al., 2022), suggesting a promising avenue for treating neurological injuries via transdifferentiation. Furthermore, both adult human and mouse astrocytes can be reprogrammed into neuroblasts\nExosomes, which are characterized by low immune reactivity, high stability, and the ability to cross the BBB, serve as significant miRNA carriers and have recently garnered attention. Recent research has indicated that miRNAs within mesenchymal stem cell (MSC)-derived exosomes play a vital role in the restoration of brain functions. There are currently only two registered clinical trials involving the use of exosomes in the treatment of stroke, a multicenter, randomized, double-blinded, placebo-controlled, dose-escalation trial initiated by Xuanwu Hospital in China to evaluate the safety and preliminary efficacy of intravenous exosomes derived from human induced pluripotent stem cells (GD-iExo-003) in acute ischemic stroke (\nStem cell therapy capitalizes on the pluripotent and regenerative capacities of stem cells, offering promising avenues for effectively repairing damaged tissues and promoting the regeneration of brain tissue both in neurons and blood vessels (Li et al., 2021). In addition, this therapeutic strategy exploits the immunomodulatory functions of stem cells, which are effective in restoring immune homeostasis and creating an immune microenvironment that facilitates tissue repair (Mazini et al., 2020; Yang et al., 2021; Hoang et al., 2022; Li et al., 2022). Despite its potential, stem cell therapy has several challenges, including immune rejection in allogeneic transplantation, difficulties in controlling cell differentiation postimplantation, and the inherent risks of tumorigenesis associated with pluripotent stem cell derivatives, highlighting the need for additional research to address these challenges (Stonesifer et al., 2017; Gautam et al., 2020; Li and Fang, 2023).\nAmong the cell types extensively tested in preclinical experiments and clinical trials, particularly for ischemic stroke, are hematopoietic stem cells, MSCs, neural stem cells, embryonic stem cells, and induced pluripotent stem cells (Kawabori et al., 2020). Stem cells mediate therapeutic effects predominantly through two mechanisms: cell differentiation and the secretion of paracrine factors (Li et al., 2021). The differentiation of transplanted stem cells into functional neurons, glial cells, and endothelial cells, which contribute to reducing inflammation and enhancing neuronal plasticity, represents the primary goal of stem cell therapy in neurological diseases (Anthony et al., 2022). Concurrently, stem cells are known to produce and secrete a diverse array of chemokines, growth factors, and extracellular vesicles (EVs), which play significant roles in mediating anti-inflammatory, antiapoptotic, angiogenic, and neurogenic effects in various neurodegenerative diseases (Zhou et al., 2022; Hirsch et al., 2023; Zhang et al., 2023). Houkin et al. (2024) reported on a phase 2/3 TREASURE randomized clinical trial of bone marrow-derived allogeneic stem cells for the treatment of acute ischemic stroke, and the results indicated that this therapy was safe for 90 days but did not improve short-term outcomes; further studies are needed to determine whether it is beneficial for patients who meet specific criteria. Lee et al. (2022) reported a neuroimaging study of the efficacy of intravenously injected MSCs on motor recovery after ischemic stroke, which demonstrated that MSCs protected the corticospinal tracts from degradation and enhanced positive changes in network reorganization, facilitating motor recovery.\nRecent advancements in stem cell research have demonstrated the efficacy of MSCs derived from adipose-derived stem cells, which can be efficiently induced into neural progenitors and can be used for cell replacement therapy during ischemic stroke (Wang et al., 2022). Chiu et al. (2022) reported a phase I study of the intracerebral transplantation of autologous adipose-derived stem cells for the treatment of chronic ischemic stroke, which revealed significant improvements in neurological measures, including the National Institute of Health stroke scale, Barthel Index, Berg balance scale, Fugl-Meyer assessment, and somatosensory evoked potentials, in all three participants with chronic stroke without adverse effects within 6 months, demonstrating the broad potential of adipose-derived stem cells in the treatment of CNS disorders such as stroke (Chiu et al., 2022). Intracerebral injection of conditioned media from MSCs derived from human embryonic stem cells to treat MCAO rats significantly enhances neurogenesis and reduces ischemic brain injury (Asgari Taei et al., 2022). Additionally, intracerebral injection of MSCs overexpressing fibroblast growth factor 21 significantly reduces infarct volume, decreases the expression of proinflammatory proteins, attenuates BBB degradation, and enhances motor function in rats (Do et al., 2024). Notably, MSCs contribute to therapeutic outcomes not only through the secretion of soluble bioactive molecules but also via the release of EVs, which are crucial in mediating intercellular communication. These vesicles facilitate the transfer of information to recipient cells, such as astrocytes and neurons, in the brain, enhancing cellular communication and potentially improving recovery processes (Do et al., 2024).\nFurther investigations revealed that MSCs promote the survival of microglia, astrocytes, and endothelial cells postischemia. This improvement is attributed to the secretion of growth factors and exosomes, along with the transfer of functional mitochondria, thus assisting in cellular repair and neuroprotection (Deng et al., 2019; Liu et al., 2019, 2021). Moreover, the cotransplantation of neural stem cells with astrocytes and brain microvascular endothelial cells has been shown to improve memory functions in ischemic rats. This enhancement suggests that the optimum milieu provided by astrocytes and brain microvascular endothelial cells supports the survival and differentiation of transplanted neural stem cells\nNevertheless, despite the demonstrated safety of MSC therapy in a clinical setting, the effectiveness of the treatment remains uncertain, and proven efficacy has not been achieved. Therefore, the interactions among stem cells, astrocytes, and/or other resident cells in the brain hold great potential targets for achieving more effective therapeutic outcomes. Cotransplantation of stem cells with astrocytes may emerge as a promising strategy for the treatment of ischemic stroke.\nAs discussed earlier, stem cell therapies present several challenges, including the risk of tumorigenesis, ethical concerns, unpredictable differentiation paths, and immune rejection. Given these instability factors, an alternative approach that has garnered attention is the in situ reprogramming of endogenous glial cells into functional neurons to rebuild CNS repair. This method may serve as a safe and reliable alternative option for treating ischemic stroke. Simultaneously, the in situ reprogramming of endogenous glial cells may also reduce the formation of glial scars following neurological injuries caused by ischemic stroke and prevent excessive inflammatory responses, thereby facilitating recovery from secondary damage. Although the technology for glial cell reprogramming is still in its nascent stage, promising prospects have been highlighted by a number of studies (Magnusson et al., 2020; Sharif et al., 2021).\nSeveral studies have robustly demonstrated that astroglia, both during early postnatal development and in the adult stage, can be reprogrammed to acquire a neuronal fate (Peng et al., 2022; Talifu et al., 2023). This transformation can be achieved\nThe clinical efficacy of this innovative approach across various neurological disorders has been well documented. Reprogrammed astrocytes have demonstrated significant therapeutic potential in the treatment of Parkinson’s disease (Zhu et al., 2019; Giehrl-Schwab et al., 2022; Wang et al., 2023), ischemic stroke (Chen et al., 2020; Ge et al., 2020), Huntington’s disease (Wu et al., 2020), spinal cord injuries (Puls et al., 2020; Tan et al., 2022), and AD (Yavarpour-Bali et al., 2020). Consequently, the astrocyte reprogramming strategy presents a promising avenue for developing astrocyte-centered therapies, particularly for ischemic stroke and other neurological conditions.\nOn the basis of astrocyte secretion profiling analysis, another approach involving direct delivery of ADEVs may offer another intervention strategy for ischemic stroke. Recent investigations have underscored the critical role of ADEVs in preserving brain homeostasis and influencing the functionality of other CNS cells. These nanosized, double-membraned vesicles efficiently cross the BBB, enabling the intercellular transfer of proteins, nucleic acids, and lipids that are instrumental in modulating cellular activities (Edwardson et al., 2024). Specifically, ADEVs from naïve astrocytes contain neuroprotective agents such as fibroblast growth factor-2, VEGF, and apolipoprotein-D, which have demonstrated potential therapeutic benefits in treating brain injuries and neurodegenerative disorders (Upadhya et al., 2020; Wang et al., 2024).\nIn contrast, in neurodegenerative diseases such as stroke, AD, and Parkinson’s disease, ADEVs sourced from activated astrocytes are implicated in worsening pathological conditions (Li et al., 2023). Studies indicate that under conditions of oxidative stress or exposure to proinflammatory cytokines such as IL-1β, ADEVs contribute to reduced synaptic growth and increased neuronal apoptosis. Furthermore, stimulation by the anti-inflammatory cytokine IL-10 has been shown to facilitate synaptic transmission and enhance neuronal survival, highlighting the variable impact of environmental factors on ADEV functionality (Datta Chaudhuri et al., 2020; You et al., 2020). Moreover, ADEVs are known to harbor the glutamate transporter proteins Excitatory amino acid transporter-1 and Excitatory amino acid transporter-2, which are essential for maintaining low levels of extracellular glutamate, thus reducing excitotoxicity, a common consequence following stroke (Wang et al., 2024).\nIn conclusion, ADEVs are particularly notable for their ability to traverse the BBB efficiently, an advantageous characteristic that highlights their potential utility as therapeutic agents. Nevertheless, investigations into the multifaceted regulatory mechanisms of ADEVs under both physiological and pathological conditions remain in the preliminary phases. A comprehensive understanding of these dynamics is imperative for effectively utilizing ADEVs in clinical settings, thus offering new avenues for the development of targeted therapies that capitalize on their natural biological functions.\nIn pursuit of efficacious stroke therapies, it is essential to address the challenge of delivering therapeutic agents directly to damaged regions of the brain. The BBB plays a critical role in safeguarding the brain against potential toxins and pathogens but simultaneously poses a formidable barrier to the direct delivery of therapeutic drugs. In this context, hydrogels have garnered attention as a potent means to improve local drug delivery specifically tailored to treating neurological disorders.\nHydrogels, synthesized from a flexible matrix and insoluble polymers, undergo transformations from solution to gel via thermal, physical, and chemical stimuli. This process endows hydrogels with a soft and porous three-dimensional structure characterized by high water content, increasing their biocompatibility with brain tissue, which minimizes the risk of immune rejection, enhances their biocompatibility, and enables precise site-specific drug delivery that can be customized according to the tissue type (Fan et al., 2024; Gao et al., 2024). Hydrogels not only have the potential to modulate inflammatory responses but also create a conducive environment for cell growth and tissue repair, addressing key hurdles in neurological recovery (Mahmoudi et al., 2024).\nMagnetic resonance imaging studies of stroke patients consistently revealed tissue loss, or voids, in 94% of cases, a phenomenon also observed in experimental stroke models in mice (Damian et al., 2021; Farrher et al., 2021). Tissue cavitation can lead to the infiltration of immune cells. Nevertheless, these cavities in the stroke-affected brain present promising targets for therapeutic interventions, as they can accommodate multiple hydrogel injections without disrupting surrounding healthy brain structures. Further research utilizing hyaluronic acid-based microporous annealed particle hydrogels in MCAO mice revealed profound therapeutic benefits. Postinjection, there was notable infiltration of astrocytes from the peri-infarct area into the stroke cavity. This migration is coupled with a reduced thickness of the astrocytic scar around the infarct core, effectively curbing the excessive influx of activated microglia and further alleviating neuroinflammation (Sideris et al., 2022).\nMoreover, the effectiveness of hydrogels in gene therapy has also been validated. For example, hybrid adeno-associated viruses carrying neural reprogramming transgenes have been precisely and controllably delivered to a mouse model of traumatic brain injury via hydrogels. This innovative approach facilitates the transdifferentiation of reactive astrocytes into neurons, markedly reducing the formation of the glial scar, thus highlighting the transformative potential of hydrogels in advanced brain injury treatments (Mahmoudi et al., 2024). In the CNS, hydrogels serve as scaffoldings that facilitate the reconstitution of the ECM and support the migration of adjacent cells. Furthermore, when combined with therapeutic agents or stem cells, hydrogels play a pivotal role in mitigating inflammation and preventing the formation of glial scar perilesions or lesion areas (Khan et al., 2021; Yu et al., 2022). This convergence of favorable characteristics makes hydrogels highly promising platforms for overcoming the current limitations faced in the targeted therapy of stroke.\nTaken together, these findings underscore the potential of hydrogels as versatile and effective therapeutic delivery tools in the treatment of stroke. Their common use as carriers, particularly through combination with drugs, cells, small molecules and exosomes, has led to promising treatment outcomes through the provision of structural support or sustained release. Such multifaceted approach systems may help to overcome some drawbacks associated with a single intervention approach.\nThis review provides a systematic overview of the fundamental functions of astrocytes, with a particular focus on their role in ischemic stroke, and discusses various astrocyte-based therapies for stroke. Given the scope of this article, we summarize key advancements in research on astrocytes in stroke in recent years, including studies on modulators of neuroinflammation, protection against oxidative damage, mitochondrial interactions with neurons, the regulation of water homeostasis, and the dual roles of astrocytes at the BBB. However, other roles of astrocytes in ischemic stroke have not been extensively discussed.\nIn the therapeutic section, we summarize astrocyte-related therapies and their preliminary clinical applications but do not fully address the challenges and limitations encountered in clinical translation. Additionally, the potential for synergistic effects from combining different therapeutic approaches has not been explored in depth. This manuscript focuses on the mechanistic roles of astrocytes in ischemic stroke; most of the evidence is extracted from animal data rather than the corresponding clinical data, although the latter may be more exciting and deserve much more attention. Another reason may be the absence or unavailability of some ongoing clinical trials related to this review topic.\nOver the past few decades, attention to the pathogenesis and treatment of ischemic stroke has focused primarily on neurons, whereas astrocytes have been largely overlooked. As the most abundant glial cells in the brain, astrocytes play crucial roles in the formation of tripartite synapses and neurovascular units, and they actively communicate with nearly all types of brain cells. An increasing number of studies have shown that astrocytes are important regulators of various CNS diseases. Under healthy physiological conditions, astrocytes supply energy for neuronal activity, support brain tissue metabolism, regulate neurotransmitter levels, and control neuronal communication, as depicted in the light blue section of\nFunctions of astrocytes under normal and ischemic conditions.\nIn a healthy state, astrocytes sustain neuronal activity by supplying energy, regulating metabolism, balancing neurotransmitter levels, and facilitating neuronal communication. Under ischemic conditions, astrocytes undergo differentiation into distinct subtypes: A1 astrocytes release inflammatory factors that can compromise neuronal function, whereas A2 astrocytes release neurotrophic factors and GF, thereby promoting neuronal protection and facilitating angiogenesis. This dual response highlights the intricate interplay between astrocytes and neurons in both physiological and pathological states. AKT/GSK3β: Protein kinase B/glycogen synthase kinase 3β; d,l-PHPB: potassium 2-(1-hydroxypentyl)-benzoate; GF: growth factor; NF-κB: nuclear factor kappa-B.\nAfter ischemic injury, astrocytes undergo activation and adopt different states, including neurotoxic type A1 and neuroprotective type A2. In the neuroprotective A2 state, astrocytes release neurotrophic factors that promote angiogenesis and synaptogenesis, reduce excitotoxicity, and facilitate neurological recovery following stroke, as shown in the pink section of\nAlthough the dual roles of astrocytes are not yet fully understood, their potential in neuroprotection and repair processes has increasingly attracted attention. With the advent of single-cell genomics and spatial genomics, analyzing the mechanisms of different astrocyte subtypes at various stages of stroke progression is expected to become a key research direction. This approach may facilitate the transformation of harmful astrocyte subtypes into beneficial subtypes, thereby enhancing therapeutic outcomes.\nRecent research into the critical role of astrocytes in ischemic stroke has increased, and various treatment strategies have been proposed. These include chemical drugs, miRNA therapy, stem cell therapy, cellular reprogramming, hydrogel applications, and ADEVs. However, each of these approaches has limitations and drawbacks. The key challenge in improving these therapeutic strategies is to further investigate the specific mechanisms underlying which astrocytes contribute to ischemic stroke and integrate advanced biotechnological methods for optimizing these treatments. One major area of stroke research is how to enhance the neuroprotective role of astrocytes while minimizing their potential harmful effects on neurons. Therefore, a comprehensive understanding of how astrocytes respond to stroke, when to target astrocytes, and which astrocyte subtypes to target is crucial for the development of safer and more effective stroke therapies.", "content_for_embedding": "Stroke is a heterogeneous category of central nervous system (CNS) disorders characterized by sudden, focal interruptions of cerebral blood flow, usually caused by a blockage of blood vessels (ischemic stroke) or a localized intracranial bleeding event (hemorrhagic stroke) that results in long-term neurologic impairments. Ischemic stroke, which accounts for approximately 80% of all strokes, is typically caused by cardiogenic embolism, cerebral microcirculatory dysfunction, atherosclerosis affecting arteries within or outside of the brain, or coagulation disorders (Barthels and Das, 2020; He et al., 2024). Stroke is the second leading cause of death and disability worldwide (Feigin et al., 2022). Following a stroke, patients commonly experience hemiparesis and speech and cognitive impairments, among other chronic neurological dysfunctions (Boyne et al., 2023; Johnston et al., 2024; Messimeris et al., 2024). Additionally, these individuals may suffer from persistent neurological complications such as movement disorders, depression, tremors, and dysphagia, all of which can significantly impact their functional recovery, quality of life, and survival (Lekoubou et al., 2023). Therefore, identification of effective therapeutic methods to reduce the global economic burden of stroke, including various acute care interventions, surveillance, and rehabilitation, is urgently needed.\nAstrocytes, also known as astroglia or astroglial cells, are star-shaped cells that account for approximately 40% of the total cells in the brain (Trujillo-Estrada et al., 2019; Zhou et al., 2019). They originate from radial glial cells in the ventricular region of the embryo. Astrocytes are traditionally divided into fibrous and protoplasmic types according to their anatomical location and morphology. Fibrous astrocytes are typically localized in the white matter of the CNS. These cells are usually elongated in shape, with long, slender processes and fewer branches at synapses. Protoplasmic astrocytes, on the other hand, are typically found in gray matter and represent a more common type of astrocyte (Hasel and Liddelow, 2021). Their cell bodies are rounded, with short and densely packed branches. The morphological variations in these cells are closely associated with the functions of different brain regions (Bugiani et al., 2022; Hasel et al., 2023). Gray matter astrocytes form tripartite synapses with neurons, where they envelop neurons and facilitate information transfer between neighboring neuron cells by releasing neurotransmitters. White matter, characterized by its myelin-rich composition, relies significantly on fibrous astrocytes for the facilitation of myelination and remyelination processes, which are crucial for the formation and maintenance of the axonal myelin sheaths essential for efficient electrical signal propagation (Xu et al., 2023). Despite the morphological and functional diversities exhibited by astrocytes, they interact with neurons, oligodendrocytes, endothelial cells of blood vessels, and other parenchymal cells in the CNS, collectively constructing intricate neural networks (Köhler et al., 2021). As integral components of normal neural function, astrocytes have become a research focus for the treatment of ischemic brain injury (\nThe roles of astrocytes in stroke treatment.\nThe five main pathways through which astrocytes influence the stroke-affected brain, as reviewed in this article, are shown at the edges of the circle. Six strategies for targeting astrocytes to treat stroke are depicted. ADEVs: Astrocyte-derived extracellular vesicles; AQP4: aquaporin 4; ARE: antioxidant responsive elements; BBB: blood–brain barrier; GSH: glutathione; IGF-1: insulin-like growth factor 1; NO: nitric oxide; Nrf2: nuclear factor erythroid 2-related factor 2; ROS: reactive oxygen species; TNF: tumor necrosis factor.\nIn this review, we first provide an overview of the basic physiological functions of astrocytes in normal or ischemic brains. We then discuss emerging insights into the diverse roles of astrocytes in ischemic stroke, including regulating neuroinflammation, reactive oxygen species, mitochondrial transportation, water homeostasis, and the blood‒brain barrier (BBB). We also highlight the probable key cellular and molecular targets identified in these studies. Finally, we review the frontier astrocyte-targeting intervention strategies for ischemic stroke, further categorizing them into traditional small chemical molecules, microRNAs, stem cells, reprogramming, hydrogels, and astrocyte-derived extracellular vesicles, with the goal of promoting novel astrocyte-targeting therapeutic development.\nIn this narrative review, articles published from inception to November 2024 were retrieved from PubMed. The following keywords were used to identify relevant research: “ischemic stroke,” “astrocyte,” “stroke,” “reactive astrocyte,” “neuron,” “middle cerebral artery occlusion,” “MCAO,” “synapse,” “oxidative damage,” “neuroinflammation,” “blood–brain barrier,” “microRNA,” “hydrogel,” “stem cell,” and “extracellular vesicle.” We comprehensively accessed the literature via various combinations of the above search terms and initially reviewed the titles and abstracts. Once the content was deemed relevant to our aim, we assessed the entire article. The majority of references (\nTimeline and key milestones in astrocyte research on stroke.\nBBB: Blood‒brain barrier.\nAstrocytes are the most abundant cell type in the CNS and play crucial roles in various physiological activities, including synaptic formation and maturation, neurotransmitter metabolism, ion balance, and metabolic modulation of neuronal cells, as well as the formation and maintenance of the BBB, as shown in\nRoles of astrocytes in the CNS.\n(A) Modulation of synaptic formation. TSP1/2 secreted by astrocytes bind to α2δ-1 receptors on synapses and activate Rac1, which further stimulates actin, promoting continuous synapse growth. Hevin secreted by astrocytes bridges Nrx-1α on the presynapse and NL-1β on the postsynaptic space to promote synaptogenesis. (B) Modulation of synaptic transportation and ion homeostasis. Excessive glutamate released from the presynapse can activate NMDARs and AMPARs localized on the postsynapse, resulting in high Ca\nProtoplasmic astrocytes located in gray matter participate in tripartite synapses and play a dynamic role in synaptic development and regulation (Zhou et al., 2019). During development, they promote synaptic formation by secreting molecules that organize synapses. Thrombospondins (TSP1/2), extracellular glycoproteins secreted by astrocytes, modulate synaptogenesis in the CNS and are essential for excitatory synaptic formation, with implications for neurological disorders (Qin et al., 2019a). The α2δ-1 protein, a membrane-anchored glycoprotein highly expressed in the brain, binds to TSP ligands from astrocytes, promoting excitatory synapse formation by recruiting NMDARs to the postsynaptic surface and facilitating dendritic spine maturation (Ablinger et al., 2020). Rac1, a key downstream signaling molecule in the TSP-α2δ-1 pathway, plays a critical role in synaptic remodeling. Upon TSP binding to α2δ-1 on neurons, Rac1 is activated, leading to actin reorganization that supports synaptic structural maturation (Risher et al., 2018; Fan et al., 2021). Although TSP1/2 expression decreases after brain development, its upregulation in the adult cortex following brain injury triggers extensive synaptic remodeling (Wang et al., 2021). High endothelial venule protein/SPARC-like 1 (Hevin/SPARCL1), another synaptic protein secreted by astrocytes, interacts with neurexin, which is involved in presynaptic vesicle release, and neuroligin, which is associated with postsynaptic proteins and neurotransmitter receptors. Hevin bridges neurexin and neuroligin, crucially influencing the formation and maturation of both excitatory and inhibitory synapses (Chen et al., 2022), as shown in\nIn addition to their role in synaptic development, astrocytes play crucial roles in maintaining the homeostasis of the microenvironment surrounding neuronal synapses. Glutamate is the main excitatory neurotransmitter in neuronal signal transmission. The normal concentration of glutamate in the CNS facilitates neuronal communication, but excessive extracellular glutamate can lead to neuronal overexcitation, causing excitotoxicity and even affecting neuronal survival (Andersen et al., 2021). The glutamate transporters (GLAST and GLT-1), which are located on the astrocyte membrane, can take up excessive amounts of extracellular glutamate, reducing the glutamate concentration in the microenvironment and keeping neurons in a normal physiological state (Pajarillo et al., 2019). The actively taken up glutamate is converted into glutamine within astrocytes and then transported back to neurons via membrane-bound glutamine transporters of the SLC38A family. SLC38A, also known as the glutamine transporter solute carrier family 38 member A, participates in the transportation of extracellular glutamate, aiding glutamine synthesis in astrocytes and converting glutamine back into glutamate in presynaptic neurons. Ultimately, glutamine is packaged into synaptic vesicles. This process is known as the Glu‒Gln cycle, which aids in maintaining the normal neurotransmission state of neuronal cells (Andersen et al., 2021). These cascaded events collectively highlight the intricate but precise interactions between astrocytes and neurons in maintaining normal synaptic function and responding efficiently to neurological challenges, as shown in\nAstrocytes are crucial for sustaining neuronal milieu stability and proper CNS functions through the regulation of neurotransmitter uptake and metabolism, as well as the modulation of ion channels and transporters. Astrocytes are integral in maintaining the balance of ions such as K\nGlutamate, the principal excitatory neurotransmitter in the CNS, can lead to neuronal hyperexcitability and neurotoxicity when released excessively. To counteract this effect, astrocytes express several Na\nUnder hypoxic conditions such as stroke injury, the activity of critical ion exchangers, including Na\nAstrocytes convert glutamate into γ-glutamylcysteine (γ-EC) via glutamate-cysteine ligase (GCL), which is subsequently catalyzed by glutathione synthetase (GS) to produce glutathione (GSH), a crucial antioxidant. Mitochondria are the powerhouses of the cell, providing ATP through oxidative phosphorylation in the mitochondrial respiratory chain, where electrons are generated by the tricarboxylic acid cycle (Han et al., 2021a). Neurons are critically dependent on ATP produced by mitochondria and are, therefore, exceptionally vulnerable to mitochondrial dysfunction. Moreover, mitochondria, which are enriched with redox enzymes, serve as the primary sites for reactive oxygen species (ROS) production within cells. Given their high oxygen demand, substantial content of polyunsaturated lipids, and limited antioxidant defenses, neurons are especially susceptible to ROS-induced oxidative damage (Singh et al., 2019). GSH, a key antioxidant, plays a vital role in reducing excessive ROS and other potential sources of ROS, thereby alleviating oxidative stress and preserving cellular redox homeostasis (Lapenna, 2023). As depicted in\nIn summary, astrocytes are crucial for maintaining neuronal stability and CNS function by regulating the uptake of neurotransmitters, particularly glutamate, which can be neurotoxic when excessively released. They express key transporters such as GLT-1 and GLAST to take up glutamate from synapses, converting it into glutamine via glutamine synthetase for recycling. During conditions such as stroke, astrocytes enhance ion exchanger functionality to mitigate excitotoxicity, highlighting their pivotal role in neuroprotection. Moreover, astrocytes contribute to antioxidant defense by synthesizing GSH, which is essential for reducing oxidative stress and preserving neuronal integrity. Overall, astrocytes are indispensable for maintaining ion balance, neurotransmitter recycling, and oxidative protection, which are crucial for CNS health and function.\nNeurons require a large amount of energy to maintain normal physiological function. However, neurons themselves cannot store energy substrates. In contrast, astrocytes can store metabolites rich in energy, including glycogen. Moreover, astrocytes exhibit greater glycolytic efficiency compared with neurons (Xue et al., 2022). To address oxygen deficiency, astrocytes can convert stored glycogen into glucose and supply energy substrates to neurons to meet their high energy demands (Chen et al., 2023). As shown in\nAdditionally, astrocytes can provide energy to neurons under hypoxic conditions through fatty acid metabolism and ketone body synthesis. Enhanced ketone body synthesis occurs when astrocytes are exposed to ischemic hypoxia and glucose-deprived environments. Like lactate, ketone bodies in astrocytes can also be delivered to neurons through monocarboxylate transporter proteins. During ischemia‒reperfusion, intracellular ketone bodies in neurons can be converted into acetyl-CoA and participate in the tricarboxylic acid cycle to generate ATP (Takahashi, 2022).\nTaken together, these results underscore the multifaceted role of astrocytes in managing and optimizing energy resources under ischemic conditions, highlighting their critical function in neuronal survival and brain resilience. Specifically, interventions aimed at increasing the efficiency of the lactate shuttle or ketone body synthesis pathways may offer new avenues to increase the energy supply of neurons, potentially improving outcomes in stroke patients. Future research could explore the potential of enhancing astrocyte metabolic functions as a novel therapeutic strategy to ameliorate ischemic stroke injury. Such therapeutic approaches could provide a significant breakthrough in treating not only ischemic stroke but also other neurological disorders where energy supply mismatches are evident.\nThe BBB is a crucial component in maintaining the homeostasis of the brain’s microenvironment (Jia et al., 2024; Wang et al., 2025). Its functions include regulating the permeability of vascular walls and restricting the passage of ions, molecules, and cells from the blood into the CNS, thus forming an isolating interface between neural tissue and the external environment (Candelario-Jalil et al., 2022). Additionally, the BBB regulates the transport of nutrients and metabolites and facilitates the expulsion of neurotoxic substances (Lochhead et al., 2020). The BBB is composed of endothelial cells, pericytes, smooth muscle cells, and astrocytes (Langen et al., 2019; Pociūtė et al., 2024). Astrocyte–endothelium interactions are essential for electrolyte homeostasis in the brain, both under normal and pathological states. These interactions strengthen tight junctions, reduce the gap junctional areas of endothelial cells, and contribute to the regulation of cerebral blood flow (Schiera et al., 2024). As depicted in\nThe integral role of astrocytes in maintaining BBB integrity and functionality suggests an important therapeutic target in the treatment of ischemic stroke and other neurovascular disorders. The development of strategies to enhance astrocyte‒endothelial interactions could lead to improved control of BBB permeability and stability, potentially reducing the pathological consequences of disrupted homeostasis. These advancements may pave the way for novel interventions aimed at preserving brain health and improving neurological outcomes following vascular challenges.\nAstrocytes can transform from a resting state to an activated state in response to neurodegeneration, pathogen infection, or cerebral ischemia. This transformation triggers a series of changes in gene expression, cellular morphology, and cellular function. Within hours of ischemia, the gene expression of astrocytes is altered in response to factors such as hypoxia, neuronal cell death, neurotransmitter release, and hemorrhage. Cytokines released by injured neurons and glial cells within the ischemic core and penumbra—such as tumor necrosis factor alpha (TNF-α), transforming growth factor, ciliary neurotrophic factor (CNTF), interleukin-1 (IL-1), and IL-6—induce astrocyte polarization. Astrocytes near the infarct core exhibit polarized morphology, whereas those located further from the lesion area display stellate morphology (Pekny et al., 2019). In the days following ischemia, astrocytes in the ischemic penumbra rapidly proliferate, with some migrating toward the infarct border, where they form glial scars by secreting extracellular matrix (ECM) molecules. These glial scars help protect healthy brain tissue by sealing the lesion site and preventing the infiltration of white blood cells. However, astrocytes within the scar also secrete inhibitory molecules that impede axonal regeneration, thus hindering neuronal recovery (Chen et al., 2023; Liang et al., 2024).\nReactive astrocytes exhibit considerable heterogeneity across the brain in various disease models, reflecting distinct functional profiles. Early transcriptome studies proposed that mouse astrocytes adopt either a neurotoxic (‘A1’) or a neuroprotective (‘A2’) phenotype in response to specific stimuli, such as cytokines released by microglia exposed to lipopolysaccharides or following ischemic stroke (Dimitrova-Shumkovska et al., 2020; Guttenplan et al., 2020). These phenotypes were characterized by a set of marker genes, with A1 astrocytes associated with neurotoxicity and A2 astrocytes associated with neuroprotection, as depicted in\nRoles of activated astrocytes in stroke.\n(A) Effects of astrocytes on neurons. Astrocyte A1 triggers an inflammatory response and neurotoxicity that disrupts neurons, whereas astrocyte A2 triggers an anti-inflammatory response and protects neurons through GSH. (B) Effects of astrocytes on the BBB. Astrocyte A1 releases inflammatory factors to disrupt the BBB, whereas astrocyte A2 releases anti-inflammatory factors to repair the BBB and promote the migration of vascular endothelial cells through VEGF. ARE: Antioxidant response elements; BBB: blood–brain barrier; BDNF: brain-derived neurotrophic factor; CCL-2: Chemokine (CC-motif) ligand 2; CNTF: ciliary neurotrophic factor; CXCL1: C-X-C motif chemokine ligand 1; GDNF: glial cell derived neurotrophic factor; GSH: glutathione; IFN-β: interferon-β; IL-10: interleukin-10; IL-1β: interleukin-1β; IL-6: interleukin-6; KLK6: kallikrein related peptidase 6; MCP-1: monocyte chemoattractant protein-1; MMPs: matrix metalloproteinases; NF-κB: nuclear factor kappa-B; NO: nitric oxide; Nrf2: nuclear factor E2-related factor 2; NT-3: neurotrophin-3; PARK7: Parkinson’s disease protein 7; ROS: reactive oxygen species; S-100β: S100 calcium-binding protein B; SDF1: stromal cell-derived factor 1; STAT3: signal transducer and activator of transcription 3; TNF-α: tumor necrosis factor-α; VEGF: vascular endothelial growth factor.\nTo better define astrocyte phenotypes, single-cell RNA sequencing has become an increasingly valuable tool for understanding the heterogeneity of both basal and reactive astrocytes (Mathys et al., 2019; Al-Dalahmah et al., 2020; Batiuk et al., 2020). Batiuk et al. (2020) utilized this approach to analyze the cortex and hippocampus of adult mice and identified five distinct molecular subtypes of astrocytes: AST1, AST2, AST3, AST4, and AST5. They also mapped the spatial distribution of these subtypes and investigated their roles within the CNS, as summarized in\nClassification of astrocytes based on single-cell RNA sequencing and spatial omics\nFurther investigations utilizing single-cell RNA sequencing in the cortical tissue of mice following ischemic stroke have provided valuable insights into the dynamic changes in astrocyte function (Ma et al., 2022; Zheng et al., 2022). At 12 hours poststroke, astrocytes predominantly exert neuroprotective effects by activating pathways such as oxidative phosphorylation, gap and tight junctions, and ferroptosis. The upregulation of the oxidative phosphorylation pathway suggests an energy deficit and increased energy demand in astrocytes during the acute phase of ischemia (Ma et al., 2022). Additionally, compared with astrocytes at 24 hours poststroke, more mitochondrial genes were upregulated at 12 hours, which may reflect either a greater proportion of dying cells or more severe metabolic dysfunction at this early time point (Ma et al., 2022). The activation of gap and tight junction pathways at 12 hours poststroke suggests the involvement of the blood–brain barrier and the enhancement of intercellular connections (Ma et al., 2022). In contrast, at 24 hours poststroke, pathways related to protein synthesis, lysosomal activity, and antigen processing and presentation were more prominently activated, suggesting a shift in astrocyte function toward enhanced interactions with the surrounding microenvironment. At this stage, astrocytes play a more prominent role as inflammatory mediators, releasing inflammatory signals such as cytokine‒cytokine receptor interactions, chemokine signaling, TNF signaling, nuclear factor kappa-B activation, and IL-17 signaling (Ma et al., 2022). This temporal shift in astrocyte function highlights their dynamic roles in both neuroprotection and inflammation during the progression of ischemic stroke. These findings demonstrate the advantages of single-cell RNA sequencing in recognizing the heterogeneity of reactive astrocytes at different time points following stroke.\nIn conclusion, astrocytes exhibit a dynamic and complex response to ischemic stroke characterized by a series of cellular and molecular changes. This response may promote neuroprotection in the early stages but later contributes to inflammation and immune regulation. The development of single-cell RNA sequencing technologies and spatial omics has greatly enhanced our ability to study astrocyte functions and their specific roles in disease, providing crucial insights for the development of effective therapies aimed at reducing neuronal damage and promoting recovery after stroke.\nUnlike the basic functions of healthy astrocytes outlined above, astrocytes undergo significant morphological and functional changes following neurological disorders such as stroke, primarily due to the disruption of brain homeostasis (Brandebura et al., 2023). These changes are characterized by hypertrophy, increased expression of glial fibrillary acidic protein, and the synthesis and release of inflammatory mediators and neurotrophic factors. Collectively, these alterations define the phenotype known as “reactive astrogliosis” (Chung et al., 2024). These changes impact the basic functions of astrocytes and have both beneficial and detrimental effects on the CNS.\nUnder hypoxic conditions, reactive astrogliosis inhibits synapse formation (Li et al., 2024), whereas uncontrolled astrocyte proliferation leads to the formation of glial scars, which hinder neuronal recovery (Zhu et al., 2022). Hypoxia enhances the activity of astrocyte ion exchangers, disrupting ionic balance and increasing neuronal excitability (Hernández et al., 2021). Additionally, reactive astrocytes release inflammatory mediators and ROS, intensifying neuroinflammation (Rauf et al., 2022) and compromising BBB integrity, which results in brain edema and the infiltration of toxic molecules into the brain (Huang et al., 2020).\nIn this section, we focus on summarizing the regulatory roles of astrocytes after stroke in neuroinflammation, oxidative damage, mitochondrial function, water homeostasis, and BBB integrity. Furthermore, the potential targets and related mechanisms in astrocytes for ischemic stroke therapy are summarized in\nPotential targets and related mechanisms in astrocytes for ischemic stroke therapy\nAng-1: Angiopoietin-1; AQP4: aquaporin 4; ARF1: ADP-ribosylation factor 1; BBB: blood‒brain barrier; BMP-2: bone morphogenetic protein 2; Akt: protein kinase B; E2: 17β-estradiol; ERK: extracellular signal-regulated kinase; GSDMD: gasdermin D; GSK-3β: glycogen synthase kinase 3β; JAK: Janus kinase; JNK: c-Jun N-terminal kinases; LCN2: lipocalin-2; LRP1: low-density lipoprotein receptor-related protein 1; MAPK: mitogen-activated protein kinase; NAD\nNeuroinflammation serves as a protective physiological response to preserve normal brain function. Nonetheless, excessive secretion of inflammatory mediators and persistent inflammation can cause significant damage to the CNS. In the brain, microglia and astrocytes orchestrate the release of inflammatory factors, including TNF-α, growth factors, adhesion molecules, and chemokines, following injury, such as stroke (Rauf et al., 2022), as depicted in\nFollowing ischemic brain injury, microglia in the vicinity of the damaged area are initially activated, followed by the activation of astrocytes. These activated astrocytes then stimulate and attract microglia from distant regions to migrate to the injury site, promoting the secretion of inflammatory factors and triggering a cascading, amplified neuroinflammatory response (Qin et al., 2019b). Sustained local inflammation leads to the upregulation of multiple signaling pathways in astrocytes. Notably, studies have shown that cerebral ischemia in mice induces the upregulation of the JAK/STAT pathway, which contributes to overload of the NLRP3 inflammasome. This overactivity promotes astrocyte activation and the subsequent release of astrocyte-derived IL-1β, thereby intensifying neuroinflammation (Xu et al., 2021a; Chen et al., 2022). Furthermore, inhibition of the elevated expression of lipocalin-2 in poststroke astrocytes can reduce NLRP3/Gasdermin D-induced pyroptosis (Li et al., 2023). Mitochondrial dysfunction has been reported to induce NLRP3 inflammasome activation. In turn, NLRP3 activation can elicit mitochondrial injury, leading to astrocyte release and increased production of ROS (Zhang et al., 2024). Inhibition of JAK2 through inhibitors reduces signal transducer and activator of transcription 3 (STAT3) phosphorylation, decreases the production of proinflammatory cytokines such as IL-1, IL-6, IL-8, and TNF-α, and provides neuroprotection following ischemia (Kumari et al., 2024), as illustrated in\nIn summary, astrocytes are crucial in regulating neuroinflammation throughout ischemic stroke. Reactive astrocytes not only facilitate the recruitment and activation of microglia but also amplify inflammatory responses through the secretion of various cytokines and chemokines. The dysregulation of inflammatory signaling pathways in astrocytes, such as the JAK/STAT and NF-κB pathways, contributes to neuroinflammatory exacerbation, potentially leading to neuronal damage. Specific inhibition of these pathways in astrocytes can therefore alleviate inflammatory responses and offer neuroprotection, underscoring the therapeutic potential of modulating astrocyte inflammatory activity in neuroinflammatory conditions after ischemic stroke.\nUnder normal conditions, the ROS generated in brain cells can be eliminated through the catalase pathway to maintain the intracellular balance of free radicals. Cerebral ischemia‒reperfusion results in excessive ROS production, leading to the accumulation of free radicals, causing oxidative stress and, subsequently, neuron damage (Shen et al., 2021). Mitochondria are the primary site of ROS production. During brain hypoxia, the respiratory chain is halted, leading to decreased mitochondrial membrane potential and significantly elevated ROS production. Moreover, elevated ROS levels sustain prolonged activation of the mitochondrial permeability transition pore and inner membrane anion channel, modifying both the intra- and intermitochondrial redox balance. This disruption leads to increased ROS production, reinforcing the ROS-induced ROS release mechanism. This cycle underscores that mitochondria are the primary contributors to ROS generation, which is pivotal in the pathogenesis of oxidative stress in stroke and other cerebral injuries (Zhu et al., 2022).\nMitochondria contain abundant antioxidants and enzymes such as superoxide dismutase, catalase, and GSH, which can eliminate excess accumulated ROS and other free radicals to counteract oxidative stress. Studies have shown that upregulating the expression of antioxidants in mitochondria can reduce the generation of ROS, thereby inhibiting the activation of microglia and astrocytes and effectively improving learning and memory impairments in mice caused by hypoxia (Han et al., 2020).\nIn addition, astrocytes can utilize the GSH antioxidant system to counteract the oxidative stress induced by excessive ROS. Compared with neurons, astrocytes exhibit greater glycolytic efficiency. Partial glycolytic products in astrocytes generate nicotinamide adenine dinucleotide phosphate through the pentose phosphate pathway, which can be used to reduce GSH. Glutathione peroxidase utilizes GSH to reduce ROS, thereby eliminating the excessive accumulation of ROS and alleviating oxidative stress between neurons and astrocytes(Shen et al., 2021). Increasing the levels of GSH and nicotinamide adenine dinucleotide phosphate can reduce ROS levels in the brains of rats after ischemia, thus inhibiting oxidative stress and preserving neurological function (Wei et al., 2019).\nIn addition to producing ROS, astrocytes also produce nitric oxide (NO). On one hand, a low dose of NO can induce vasodilation, improving the blood flow supply to the ischemic penumbra and exerting protective effects during the early stage of stroke. On the other hand, excessive NO reacts with O\nNuclear factor erythroid 2-related factor 2 (Nrf2) is involved in the regulation of oxidative stress in astrocytes. Following hypoxia, ROS stimulation leads to the binding of Nrf2 with antioxidant response elements (AREs), regulating the transcription of antioxidant enzymes (Han et al., 2022; Liu et al., 2023), as shown in\nIn summary, a precise understanding of the variation in mitochondrial dynamics in astrocytes and an analysis of their roles during ROS generation could lead to the development of targeted therapies to normalize mitochondrial function and alleviate oxidative stress. Additionally, investigating astrocytic antioxidant pathways, including the GSH system and Nrf2-mediated responses, may lead to intervention strategies that strengthen these pathways and reduce ROS-induced damage. Further research into the dual role of NO produced by astrocytes could also offer strategies to utilize its beneficial effects while minimizing its harmful effects, potentially promoting neurological recovery in stroke patients.\nAs discussed above, mitochondria play a critical role in the oxidative stress response induced by stroke. Following cerebral ischemia, an insufficient oxygen supply to mitochondria results in the generation of large amounts of ROS within neurons (Shen et al., 2021). Neurons have a limited capacity to activate glycolysis and defend against oxidative stress, increasing their susceptibility to cerebral ischemic injury. Conversely, damaged mitochondria released from neurons can be rapidly degraded within astrocytes, thereby mitigating oxidative stress after stroke (Fairley et al., 2022). Some studies have shown that mitochondria can be transported between astrocytes and neurons, thereby increasing intracellular ATP levels, promoting neuronal survival and alleviating brain damage (Ni et al., 2022). Rb1 has been shown to block the production of ROS in mitochondria, suppress the activation of astrocytes, and facilitate the transfer of functional mitochondria from astrocytes to neurons following stroke. Thus, it has promising potential in stroke therapy (Jung et al., 2020; Liu et al., 2023). Moreover, in a mouse model of intracerebral hemorrhage, functional mitochondria released by astrocytes can also be transferred to microglia to stimulate the activation of M2 microglia. This process enhances their phagocytic capacity, leading to improved brain hematoma clearance and functional recovery (Jung et al., 2020). Whether this regulatory mechanism is also present in ischemic stroke warrants further investigation and validation in future studies. In astrocytes, low-density lipoprotein receptor-related protein-1 has been shown to suppress glucose uptake, glycolysis, and lactate production, consequently leading to reduced lactylation of ADP-ribosylation factor 1. Recent research has shown that astrocytic low-density lipoprotein receptor-related protein-1 mediates the transfer of healthy mitochondria from astrocytes to damaged neurons by inhibiting ADP-ribosylation factor 1 prenylation, thereby providing protection against brain ischemia‒reperfusion injury in MCAO mice (Zhou et al., 2024).\nTaken together, future research should focus on exploring the molecular pathways that enable the transport of mitochondria between astrocytes and neurons, as well as the roles of various neuroprotective agents, such as ginsenoside Rb1, in enhancing this process. Additionally, understanding the specific conditions in which mitochondria are transferred among microglia, astrocytes and neurons and the resulting effects on neuron recovery could unveil new therapeutic avenues for ischemic stroke, potentially leading to the development of targeted treatments that harness the neuroprotective properties of astrocytes.\nCytotoxic edema is caused by disruption of ionic homeostasis and is characterized mainly by excessive cell swelling. Aquaporins, which are predominantly expressed in epithelial cells and astrocytes within the CNS, are critical for maintaining water homeostasis. Astrocytic endfeet surrounding the BBB regulate neural microenvironmental water homeostasis through AQP4 channels (Zhou et al., 2022). Increased AQP4 expression in astrocytes increases water influx, leading to astrocytic swelling and exacerbating brain edema at the injury site (Kitchen et al., 2020). Inhibition of AQP4 expression in mice reduces astrocytic swelling, thereby alleviating the neurogenic pain associated with brain edema in stroke models (Kitchen et al., 2020; Sun et al., 2022). The sulfonylurea receptor 1-transient receptor potential M4 channel was first discovered in postischemic astrocytes. Sulfonylurea receptor 1-transient receptor potential M4 and pH-sensitive Na\nThe BBB is a multicellular vascular structure that separates the CNS from the external environment, serving as a barrier to strictly control the entry and exit of substances through tight junctions between endothelial cells, preventing exogenous neurotoxic substances from entering the CNS and protecting the brain from external pathogen interference. This tight connection involves three transmembrane proteins, namely, claudins, occludin, and junctional adhesion molecules, which collectively maintain the low permeability of the BBB (Qiu et al., 2021; Lu and Wen, 2024).\nDuring ischemic stroke, however, the BBB is disrupted. This disruption leads to vascular edema, breakdown of tight junctions between endothelial cells, altered BBB permeability, and entry of toxic molecules from the external environment into the brain (Huang et al., 2020; Yang and Torbey, 2020; Xue et al., 2023). In this context, astrocytes play a crucial role in regulating the BBB following ischemic stroke. Nhe1 is the primary pathway for Na\nOn one hand, astrocytes can exacerbate inflammation and disrupt BBB integrity by upregulating the expression of various factors, such as vascular endothelial growth factor (VEGF), chemokines (monocyte chemoattractant protein-1, stromal cell-derived factor 1 and chemokine (CC-motif) ligand 5), cytokines (TNF-α, IL-1β, IL-6, and IL-15), ROS, matrix metalloproteinases (MMPs), and lipocalin-2. Specifically, VEGF secreted by astrocytes can decrease the expression of endothelial cell tight junctions, exacerbating BBB damage (Kim et al., 2020), as shown in\nDuring the acute phase, BBB disruption and astrocyte activation lead to the secretion of inflammatory factors, directly or indirectly exacerbating BBB dysfunction. The upregulation of IL-1β increases the expression of VEGF-A in activated astrocytes, which disrupts endothelial cell tight junctions by downregulating claudin-5 expression, thereby increasing BBB permeability and worsening brain injury (Li et al., 2022). Conversely, in the late stage of stroke, VEGF can promote vascular regeneration and improve neurological function (Moon et al., 2021). Therefore, VEGF plays different roles in BBB maintenance during different stages of the stroke process.\nStroke leads to the secretion of MMPs by astrocytes, which disrupt tight junction-associated proteins of endothelial cells and ECM molecules. Inhibiting MMP2/9 can enhance BBB integrity, reduce astrocyte activation, and alleviate cognitive impairment in mice (Ji et al., 2023). Secreted by astrocytes, sonic hedgehog (Shh) is a glycoprotein that regulates the expression of zonula occludens-1 and claudin-5 to protect the integrity of the BBB. Research has shown that Shh upregulates angiopoietin-1 (Ang-1) to modulate endothelial cell tight junctions, thereby alleviating stroke-induced brain edema (Michinaga et al., 2024).\nAstrocytes act as both mediators of BBB disruption and facilitators of its repair. Future studies should further elucidate the contradictory roles of astrocyte-secreted factors, such as VEGF and MMPs. This knowledge will enable the development of interventions that effectively control their harmful effects while enhancing their regenerative potential. In addition, a comprehensive exploration of the protective mechanisms of molecules such as insulin-like growth factor-1 and Shh is crucial. Understanding these mechanisms may optimize therapeutic strategies and improve overall outcomes in stroke patients.\nThe potential of astrocytes as therapeutic targets for ischemic stroke has been demonstrated through various approaches, including chemical medications, miRNA therapy, stem cell therapy, cellular reprogramming, hydrogel applications, and astrocyte-derived extracellular vesicles (ADEVs), each offering promising therapeutic avenues.\nThe advantages of medication therapy for ischemic stroke lie in its convenience and affordability. However, it is limited by drug tolerance, poor efficacy, drug interactions, and other factors (Marto et al., 2021). MiRNA therapy aims to regulate miRNA levels, potentially mitigating the detrimental effects of ischemic stroke and promoting the recovery of neurological function by influencing cellular functions and biological processes (Eyileten et al., 2018; Vijayan et al., 2018). Stem cell therapy utilizes the pluripotency and regenerative capabilities of stem cells to repair damaged tissues and promote neuroregeneration. However, these methods face challenges such as allogeneic transplant rejection, difficulties in controlling cell differentiation posttransplantation, and risks of tumorigenesis (Gautam et al., 2020; Mazini et al., 2020; Hoang et al., 2022). Reprogramming involves the in situ conversion of endogenous glial cells into functional neurons for CNS repair, thereby avoiding the uncertainties mentioned above (Sharif et al., 2021). Hydrogels, as carriers, have achieved promising results through drug‒cell combination therapy or the coadministration of small molecules and exosomes (Damian et al., 2021; Fan et al., 2024). ADEVs regulate cellular activities by carrying neuroprotective factors, suggesting potential applications in the treatment of brain injuries and neurodegenerative diseases. However, their functions are influenced by environmental factors and inflammation (Wang et al., 2024).\nIn recent years, research on the role of astrocytes in stroke, along with emerging therapeutic approaches targeting these glial cells, has proliferated. Despite these advances, each approach faces technical challenges and limitations, requiring further research and technological advancements to optimize its application in the treatment of ischemic stroke. The therapeutic strategies related to astrocytes following stroke are summarized in\nSummary of astrocyte-targeted therapeutic strategies following stroke\nADEVs: Astrocyte-derived extracellular vesicles.\nMedication therapy is a commonly used treatment method for ischemic stroke. Its advantages include convenience, affordability, and the accumulation of extensive clinical experience over time. However, medication may have side effects and limitations, such as drug tolerance, poor efficacy, and drug interactions. Moreover, relying solely on medication often cannot achieve desirable brain tissue repair and regeneration in ischemic stroke treatment (Marto et al., 2021).\nMedications targeting astrocytes have shown potential in promoting neurological recovery after ischemic stroke. For example, matrine treatment can reduce the quantity of neuroprotective-phenotype astrocytes while increasing the number of neurotoxic-phenotype astrocytes. It also enhances the expression of the tight junction proteins Claudin 5 and Occludin. This collectively aids in reducing neurological injuries poststroke (Jing et al., 2021). Celastrol decreases ROS levels by upregulating Nrf2 in astrocytes, eventually reducing neuronal damage (Hong et al., 2023). The small peptide OM-LV20 can upregulate pituitary adenylate cyclase-activating peptide type 1 receptor expression in astrocytes, thereby increasing the expression of tryptophan hydroxylase 1 in these cells, which in turn protects them from oxidative stress-induced damage (Yin et al., 2022). Another promising drug candidate, potassium 2-(1-hydroxypentyl)-benzoate, which is currently in phase II clinical trials for ischemic stroke, has demonstrated protective effects on neurons, particularly in the presence of astrocytes. Studies have revealed that potassium 2-(1-hydroxypentyl)-benzoate therapy stimulates the release of astrocyte-derived neurotrophic factors, including brain-derived neurotrophic factor and nerve growth factor, thereby reducing neuronal death after ischemic stroke (Liu et al., 2017). Salidroside has been reported to inhibit astrocyte overreaction and glial scar formation by suppressing the AKT/GSK3β signaling pathway (Dong et al., 2021). Consequently, salidroside also shows promise in the treatment of ischemic stroke.\nEdaravone is a neuroprotective agent clinically employed in China and other Asian countries for treating ischemic stroke. It prevents neuronal death by inhibiting lipid peroxidation, scavenging free radicals, acting as an antioxidant, inhibiting ischemic brain edema, and improving neurological function (Kawasaki et al., 2020). Borneol is an organic compound derived from herbs with analgesic, anti-inflammatory, and anti-epileptogenic properties (Wang et al., 2024). It has been demonstrated that combining edaravone and (+)-borneol effectively inhibits microglia and astrocyte polarization toward a proinflammatory phenotype (Wang et al., 2024). This combination also reduces the infiltration of inflammatory cells such as leukocytes, alleviates neuroinflammation, improves the permeability of the BBB, and significantly reduces cerebral infarctions (Huang et al., 2022; Wang et al., 2024). The present phase III clinical trial demonstrated that female patients who received intravenous edaravone dexborneol treatment within 48 hours post-stroke exhibited better postoperative recovery outcomes than did those treated with edaravone (Xu et al., 2021b). In addition, intravenous administration of ABT263, an antiaging drug, to rats following stroke resulted in the elimination of senescent astrocytes in the brain injury area, reduced the infarcted area of the rat brain, and improved neurological function (Lim et al., 2021).\nThe emergence of drugs targeting astrocytes for the treatment of poststroke neural injury underscores the essential role of astrocytes in the modulation of neurological function after stroke. Further elaboration of astrocyte function during the poststroke period will contribute to an in-depth understanding of the cellular response after stroke and facilitate the research and development of astrocyte-targeting therapeutic drugs for stroke.\nMiRNAs constitute a class of noncoding, single-stranded RNA molecules involved in the posttranscriptional regulation of gene expression and play active roles in cell proliferation, differentiation and function (Kim et al., 2018). miRNA therapy involves regulating the expression levels of miRNAs to intervene in gene expression, thereby impacting cellular functions and biological processes. Its advantages lie in its high specificity and regulatory precision, enabling precise control over specific genes (Eyileten et al., 2018). However, miRNA therapy still faces challenges in aspects such as the effectiveness of delivery vehicles, selection of target genes, safety, which require further research for resolution (Vijayan et al., 2018). In summary, manipulating miRNA levels through nucleic acid–based therapies represents a promising strategy for developing targeted treatments that could mitigate the detrimental effects of ischemic stroke and promote neurological recovery.\nTargeted miRNA therapy is neuroprotective against stroke-related inflammation, glutamate excitotoxicity and glial scar formation. Research suggests that specific miRNAs positively regulate reactive astrocyte activity and function poststroke (Sun et al., 2019b; Li et al., 2021). MiR-210 targets the NF-κB pathway in astrocytes, reducing the expression of the proinflammatory factors IL-6 and C–X–C motif chemokine ligand 10 to inhibit the inflammatory cascade (Kieran et al., 2022). Inhibiting miR-182 enhances astrocyte viability and reduces the release of the inflammatory factor NO during lipopolysaccharide stimulation, thereby exerting an anti-inflammatory effect (Long et al., 2024). Conversely, astrocytes pretreated with berberine and subjected to OGD secreted greater levels of miR-182-5p than did astrocytes treated with only OGD. Upon intravenous injection into MCAO model mice, miR-182-5p-enriched exosomes facilitate neuronal uptake and reduce Rac1 expression, thereby attenuating neuroinflammation (Ding et al., 2023). These results indicate that the regulatory roles of miRNAs in cells can significantly vary across different disease contexts, highlighting the complexity of miRNA-mediated regulation. GLT-1 maintains extracellular homeostasis and attenuates ischemic injury by transferring accumulated glutamate. It has been reported that miR-124 enhances GLT-1 expression in astrocytes via protein kinase B and mechanistic target of rapamycin pathways during cerebral ischemia, improving stroke outcomes (Long et al., 2024). Neuroglial scar formation impedes axonal regeneration and functional recovery postischemic stroke. STAT3 regulates astrogliosis, and miR-124 expression is significantly elevated in M2 microglia exosomes. This elevation could decrease STAT3 expression in astrocytes, reduce neuroglial scar formation, and promote neurological function recovery after stroke (Li et al., 2021). Recent meta-analysis results indicate that miR-124-3p is specifically expressed in the CNS and that its expression is lower in stroke patients than in healthy controls (Wang et al., 2023). Therefore, miR-124-3p could serve as a potential clinical diagnostic biomarker for ischemic stroke.\nRecent studies have demonstrated the successful transdifferentiation of fibroblasts and oligodendrocytes into neurons through miRNA manipulation (Han et al., 2021b; Gu et al., 2022), suggesting a promising avenue for treating neurological injuries via transdifferentiation. Furthermore, both adult human and mouse astrocytes can be reprogrammed into neuroblasts\nExosomes, which are characterized by low immune reactivity, high stability, and the ability to cross the BBB, serve as significant miRNA carriers and have recently garnered attention. Recent research has indicated that miRNAs within mesenchymal stem cell (MSC)-derived exosomes play a vital role in the restoration of brain functions. There are currently only two registered clinical trials involving the use of exosomes in the treatment of stroke, a multicenter, randomized, double-blinded, placebo-controlled, dose-escalation trial initiated by Xuanwu Hospital in China to evaluate the safety and preliminary efficacy of intravenous exosomes derived from human induced pluripotent stem cells (GD-iExo-003) in acute ischemic stroke (\nStem cell therapy capitalizes on the pluripotent and regenerative capacities of stem cells, offering promising avenues for effectively repairing damaged tissues and promoting the regeneration of brain tissue both in neurons and blood vessels (Li et al., 2021). In addition, this therapeutic strategy exploits the immunomodulatory functions of stem cells, which are effective in restoring immune homeostasis and creating an immune microenvironment that facilitates tissue repair (Mazini et al., 2020; Yang et al., 2021; Hoang et al., 2022; Li et al., 2022). Despite its potential, stem cell therapy has several challenges, including immune rejection in allogeneic transplantation, difficulties in controlling cell differentiation postimplantation, and the inherent risks of tumorigenesis associated with pluripotent stem cell derivatives, highlighting the need for additional research to address these challenges (Stonesifer et al., 2017; Gautam et al., 2020; Li and Fang, 2023).\nAmong the cell types extensively tested in preclinical experiments and clinical trials, particularly for ischemic stroke, are hematopoietic stem cells, MSCs, neural stem cells, embryonic stem cells, and induced pluripotent stem cells (Kawabori et al., 2020). Stem cells mediate therapeutic effects predominantly through two mechanisms: cell differentiation and the secretion of paracrine factors (Li et al., 2021). The differentiation of transplanted stem cells into functional neurons, glial cells, and endothelial cells, which contribute to reducing inflammation and enhancing neuronal plasticity, represents the primary goal of stem cell therapy in neurological diseases (Anthony et al., 2022). Concurrently, stem cells are known to produce and secrete a diverse array of chemokines, growth factors, and extracellular vesicles (EVs), which play significant roles in mediating anti-inflammatory, antiapoptotic, angiogenic, and neurogenic effects in various neurodegenerative diseases (Zhou et al., 2022; Hirsch et al., 2023; Zhang et al., 2023). Houkin et al. (2024) reported on a phase 2/3 TREASURE randomized clinical trial of bone marrow-derived allogeneic stem cells for the treatment of acute ischemic stroke, and the results indicated that this therapy was safe for 90 days but did not improve short-term outcomes; further studies are needed to determine whether it is beneficial for patients who meet specific criteria. Lee et al. (2022) reported a neuroimaging study of the efficacy of intravenously injected MSCs on motor recovery after ischemic stroke, which demonstrated that MSCs protected the corticospinal tracts from degradation and enhanced positive changes in network reorganization, facilitating motor recovery.\nRecent advancements in stem cell research have demonstrated the efficacy of MSCs derived from adipose-derived stem cells, which can be efficiently induced into neural progenitors and can be used for cell replacement therapy during ischemic stroke (Wang et al., 2022). Chiu et al. (2022) reported a phase I study of the intracerebral transplantation of autologous adipose-derived stem cells for the treatment of chronic ischemic stroke, which revealed significant improvements in neurological measures, including the National Institute of Health stroke scale, Barthel Index, Berg balance scale, Fugl-Meyer assessment, and somatosensory evoked potentials, in all three participants with chronic stroke without adverse effects within 6 months, demonstrating the broad potential of adipose-derived stem cells in the treatment of CNS disorders such as stroke (Chiu et al., 2022). Intracerebral injection of conditioned media from MSCs derived from human embryonic stem cells to treat MCAO rats significantly enhances neurogenesis and reduces ischemic brain injury (Asgari Taei et al., 2022). Additionally, intracerebral injection of MSCs overexpressing fibroblast growth factor 21 significantly reduces infarct volume, decreases the expression of proinflammatory proteins, attenuates BBB degradation, and enhances motor function in rats (Do et al., 2024). Notably, MSCs contribute to therapeutic outcomes not only through the secretion of soluble bioactive molecules but also via the release of EVs, which are crucial in mediating intercellular communication. These vesicles facilitate the transfer of information to recipient cells, such as astrocytes and neurons, in the brain, enhancing cellular communication and potentially improving recovery processes (Do et al., 2024).\nFurther investigations revealed that MSCs promote the survival of microglia, astrocytes, and endothelial cells postischemia. This improvement is attributed to the secretion of growth factors and exosomes, along with the transfer of functional mitochondria, thus assisting in cellular repair and neuroprotection (Deng et al., 2019; Liu et al., 2019, 2021). Moreover, the cotransplantation of neural stem cells with astrocytes and brain microvascular endothelial cells has been shown to improve memory functions in ischemic rats. This enhancement suggests that the optimum milieu provided by astrocytes and brain microvascular endothelial cells supports the survival and differentiation of transplanted neural stem cells\nNevertheless, despite the demonstrated safety of MSC therapy in a clinical setting, the effectiveness of the treatment remains uncertain, and proven efficacy has not been achieved. Therefore, the interactions among stem cells, astrocytes, and/or other resident cells in the brain hold great potential targets for achieving more effective therapeutic outcomes. Cotransplantation of stem cells with astrocytes may emerge as a promising strategy for the treatment of ischemic stroke.\nAs discussed earlier, stem cell therapies present several challenges, including the risk of tumorigenesis, ethical concerns, unpredictable differentiation paths, and immune rejection. Given these instability factors, an alternative approach that has garnered attention is the in situ reprogramming of endogenous glial cells into functional neurons to rebuild CNS repair. This method may serve as a safe and reliable alternative option for treating ischemic stroke. Simultaneously, the in situ reprogramming of endogenous glial cells may also reduce the formation of glial scars following neurological injuries caused by ischemic stroke and prevent excessive inflammatory responses, thereby facilitating recovery from secondary damage. Although the technology for glial cell reprogramming is still in its nascent stage, promising prospects have been highlighted by a number of studies (Magnusson et al., 2020; Sharif et al., 2021).\nSeveral studies have robustly demonstrated that astroglia, both during early postnatal development and in the adult stage, can be reprogrammed to acquire a neuronal fate (Peng et al., 2022; Talifu et al., 2023). This transformation can be achieved\nThe clinical efficacy of this innovative approach across various neurological disorders has been well documented. Reprogrammed astrocytes have demonstrated significant therapeutic potential in the treatment of Parkinson’s disease (Zhu et al., 2019; Giehrl-Schwab et al., 2022; Wang et al., 2023), ischemic stroke (Chen et al., 2020; Ge et al., 2020), Huntington’s disease (Wu et al., 2020), spinal cord injuries (Puls et al., 2020; Tan et al., 2022), and AD (Yavarpour-Bali et al., 2020). Consequently, the astrocyte reprogramming strategy presents a promising avenue for developing astrocyte-centered therapies, particularly for ischemic stroke and other neurological conditions.\nOn the basis of astrocyte secretion profiling analysis, another approach involving direct delivery of ADEVs may offer another intervention strategy for ischemic stroke. Recent investigations have underscored the critical role of ADEVs in preserving brain homeostasis and influencing the functionality of other CNS cells. These nanosized, double-membraned vesicles efficiently cross the BBB, enabling the intercellular transfer of proteins, nucleic acids, and lipids that are instrumental in modulating cellular activities (Edwardson et al., 2024). Specifically, ADEVs from naïve astrocytes contain neuroprotective agents such as fibroblast growth factor-2, VEGF, and apolipoprotein-D, which have demonstrated potential therapeutic benefits in treating brain injuries and neurodegenerative disorders (Upadhya et al., 2020; Wang et al., 2024).\nIn contrast, in neurodegenerative diseases such as stroke, AD, and Parkinson’s disease, ADEVs sourced from activated astrocytes are implicated in worsening pathological conditions (Li et al., 2023). Studies indicate that under conditions of oxidative stress or exposure to proinflammatory cytokines such as IL-1β, ADEVs contribute to reduced synaptic growth and increased neuronal apoptosis. Furthermore, stimulation by the anti-inflammatory cytokine IL-10 has been shown to facilitate synaptic transmission and enhance neuronal survival, highlighting the variable impact of environmental factors on ADEV functionality (Datta Chaudhuri et al., 2020; You et al., 2020). Moreover, ADEVs are known to harbor the glutamate transporter proteins Excitatory amino acid transporter-1 and Excitatory amino acid transporter-2, which are essential for maintaining low levels of extracellular glutamate, thus reducing excitotoxicity, a common consequence following stroke (Wang et al., 2024).\nIn conclusion, ADEVs are particularly notable for their ability to traverse the BBB efficiently, an advantageous characteristic that highlights their potential utility as therapeutic agents. Nevertheless, investigations into the multifaceted regulatory mechanisms of ADEVs under both physiological and pathological conditions remain in the preliminary phases. A comprehensive understanding of these dynamics is imperative for effectively utilizing ADEVs in clinical settings, thus offering new avenues for the development of targeted therapies that capitalize on their natural biological functions.\nIn pursuit of efficacious stroke therapies, it is essential to address the challenge of delivering therapeutic agents directly to damaged regions of the brain. The BBB plays a critical role in safeguarding the brain against potential toxins and pathogens but simultaneously poses a formidable barrier to the direct delivery of therapeutic drugs. In this context, hydrogels have garnered attention as a potent means to improve local drug delivery specifically tailored to treating neurological disorders.\nHydrogels, synthesized from a flexible matrix and insoluble polymers, undergo transformations from solution to gel via thermal, physical, and chemical stimuli. This process endows hydrogels with a soft and porous three-dimensional structure characterized by high water content, increasing their biocompatibility with brain tissue, which minimizes the risk of immune rejection, enhances their biocompatibility, and enables precise site-specific drug delivery that can be customized according to the tissue type (Fan et al., 2024; Gao et al., 2024). Hydrogels not only have the potential to modulate inflammatory responses but also create a conducive environment for cell growth and tissue repair, addressing key hurdles in neurological recovery (Mahmoudi et al., 2024).\nMagnetic resonance imaging studies of stroke patients consistently revealed tissue loss, or voids, in 94% of cases, a phenomenon also observed in experimental stroke models in mice (Damian et al., 2021; Farrher et al., 2021). Tissue cavitation can lead to the infiltration of immune cells. Nevertheless, these cavities in the stroke-affected brain present promising targets for therapeutic interventions, as they can accommodate multiple hydrogel injections without disrupting surrounding healthy brain structures. Further research utilizing hyaluronic acid-based microporous annealed particle hydrogels in MCAO mice revealed profound therapeutic benefits. Postinjection, there was notable infiltration of astrocytes from the peri-infarct area into the stroke cavity. This migration is coupled with a reduced thickness of the astrocytic scar around the infarct core, effectively curbing the excessive influx of activated microglia and further alleviating neuroinflammation (Sideris et al., 2022).\nMoreover, the effectiveness of hydrogels in gene therapy has also been validated. For example, hybrid adeno-associated viruses carrying neural reprogramming transgenes have been precisely and controllably delivered to a mouse model of traumatic brain injury via hydrogels. This innovative approach facilitates the transdifferentiation of reactive astrocytes into neurons, markedly reducing the formation of the glial scar, thus highlighting the transformative potential of hydrogels in advanced brain injury treatments (Mahmoudi et al., 2024). In the CNS, hydrogels serve as scaffoldings that facilitate the reconstitution of the ECM and support the migration of adjacent cells. Furthermore, when combined with therapeutic agents or stem cells, hydrogels play a pivotal role in mitigating inflammation and preventing the formation of glial scar perilesions or lesion areas (Khan et al., 2021; Yu et al., 2022). This convergence of favorable characteristics makes hydrogels highly promising platforms for overcoming the current limitations faced in the targeted therapy of stroke.\nTaken together, these findings underscore the potential of hydrogels as versatile and effective therapeutic delivery tools in the treatment of stroke. Their common use as carriers, particularly through combination with drugs, cells, small molecules and exosomes, has led to promising treatment outcomes through the provision of structural support or sustained release. Such multifaceted approach systems may help to overcome some drawbacks associated with a single intervention approach.\nThis review provides a systematic overview of the fundamental functions of astrocytes, with a particular focus on their role in ischemic stroke, and discusses various astrocyte-based therapies for stroke. Given the scope of this article, we summarize key advancements in research on astrocytes in stroke in recent years, including studies on modulators of neuroinflammation, protection against oxidative damage, mitochondrial interactions with neurons, the regulation of water homeostasis, and the dual roles of astrocytes at the BBB. However, other roles of astrocytes in ischemic stroke have not been extensively discussed.\nIn the therapeutic section, we summarize astrocyte-related therapies and their preliminary clinical applications but do not fully address the challenges and limitations encountered in clinical translation. Additionally, the potential for synergistic effects from combining different therapeutic approaches has not been explored in depth. This manuscript focuses on the mechanistic roles of astrocytes in ischemic stroke; most of the evidence is extracted from animal data rather than the corresponding clinical data, although the latter may be more exciting and deserve much more attention. Another reason may be the absence or unavailability of some ongoing clinical trials related to this review topic.\nOver the past few decades, attention to the pathogenesis and treatment of ischemic stroke has focused primarily on neurons, whereas astrocytes have been largely overlooked. As the most abundant glial cells in the brain, astrocytes play crucial roles in the formation of tripartite synapses and neurovascular units, and they actively communicate with nearly all types of brain cells. An increasing number of studies have shown that astrocytes are important regulators of various CNS diseases. Under healthy physiological conditions, astrocytes supply energy for neuronal activity, support brain tissue metabolism, regulate neurotransmitter levels, and control neuronal communication, as depicted in the light blue section of\nFunctions of astrocytes under normal and ischemic conditions.\nIn a healthy state, astrocytes sustain neuronal activity by supplying energy, regulating metabolism, balancing neurotransmitter levels, and facilitating neuronal communication. Under ischemic conditions, astrocytes undergo differentiation into distinct subtypes: A1 astrocytes release inflammatory factors that can compromise neuronal function, whereas A2 astrocytes release neurotrophic factors and GF, thereby promoting neuronal protection and facilitating angiogenesis. This dual response highlights the intricate interplay between astrocytes and neurons in both physiological and pathological states. AKT/GSK3β: Protein kinase B/glycogen synthase kinase 3β; d,l-PHPB: potassium 2-(1-hydroxypentyl)-benzoate; GF: growth factor; NF-κB: nuclear factor kappa-B.\nAfter ischemic injury, astrocytes undergo activation and adopt different states, including neurotoxic type A1 and neuroprotective type A2. In the neuroprotective A2 state, astrocytes release neurotrophic factors that promote angiogenesis and synaptogenesis, reduce excitotoxicity, and facilitate neurological recovery following stroke, as shown in the pink section of\nAlthough the dual roles of astrocytes are not yet fully understood, their potential in neuroprotection and repair processes has increasingly attracted attention. With the advent of single-cell genomics and spatial genomics, analyzing the mechanisms of different astrocyte subtypes at various stages of stroke progression is expected to become a key research direction. This approach may facilitate the transformation of harmful astrocyte subtypes into beneficial subtypes, thereby enhancing therapeutic outcomes.\nRecent research into the critical role of astrocytes in ischemic stroke has increased, and various treatment strategies have been proposed. These include chemical drugs, miRNA therapy, stem cell therapy, cellular reprogramming, hydrogel applications, and ADEVs. However, each of these approaches has limitations and drawbacks. The key challenge in improving these therapeutic strategies is to further investigate the specific mechanisms underlying which astrocytes contribute to ischemic stroke and integrate advanced biotechnological methods for optimizing these treatments. One major area of stroke research is how to enhance the neuroprotective role of astrocytes while minimizing their potential harmful effects on neurons. Therefore, a comprehensive understanding of how astrocytes respond to stroke, when to target astrocytes, and which astrocyte subtypes to target is crucial for the development of safer and more effective stroke therapies.", "topic": "Brain"}
